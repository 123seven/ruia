{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Overview An async web scraping micro-framework, written with asyncio and aiohttp , aims to make crawling url as convenient as possible. Write less, run faster: Documentation: \u4e2d\u6587\u6587\u6863 | documentation Plugins: https://github.com/ruia-plugins Installation # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia Usage Request & Response We provide an easy way to request a url and return a friendly response : import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}> JavaScript Support : You can load js by using ruia-pyppeteer . For example: import asyncio from ruia_pyppeteer import PyppeteerRequest as Request request = Request ( \"https://www.jianshu.com/\" , load_js = True ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) print ( response . html ) Item Let's take a look at a quick example of using Item to extract target data. Start off by adding the following to your demo.py: import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) items = asyncio . get_event_loop () . run_until_complete ( HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" )) for item in items : print ( item . title , item . url ) Run: python demo.py Notorious \u2018Hijack Factory\u2019 Shunned from Web https://krebsonsecurity.com/2018/07/notorious-hijack-factory-shunned-from-web/ ...... Spider For multiple pages, you can solve this with Spider Create hacker_news_spider.py : import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( item . title + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Run hacker_news_spider.py : [ 2018 -09-21 17 :27:14,497 ] -ruia-INFO spider::l54: Spider started! [ 2018 -09-21 17 :27:14,502 ] -Request-INFO request::l77: <GET: https://news.ycombinator.com/news?p = 2 > [ 2018 -09-21 17 :27:14,527 ] -Request-INFO request::l77: <GET: https://news.ycombinator.com/news?p = 1 > [ 2018 -09-21 17 :27:16,388 ] -ruia-INFO spider::l122: Stopping spider: ruia [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l68: Total requests: 2 [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l71: Time usage: 0 :00:01.891688 [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l72: Spider finished! Custom middleware ruia provides an easy way to customize requests, as long as it does not return it . The following middleware code is based on the above example: from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): request . metadata = { 'index' : request . url . split ( '=' )[ - 1 ] } print ( f \"request: {request.metadata}\" ) @middleware.response async def print_on_response ( request , response ): print ( f \"response: {response.metadata}\" ) # Add HackerNewsSpider if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) Features Custom middleware JavaScript support Friendly response TODO [ ] Distributed crawling/scraping Contribution Pull Request Open Issue Thanks sanic demiurge Congrats\ud83c\udf89, you finished your first crawler by using ruia, wanna learn more? Tutorials are available!","title":"Introduction"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#overview","text":"An async web scraping micro-framework, written with asyncio and aiohttp , aims to make crawling url as convenient as possible. Write less, run faster: Documentation: \u4e2d\u6587\u6587\u6863 | documentation Plugins: https://github.com/ruia-plugins","title":"Overview"},{"location":"#installation","text":"# For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#request-response","text":"We provide an easy way to request a url and return a friendly response : import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}> JavaScript Support : You can load js by using ruia-pyppeteer . For example: import asyncio from ruia_pyppeteer import PyppeteerRequest as Request request = Request ( \"https://www.jianshu.com/\" , load_js = True ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) print ( response . html )","title":"Request &amp; Response"},{"location":"#item","text":"Let's take a look at a quick example of using Item to extract target data. Start off by adding the following to your demo.py: import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) items = asyncio . get_event_loop () . run_until_complete ( HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" )) for item in items : print ( item . title , item . url ) Run: python demo.py Notorious \u2018Hijack Factory\u2019 Shunned from Web https://krebsonsecurity.com/2018/07/notorious-hijack-factory-shunned-from-web/ ......","title":"Item"},{"location":"#spider","text":"For multiple pages, you can solve this with Spider Create hacker_news_spider.py : import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( item . title + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Run hacker_news_spider.py : [ 2018 -09-21 17 :27:14,497 ] -ruia-INFO spider::l54: Spider started! [ 2018 -09-21 17 :27:14,502 ] -Request-INFO request::l77: <GET: https://news.ycombinator.com/news?p = 2 > [ 2018 -09-21 17 :27:14,527 ] -Request-INFO request::l77: <GET: https://news.ycombinator.com/news?p = 1 > [ 2018 -09-21 17 :27:16,388 ] -ruia-INFO spider::l122: Stopping spider: ruia [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l68: Total requests: 2 [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l71: Time usage: 0 :00:01.891688 [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l72: Spider finished!","title":"Spider"},{"location":"#custom-middleware","text":"ruia provides an easy way to customize requests, as long as it does not return it . The following middleware code is based on the above example: from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): request . metadata = { 'index' : request . url . split ( '=' )[ - 1 ] } print ( f \"request: {request.metadata}\" ) @middleware.response async def print_on_response ( request , response ): print ( f \"response: {response.metadata}\" ) # Add HackerNewsSpider if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware )","title":"Custom middleware"},{"location":"#features","text":"Custom middleware JavaScript support Friendly response","title":"Features"},{"location":"#todo","text":"[ ] Distributed crawling/scraping","title":"TODO"},{"location":"#contribution","text":"Pull Request Open Issue","title":"Contribution"},{"location":"#thanks","text":"sanic demiurge Congrats\ud83c\udf89, you finished your first crawler by using ruia, wanna learn more? Tutorials are available!","title":"Thanks"},{"location":"cn/","text":"Ruia Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u5b83\u7684\u76ee\u6807\u662f\u8ba9\u4f60\u66f4\u52a0\u65b9\u4fbf\u4e14\u8fc5\u901f\u5730\u7f16\u5199\u51fa\u5c5e\u4e8e\u81ea\u5df1\u7684\u722c\u866b \u5f88\u9ad8\u5174\u4f60\u80fd\u4f7f\u7528 Ruia \u6765\u5b9e\u73b0\u722c\u866b\u7a0b\u5e8f\uff0c\u4e0d\u8fc7\u5728\u7f16\u7801\u4e4b\u524d\uff0c\u5e0c\u671b\u4f60\u80fd\u901a\u8bfb\u6b64\u6587\u6863\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e86 Ruia \u7684\u4f7f\u7528\u65b9\u6cd5\u4ee5\u53ca\u4e00\u4e9b\u57fa\u7840\u6982\u5ff5\u4ecb\u7ecd First steps Introduction \uff1a\u4ecb\u7ecdRuia Tutorials \uff1a\u4f7f\u7528Ruia\u5feb\u901f\u7f16\u5199\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b Plugins \uff1a\u7f16\u5199Ruia\u6269\u5c55 Topics Item \uff1a\u5b9a\u4e49\u722c\u866b\u7684\u76ee\u6807\u5b57\u6bb5 Selector \uff1a\u4eceHTML\u4e2d\u63d0\u53d6\u51fa\u76ee\u6807\u5b57\u6bb5 Request \uff1a\u8bf7\u6c42\u5e76\u6293\u53d6\u76ee\u6807\u7f51\u7ad9\u8d44\u6e90 Response \uff1a\u8fdb\u4e00\u6b65\u5c01\u88c5\u54cd\u5e94\u5185\u5bb9 Middleware \uff1a\u4f7f\u722c\u866b\u652f\u6301\u7b2c\u4e09\u65b9\u6269\u5c55 Spider \uff1a\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3 Getting help \u5982\u679c\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u9047\u5230\u4e86\u56f0\u96be\uff0c\u968f\u65f6\u6b22\u8fce\u63d0 Issue \u4e5f\u968f\u65f6\u6b22\u8fce\u52a0\u6211\u5fae\u4fe1\u62c9\u60a8\u8fdb\u7fa4\u4ea4\u6d41\uff0c\u5907\u6ce8(Ruia)\uff1a","title":"Ruia"},{"location":"cn/#ruia","text":"Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u5b83\u7684\u76ee\u6807\u662f\u8ba9\u4f60\u66f4\u52a0\u65b9\u4fbf\u4e14\u8fc5\u901f\u5730\u7f16\u5199\u51fa\u5c5e\u4e8e\u81ea\u5df1\u7684\u722c\u866b \u5f88\u9ad8\u5174\u4f60\u80fd\u4f7f\u7528 Ruia \u6765\u5b9e\u73b0\u722c\u866b\u7a0b\u5e8f\uff0c\u4e0d\u8fc7\u5728\u7f16\u7801\u4e4b\u524d\uff0c\u5e0c\u671b\u4f60\u80fd\u901a\u8bfb\u6b64\u6587\u6863\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e86 Ruia \u7684\u4f7f\u7528\u65b9\u6cd5\u4ee5\u53ca\u4e00\u4e9b\u57fa\u7840\u6982\u5ff5\u4ecb\u7ecd","title":"Ruia"},{"location":"cn/#first-steps","text":"Introduction \uff1a\u4ecb\u7ecdRuia Tutorials \uff1a\u4f7f\u7528Ruia\u5feb\u901f\u7f16\u5199\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b Plugins \uff1a\u7f16\u5199Ruia\u6269\u5c55","title":"First steps"},{"location":"cn/#topics","text":"Item \uff1a\u5b9a\u4e49\u722c\u866b\u7684\u76ee\u6807\u5b57\u6bb5 Selector \uff1a\u4eceHTML\u4e2d\u63d0\u53d6\u51fa\u76ee\u6807\u5b57\u6bb5 Request \uff1a\u8bf7\u6c42\u5e76\u6293\u53d6\u76ee\u6807\u7f51\u7ad9\u8d44\u6e90 Response \uff1a\u8fdb\u4e00\u6b65\u5c01\u88c5\u54cd\u5e94\u5185\u5bb9 Middleware \uff1a\u4f7f\u722c\u866b\u652f\u6301\u7b2c\u4e09\u65b9\u6269\u5c55 Spider \uff1a\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3","title":"Topics"},{"location":"cn/#getting-help","text":"\u5982\u679c\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u9047\u5230\u4e86\u56f0\u96be\uff0c\u968f\u65f6\u6b22\u8fce\u63d0 Issue \u4e5f\u968f\u65f6\u6b22\u8fce\u52a0\u6211\u5fae\u4fe1\u62c9\u60a8\u8fdb\u7fa4\u4ea4\u6d41\uff0c\u5907\u6ce8(Ruia)\uff1a","title":"Getting help"},{"location":"cn/first_steps/introduction/","text":"Introduction Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u5b83\u5177\u6709\u7f16\u5199\u5feb\u901f\uff0c\u975e\u963b\u585e\uff0c\u6269\u5c55\u6027\u5f3a\u7b49\u7279\u70b9\uff0c\u8ba9\u4f60\u5199\u66f4\u5c11\u7684\u4ee3\u7801\uff0c\u6536\u83b7\u66f4\u5feb\u7684\u8fd0\u884c\u901f\u5ea6 \u7279\u6027\u5982\u4e0b\uff1a - \u81ea\u5b9a\u4e49\u4e2d\u95f4\u4ef6 - \u652f\u6301js\u52a0\u8f7d\u7c7b\u578b\u7f51\u9875 - \u53cb\u597d\u5730\u6570\u636e\u54cd\u5e94\u7c7b - \u5f02\u6b65\u65e0\u963b\u585e Installation \u5b89\u88c5 Ruia \u4e4b\u524d\u8bf7\u5148\u786e\u4fdd\u4f60\u4f7f\u7528\u7684\u662f Python3.6+ # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia Code Snippets \u4e0b\u9762\u6211\u5c06\u4e3e\u4e2a\u4f8b\u5b50\u7b80\u5355\u4ecb\u7ecd\u4e0b Ruia \u7684\u4f7f\u7528\u65b9\u5f0f\u4ee5\u53ca\u6846\u67b6\u8fd0\u884c\u6d41\u7a0b\uff0c\u521b\u5efa\u6587\u4ef6 hacker_news_spider.py \uff0c\u7136\u540e\u62f7\u8d1d\u4e0b\u9762\u4ee3\u7801\u5230\u6587\u4ef6\u4e2d\uff1a #!/usr/bin/env python \"\"\" Target: https://news.ycombinator.com/ pip install aiofiles \"\"\" import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): \"\"\" \u5982\u679c\u5b57\u6bb5\u4e0d\u9700\u8981\u6e05\u6d17 \u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u4e0d\u5199 \"\"\" return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( item . title + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = None ) \u5728\u7ec8\u7aef\u6267\u884c python hacker_news_spider.py \uff0c\u5982\u679c\u987a\u5229\u7684\u8bdd\u5c06\u4f1a\u5f97\u5230\u5982\u4e0b\u8f93\u51fa\uff0c\u5e76\u4e14\u76ee\u6807\u6570\u636e\u4f1a\u5b58\u50a8\u5728 hacker_news.txt \u6587\u4ef6\u4e2d\uff1a [ 2018 -09-24 11 :02:05,088 ] -ruia-INFO spider : Spider started! [ 2018 -09-24 11 :02:05,089 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 2 > [ 2018 -09-24 11 :02:05,113 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 1 > [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Stopping spider: ruia [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Total requests: 2 [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Time usage: 0 :00:01.731780 [ 2018 -09-24 11 :02:09,821 ] -ruia-INFO spider : Spider finished! Getting help \u5982\u679c\u7a0b\u5e8f\u8fd0\u884c\u5730\u4e0d\u591f\u987a\u5229\uff0c\u8bf7\u603b\u7ed3\u95ee\u9898\u65e5\u5fd7\uff0c\u5e76\u7ed9\u6211\u4eec\u63d0\u4ea4 Issue \u606d\u559c\u4f60\uff0c\u4f60\u5df2\u7ecf\u7f16\u5199\u4e86\u4e00\u4e2a\u5c5e\u4e8e\u81ea\u5df1\u7684\u5f02\u6b65\u722c\u866b\uff0c\u662f\u4e0d\u662f\u5f88\u7b80\u5355\uff0c\u63a5\u4e0b\u6765\u4f60\u5c06\u5b9e\u9645\u7f16\u5199\u4e00\u4e2a\u4f8b\u5b50\uff0c\u4f1a\u66f4\u52a0\u6df1\u523b\u5730\u8ba4\u8bc6\u5230 Ruia \u7684\u5f3a\u5927\u4e4b\u5904\uff0c\u8bf7\u7ee7\u7eed\u9605\u8bfb Tutorials","title":"Introduction"},{"location":"cn/first_steps/introduction/#introduction","text":"Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u5b83\u5177\u6709\u7f16\u5199\u5feb\u901f\uff0c\u975e\u963b\u585e\uff0c\u6269\u5c55\u6027\u5f3a\u7b49\u7279\u70b9\uff0c\u8ba9\u4f60\u5199\u66f4\u5c11\u7684\u4ee3\u7801\uff0c\u6536\u83b7\u66f4\u5feb\u7684\u8fd0\u884c\u901f\u5ea6 \u7279\u6027\u5982\u4e0b\uff1a - \u81ea\u5b9a\u4e49\u4e2d\u95f4\u4ef6 - \u652f\u6301js\u52a0\u8f7d\u7c7b\u578b\u7f51\u9875 - \u53cb\u597d\u5730\u6570\u636e\u54cd\u5e94\u7c7b - \u5f02\u6b65\u65e0\u963b\u585e","title":"Introduction"},{"location":"cn/first_steps/introduction/#installation","text":"\u5b89\u88c5 Ruia \u4e4b\u524d\u8bf7\u5148\u786e\u4fdd\u4f60\u4f7f\u7528\u7684\u662f Python3.6+ # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia","title":"Installation"},{"location":"cn/first_steps/introduction/#code-snippets","text":"\u4e0b\u9762\u6211\u5c06\u4e3e\u4e2a\u4f8b\u5b50\u7b80\u5355\u4ecb\u7ecd\u4e0b Ruia \u7684\u4f7f\u7528\u65b9\u5f0f\u4ee5\u53ca\u6846\u67b6\u8fd0\u884c\u6d41\u7a0b\uff0c\u521b\u5efa\u6587\u4ef6 hacker_news_spider.py \uff0c\u7136\u540e\u62f7\u8d1d\u4e0b\u9762\u4ee3\u7801\u5230\u6587\u4ef6\u4e2d\uff1a #!/usr/bin/env python \"\"\" Target: https://news.ycombinator.com/ pip install aiofiles \"\"\" import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): \"\"\" \u5982\u679c\u5b57\u6bb5\u4e0d\u9700\u8981\u6e05\u6d17 \u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u4e0d\u5199 \"\"\" return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( item . title + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = None ) \u5728\u7ec8\u7aef\u6267\u884c python hacker_news_spider.py \uff0c\u5982\u679c\u987a\u5229\u7684\u8bdd\u5c06\u4f1a\u5f97\u5230\u5982\u4e0b\u8f93\u51fa\uff0c\u5e76\u4e14\u76ee\u6807\u6570\u636e\u4f1a\u5b58\u50a8\u5728 hacker_news.txt \u6587\u4ef6\u4e2d\uff1a [ 2018 -09-24 11 :02:05,088 ] -ruia-INFO spider : Spider started! [ 2018 -09-24 11 :02:05,089 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 2 > [ 2018 -09-24 11 :02:05,113 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 1 > [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Stopping spider: ruia [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Total requests: 2 [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Time usage: 0 :00:01.731780 [ 2018 -09-24 11 :02:09,821 ] -ruia-INFO spider : Spider finished!","title":"Code Snippets"},{"location":"cn/first_steps/introduction/#getting-help","text":"\u5982\u679c\u7a0b\u5e8f\u8fd0\u884c\u5730\u4e0d\u591f\u987a\u5229\uff0c\u8bf7\u603b\u7ed3\u95ee\u9898\u65e5\u5fd7\uff0c\u5e76\u7ed9\u6211\u4eec\u63d0\u4ea4 Issue \u606d\u559c\u4f60\uff0c\u4f60\u5df2\u7ecf\u7f16\u5199\u4e86\u4e00\u4e2a\u5c5e\u4e8e\u81ea\u5df1\u7684\u5f02\u6b65\u722c\u866b\uff0c\u662f\u4e0d\u662f\u5f88\u7b80\u5355\uff0c\u63a5\u4e0b\u6765\u4f60\u5c06\u5b9e\u9645\u7f16\u5199\u4e00\u4e2a\u4f8b\u5b50\uff0c\u4f1a\u66f4\u52a0\u6df1\u523b\u5730\u8ba4\u8bc6\u5230 Ruia \u7684\u5f3a\u5927\u4e4b\u5904\uff0c\u8bf7\u7ee7\u7eed\u9605\u8bfb Tutorials","title":"Getting help"},{"location":"cn/first_steps/plugins/","text":"Plugins \u6269\u5c55\u7684\u76ee\u7684\u662f\u5c06\u4e00\u4e9b\u5728\u722c\u866b\u7a0b\u5e8f\u4e2d\u9891\u7e41\u4f7f\u7528\u7684\u529f\u80fd\u5c01\u88c5\u8d77\u6765\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u5757\u4f9b\u7b2c\u4e09\u65b9\u8c03\u7528\uff0c Ruia \u901a\u8fc7 Middleware \u6765\u8ba9\u5f00\u53d1\u8005\u5feb\u901f\u5730\u5b9e\u73b0\u7b2c\u4e09\u65b9\u6269\u5c55 \u524d\u9762\u4e00\u8282\u5df2\u7ecf\u8bf4\u8fc7\uff0c Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u7136\u540e\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u529f\u80fd\uff0c\u5c31\u662f\u5728\u8bf7\u6c42\u5934\u91cc\u9762\u52a0\u5165 User-Agent \u53ef\u80fd\u4efb\u610f\u4e00\u4e2a\u722c\u866b\u90fd\u4f1a\u9700\u8981\u81ea\u52a8\u6dfb\u52a0\u968f\u673a User-Agent \u7684\u529f\u80fd\uff0c\u8ba9\u6211\u5c06\u8fd9\u4e2a\u529f\u80fd\u5c01\u88c5\u4e0b\uff0c\u4f7f\u5176\u6210\u4e3a Ruia \u7684\u4e00\u4e2a\u7b2c\u4e09\u65b9\u6269\u5c55\u5427\uff0c\u8ba9\u6211\u4eec\u73b0\u5728\u5c31\u5f00\u59cb\u5427 Creating a project \u9879\u76ee\u540d\u79f0\u4e3a\uff1a ruia-ua \uff0c\u56e0\u4e3a Ruia \u57fa\u4e8e Python3.6+ \uff0c\u6240\u4ee5\u6269\u5c55 ruia-ua \u4e5f\u4ea6\u7136\uff0c\u5047\u8bbe\u4f60\u6b64\u65f6\u4f7f\u7528\u7684\u662f Python3.6+ \uff0c\u8bf7\u6309\u7167\u5982\u4e0b\u64cd\u4f5c\uff1a # \u5b89\u88c5\u5305\u7ba1\u7406\u5de5\u5177 pipenv pip install pipenv # \u521b\u5efa\u9879\u76ee\u6587\u4ef6\u5939 mkdir ruia-ua cd ruia-ua # \u5b89\u88c5\u865a\u62df\u73af\u5883 pipenv install # \u5b89\u88c5 ruia pipenv install ruia # \u5b89\u88c5 aiofiles pipenv install aiofiles # \u521b\u5efa\u9879\u76ee\u76ee\u5f55 mkdir ruia_ua cd ruia_ua # \u5b9e\u73b0\u4ee3\u7801\u653e\u5728\u8fd9\u91cc touch __init__.py \u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a ruia-ua \u251c\u2500\u2500 LICENSE # \u5f00\u6e90\u534f\u8bae \u251c\u2500\u2500 Pipfile # pipenv \u7ba1\u7406\u5de5\u5177\u751f\u6210\u6587\u4ef6 \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # \u4ee3\u7801\u5b9e\u73b0 \u2502 \u2514\u2500\u2500 user_agents.txt # \u968f\u673aua\u96c6\u5408 \u2514\u2500\u2500 setup.py First extension user_agents.txt \u6587\u4ef6\u5305\u542b\u4e86\u5404\u79cd ua \uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u53ea\u8981\u5229\u7528 ruia \u7684 Middleware \u5b9e\u73b0\u5728\u6bcf\u6b21\u8bf7\u6c42\u524d\u968f\u673a\u6dfb\u52a0\u4e00\u4e2a User-Agent \u5373\u53ef\uff0c\u5b9e\u73b0\u4ee3\u7801\u5982\u4e0b\uff1a import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware.request async def add_random_ua ( request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } \u7f16\u5199\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5c06 ruia-ua \u4e0a\u4f20\u81f3\u793e\u533a\uff0c\u8fd9\u6837\u6240\u6709\u7684 ruia \u4f7f\u7528\u8005\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u4f60\u7f16\u5199\u7684\u7b2c\u4e09\u65b9\u6269\u5c55\uff0c\u591a\u4e48\u7f8e\u597d\u7684\u4e00\u4ef6\u4e8b Usage \u6240\u6709\u7684\u722c\u866b\u7a0b\u5e8f\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 ruia-ua \u6765\u5b9e\u73b0\u81ea\u52a8\u6dfb\u52a0 User-Agent pip install ruia - ua \u4e3e\u4e2a\u5b9e\u9645\u4f7f\u7528\u7684\u4f8b\u5b50\uff1a from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) \u7b2c\u4e09\u65b9\u6269\u5c55\u7684\u5b9e\u73b0\u5c06\u4f1a\u5927\u5927\u51cf\u5c11\u722c\u866b\u5de5\u7a0b\u5e08\u7684\u5f00\u53d1\u5468\u671f\uff0c ruia \u975e\u5e38\u5e0c\u671b\u4f60\u53ef\u4ee5\u5f00\u53d1\u5e76\u63d0\u4ea4\u81ea\u5df1\u7684\u7b2c\u4e09\u65b9\u6269\u5c55","title":"Plugins"},{"location":"cn/first_steps/plugins/#plugins","text":"\u6269\u5c55\u7684\u76ee\u7684\u662f\u5c06\u4e00\u4e9b\u5728\u722c\u866b\u7a0b\u5e8f\u4e2d\u9891\u7e41\u4f7f\u7528\u7684\u529f\u80fd\u5c01\u88c5\u8d77\u6765\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u5757\u4f9b\u7b2c\u4e09\u65b9\u8c03\u7528\uff0c Ruia \u901a\u8fc7 Middleware \u6765\u8ba9\u5f00\u53d1\u8005\u5feb\u901f\u5730\u5b9e\u73b0\u7b2c\u4e09\u65b9\u6269\u5c55 \u524d\u9762\u4e00\u8282\u5df2\u7ecf\u8bf4\u8fc7\uff0c Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u7136\u540e\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u529f\u80fd\uff0c\u5c31\u662f\u5728\u8bf7\u6c42\u5934\u91cc\u9762\u52a0\u5165 User-Agent \u53ef\u80fd\u4efb\u610f\u4e00\u4e2a\u722c\u866b\u90fd\u4f1a\u9700\u8981\u81ea\u52a8\u6dfb\u52a0\u968f\u673a User-Agent \u7684\u529f\u80fd\uff0c\u8ba9\u6211\u5c06\u8fd9\u4e2a\u529f\u80fd\u5c01\u88c5\u4e0b\uff0c\u4f7f\u5176\u6210\u4e3a Ruia \u7684\u4e00\u4e2a\u7b2c\u4e09\u65b9\u6269\u5c55\u5427\uff0c\u8ba9\u6211\u4eec\u73b0\u5728\u5c31\u5f00\u59cb\u5427","title":"Plugins"},{"location":"cn/first_steps/plugins/#creating-a-project","text":"\u9879\u76ee\u540d\u79f0\u4e3a\uff1a ruia-ua \uff0c\u56e0\u4e3a Ruia \u57fa\u4e8e Python3.6+ \uff0c\u6240\u4ee5\u6269\u5c55 ruia-ua \u4e5f\u4ea6\u7136\uff0c\u5047\u8bbe\u4f60\u6b64\u65f6\u4f7f\u7528\u7684\u662f Python3.6+ \uff0c\u8bf7\u6309\u7167\u5982\u4e0b\u64cd\u4f5c\uff1a # \u5b89\u88c5\u5305\u7ba1\u7406\u5de5\u5177 pipenv pip install pipenv # \u521b\u5efa\u9879\u76ee\u6587\u4ef6\u5939 mkdir ruia-ua cd ruia-ua # \u5b89\u88c5\u865a\u62df\u73af\u5883 pipenv install # \u5b89\u88c5 ruia pipenv install ruia # \u5b89\u88c5 aiofiles pipenv install aiofiles # \u521b\u5efa\u9879\u76ee\u76ee\u5f55 mkdir ruia_ua cd ruia_ua # \u5b9e\u73b0\u4ee3\u7801\u653e\u5728\u8fd9\u91cc touch __init__.py \u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a ruia-ua \u251c\u2500\u2500 LICENSE # \u5f00\u6e90\u534f\u8bae \u251c\u2500\u2500 Pipfile # pipenv \u7ba1\u7406\u5de5\u5177\u751f\u6210\u6587\u4ef6 \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # \u4ee3\u7801\u5b9e\u73b0 \u2502 \u2514\u2500\u2500 user_agents.txt # \u968f\u673aua\u96c6\u5408 \u2514\u2500\u2500 setup.py","title":"Creating a project"},{"location":"cn/first_steps/plugins/#first-extension","text":"user_agents.txt \u6587\u4ef6\u5305\u542b\u4e86\u5404\u79cd ua \uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u53ea\u8981\u5229\u7528 ruia \u7684 Middleware \u5b9e\u73b0\u5728\u6bcf\u6b21\u8bf7\u6c42\u524d\u968f\u673a\u6dfb\u52a0\u4e00\u4e2a User-Agent \u5373\u53ef\uff0c\u5b9e\u73b0\u4ee3\u7801\u5982\u4e0b\uff1a import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware.request async def add_random_ua ( request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } \u7f16\u5199\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5c06 ruia-ua \u4e0a\u4f20\u81f3\u793e\u533a\uff0c\u8fd9\u6837\u6240\u6709\u7684 ruia \u4f7f\u7528\u8005\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u4f60\u7f16\u5199\u7684\u7b2c\u4e09\u65b9\u6269\u5c55\uff0c\u591a\u4e48\u7f8e\u597d\u7684\u4e00\u4ef6\u4e8b","title":"First extension"},{"location":"cn/first_steps/plugins/#usage","text":"\u6240\u6709\u7684\u722c\u866b\u7a0b\u5e8f\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 ruia-ua \u6765\u5b9e\u73b0\u81ea\u52a8\u6dfb\u52a0 User-Agent pip install ruia - ua \u4e3e\u4e2a\u5b9e\u9645\u4f7f\u7528\u7684\u4f8b\u5b50\uff1a from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) \u7b2c\u4e09\u65b9\u6269\u5c55\u7684\u5b9e\u73b0\u5c06\u4f1a\u5927\u5927\u51cf\u5c11\u722c\u866b\u5de5\u7a0b\u5e08\u7684\u5f00\u53d1\u5468\u671f\uff0c ruia \u975e\u5e38\u5e0c\u671b\u4f60\u53ef\u4ee5\u5f00\u53d1\u5e76\u63d0\u4ea4\u81ea\u5df1\u7684\u7b2c\u4e09\u65b9\u6269\u5c55","title":"Usage"},{"location":"cn/first_steps/tutorials/","text":"Tutorials \u76ee\u6807\uff1a\u901a\u8fc7\u5bf9 Hacker News \u7684\u722c\u53d6\u6765\u5c55\u793a\u5982\u4f55\u4f7f\u7528 ruia \uff0c\u4e0b\u56fe\u7ea2\u6846\u4e2d\u7684\u6570\u636e\u5c31\u662f\u6211\u4eec\u9700\u8981\u722c\u53d6\u7684\uff1a \u5047\u8bbe\u6211\u4eec\u5c06\u6b64\u9879\u76ee\u547d\u540d\u4e3a hacker_news_spider \uff0c\u9879\u76ee\u7ed3\u6784\u5982\u4e0b\uff1a hacker_news_spider \u251c\u2500\u2500 db.py \u251c\u2500\u2500 hacker_news.py \u251c\u2500\u2500 items.py \u2514\u2500\u2500 middlewares.py Item Item \u7684\u76ee\u7684\u662f\u5b9a\u4e49\u76ee\u6807\u7f51\u7ad9\u4e2d\u4f60\u9700\u8981\u722c\u53d6\u7684\u6570\u636e\uff0c\u6b64\u65f6\uff0c\u722c\u866b\u7684\u76ee\u6807\u6570\u636e\u5c31\u662f\u9875\u9762\u4e2d\u7684 Title \u548c Url \uff0c\u600e\u4e48\u63d0\u53d6\u6570\u636e\uff0c ruia \u63d0\u4f9b\u4e86 CSS Selector \u548c XPath \u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e Notice: \u540e\u7eed\u722c\u866b\u4f8b\u5b50\u90fd\u9ed8\u8ba4\u4f7f\u7528CSS Selector\u7684\u89c4\u5219\u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 CSS Selector \u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u7528\u6d4f\u89c8\u5668\u6253\u5f00 Hacker News \uff0c\u53f3\u952e\u5ba1\u67e5\u5143\u7d20\uff1a \u663e\u800c\u6613\u89c1\uff0c\u6bcf\u9875\u5305\u542b 30 \u6761\u8d44\u8baf\uff0c\u90a3\u4e48\u76ee\u6807\u6570\u636e\u7684\u89c4\u5219\u53ef\u4ee5\u603b\u7ed3\u4e3a\uff1a Param Rule Description target_item tr.athing \u8868\u793a\u6bcf\u6761\u8d44\u8baf title a.storylink \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u7684\u6807\u9898 url a.storylink->href \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u6807\u9898\u7684\u94fe\u63a5 \u89c4\u5219\u660e\u786e\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u7528 Item \u6765\u5b9e\u73b0\u4e00\u4e2a\u9488\u5bf9\u4e8e\u76ee\u6807\u6570\u636e\u7684ORM\uff0c\u521b\u5efa\u6587\u4ef6 items.py \uff0c\u590d\u5236\u4e0b\u9762\u4ee3\u7801\uff1a from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) \u53ea\u9700\u8981\u7ee7\u627f Item \u7c7b\uff0c\u5c06\u76ee\u6807\u53c2\u6570\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5c5e\u6027\u5373\u53ef\uff0c\u5982\u679c\u76ee\u6807\u6570\u636e\u662f\u53ef\u4ee5\u5faa\u73af\u63d0\u53d6\u7684\uff0c\u6bd4\u5982\u6b64\u65f6\u6bcf\u4e00\u9875\u91cc\u9762\u6709 30 \u6761\u6570\u636e\uff0c\u90a3\u4e48\u5c31\u9700\u8981\u5b9a\u4e49 target_item \u6765\u5faa\u73af\u63d0\u53d6\u6bcf\u4e00\u6761\u6570\u636e\u91cc\u9762\u7684 Title \u548c Url Middleware Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u5206\u4e0b\u9762\u4e24\u79cd\u60c5\u51b5\uff1a \u5728\u6bcf\u6b21\u8bf7\u6c42\u4e4b\u524d\u505a\u4e00\u4e9b\u4e8b \u5728\u6bcf\u6b21\u8bf7\u6c42\u540e\u505a\u4e00\u4e9b\u4e8b \u6bd4\u5982\u6b64\u65f6\u722c\u53d6 Hacker News \uff0c\u4f60\u5e0c\u671b\u5728\u6bcf\u6b21\u8bf7\u6c42\u65f6\u5019\u81ea\u52a8\u6dfb\u52a0 Headers \u7684 User-Agent \uff0c\u53ef\u4ee5\u5c06\u4e0b\u9762\u4ee3\u7801\u590d\u5236\u5230\u4f60\u5efa\u7acb\u7684 middlewares.py \u6587\u4ef6\u4e2d\uff1a from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): ua = 'ruia user-agent' request . headers . update ({ 'User-Agent' : ua }) \u8fd9\u6837\uff0c\u7a0b\u5e8f\u4f1a\u5728\u722c\u866b\u8bf7\u6c42\u7f51\u9875\u8d44\u6e90\u4e4b\u524d\u81ea\u52a8\u52a0\u4e0a User-Agent Database \u5bf9\u4e8e\u6570\u636e\u6301\u4e45\u5316\uff0c\u4f60\u53ef\u4ee5\u6309\u7167\u81ea\u5df1\u559c\u6b22\u7684\u65b9\u5f0f\u53bb\u505a\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u4ee5 MongoDB \u4e3a\u4f8b\u5bf9\u722c\u53d6\u7684\u6570\u636e\u8fdb\u884c\u5b58\u50a8\uff0c\u521b\u5efa db.py \u6587\u4ef6\uff1a import asyncio from motor.motor_asyncio import AsyncIOMotorClient class MotorBase : \"\"\" About motor's doc: https://github.com/mongodb/motor \"\"\" _db = {} _collection = {} def __init__ ( self , loop = None ): self . motor_uri = '' self . loop = loop or asyncio . get_event_loop () def client ( self , db ): # motor self . motor_uri = f \"mongodb://localhost:27017/{db}\" return AsyncIOMotorClient ( self . motor_uri , io_loop = self . loop ) def get_db ( self , db = 'test' ): \"\"\" Get a db instance :param db: database name :return: the motor db instance \"\"\" if db not in self . _db : self . _db [ db ] = self . client ( db )[ db ] return self . _db [ db ] Spider Spider \u53ef\u4ee5\u8bf4\u662f\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3\uff0c\u5b83\u5c06 Item \u3001 Middleware \u3001 Request \u3001\u7b49\u6a21\u5757\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u4e3a\u4f60\u6784\u9020\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b\u7a0b\u5e8f \u8fd9\u6b21\u7684\u76ee\u7684\u4ec5\u4ec5\u662f\u4e3a\u4e86\u6f14\u793a\u5982\u4f55\u4f7f\u7528 ruia \u7f16\u5199\u722c\u866b\uff0c\u6240\u4ee5\u8fd9\u4e2a\u4f8b\u5b50\u4ec5\u4ec5\u722c\u53d6 Hacker News \u7684\u524d\u4e24\u9875\u6570\u636e\uff0c\u521b\u5efa hacker_news.py \u6587\u4ef6\uff1a from ruia import Request , Spider from items import HackerNewsItem from middlewares import middleware from db import MotorBase class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com' ] concurrency = 3 async def parse ( self , res ): self . mongo_db = MotorBase () . get_db ( 'ruia_test' ) urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] for index , url in enumerate ( urls ): yield Request ( url , callback = self . parse_item , metadata = { 'index' : index } ) async def parse_item ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : try : await self . mongo_db . news . update_one ({ 'url' : item . url }, { '$set' : { 'url' : item . url , 'title' : item . title }}, upsert = True ) except Exception as e : self . logger . exception ( e ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) HackerNewsSpider \u7ee7\u627f\u4e8e Spider \u7c7b\uff0c\u5176\u4e2d\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0 parse() \u65b9\u6cd5\uff0c\u8fd0\u884c python hacker_news.py \uff1a [2018-09-24 17:59:19,865]-ruia-INFO spider : Spider started! [2018-09-24 17:59:19,866]-Request-INFO request: <GET: https://news.ycombinator.com> [2018-09-24 17:59:23,259]-Request-INFO request: <GET: https://news.ycombinator.com/news?p=1> [2018-09-24 17:59:23,260]-Request-INFO request: <GET: https://news.ycombinator.com/news?p=2> [2018-09-24 18:03:05,562]-ruia-INFO spider : Stopping spider: ruia [2018-09-24 18:03:05,562]-ruia-INFO spider : Total requests: 3 [2018-09-24 18:03:05,562]-ruia-INFO spider : Time usage: 0:00:02.802862 [2018-09-24 18:03:05,562]-ruia-INFO spider : Spider finished! \u6570\u636e\u5e93\u4e2d\u53ef\u4ee5\u770b\u5230\uff1a \u901a\u8fc7\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u4f60\u5df2\u7ecf\u57fa\u672c\u638c\u63e1\u4e86 ruia \u7684 Item \u3001 Middleware \u3001 Request \u7b49\u6a21\u5757\u7684\u7528\u6cd5\uff0c\u7ed3\u5408\u81ea\u8eab\u9700\u6c42\uff0c\u4f60\u53ef\u4ee5\u7f16\u5199\u4efb\u4f55\u722c\u866b\uff0c\u4f8b\u5b50\u4ee3\u7801\u89c1 hacker_news_spider \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u7ed3\u5408\u5b9e\u4f8b\uff0c\u7f16\u5199\u4e00\u4e2a ruia \u7684\u7b2c\u4e09\u65b9\u6269\u5c55\uff0c\u8be6\u89c1\uff1a Plugins","title":"Tutorials"},{"location":"cn/first_steps/tutorials/#tutorials","text":"\u76ee\u6807\uff1a\u901a\u8fc7\u5bf9 Hacker News \u7684\u722c\u53d6\u6765\u5c55\u793a\u5982\u4f55\u4f7f\u7528 ruia \uff0c\u4e0b\u56fe\u7ea2\u6846\u4e2d\u7684\u6570\u636e\u5c31\u662f\u6211\u4eec\u9700\u8981\u722c\u53d6\u7684\uff1a \u5047\u8bbe\u6211\u4eec\u5c06\u6b64\u9879\u76ee\u547d\u540d\u4e3a hacker_news_spider \uff0c\u9879\u76ee\u7ed3\u6784\u5982\u4e0b\uff1a hacker_news_spider \u251c\u2500\u2500 db.py \u251c\u2500\u2500 hacker_news.py \u251c\u2500\u2500 items.py \u2514\u2500\u2500 middlewares.py","title":"Tutorials"},{"location":"cn/first_steps/tutorials/#item","text":"Item \u7684\u76ee\u7684\u662f\u5b9a\u4e49\u76ee\u6807\u7f51\u7ad9\u4e2d\u4f60\u9700\u8981\u722c\u53d6\u7684\u6570\u636e\uff0c\u6b64\u65f6\uff0c\u722c\u866b\u7684\u76ee\u6807\u6570\u636e\u5c31\u662f\u9875\u9762\u4e2d\u7684 Title \u548c Url \uff0c\u600e\u4e48\u63d0\u53d6\u6570\u636e\uff0c ruia \u63d0\u4f9b\u4e86 CSS Selector \u548c XPath \u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e Notice: \u540e\u7eed\u722c\u866b\u4f8b\u5b50\u90fd\u9ed8\u8ba4\u4f7f\u7528CSS Selector\u7684\u89c4\u5219\u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 CSS Selector \u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u7528\u6d4f\u89c8\u5668\u6253\u5f00 Hacker News \uff0c\u53f3\u952e\u5ba1\u67e5\u5143\u7d20\uff1a \u663e\u800c\u6613\u89c1\uff0c\u6bcf\u9875\u5305\u542b 30 \u6761\u8d44\u8baf\uff0c\u90a3\u4e48\u76ee\u6807\u6570\u636e\u7684\u89c4\u5219\u53ef\u4ee5\u603b\u7ed3\u4e3a\uff1a Param Rule Description target_item tr.athing \u8868\u793a\u6bcf\u6761\u8d44\u8baf title a.storylink \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u7684\u6807\u9898 url a.storylink->href \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u6807\u9898\u7684\u94fe\u63a5 \u89c4\u5219\u660e\u786e\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u7528 Item \u6765\u5b9e\u73b0\u4e00\u4e2a\u9488\u5bf9\u4e8e\u76ee\u6807\u6570\u636e\u7684ORM\uff0c\u521b\u5efa\u6587\u4ef6 items.py \uff0c\u590d\u5236\u4e0b\u9762\u4ee3\u7801\uff1a from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) \u53ea\u9700\u8981\u7ee7\u627f Item \u7c7b\uff0c\u5c06\u76ee\u6807\u53c2\u6570\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5c5e\u6027\u5373\u53ef\uff0c\u5982\u679c\u76ee\u6807\u6570\u636e\u662f\u53ef\u4ee5\u5faa\u73af\u63d0\u53d6\u7684\uff0c\u6bd4\u5982\u6b64\u65f6\u6bcf\u4e00\u9875\u91cc\u9762\u6709 30 \u6761\u6570\u636e\uff0c\u90a3\u4e48\u5c31\u9700\u8981\u5b9a\u4e49 target_item \u6765\u5faa\u73af\u63d0\u53d6\u6bcf\u4e00\u6761\u6570\u636e\u91cc\u9762\u7684 Title \u548c Url","title":"Item"},{"location":"cn/first_steps/tutorials/#middleware","text":"Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u5206\u4e0b\u9762\u4e24\u79cd\u60c5\u51b5\uff1a \u5728\u6bcf\u6b21\u8bf7\u6c42\u4e4b\u524d\u505a\u4e00\u4e9b\u4e8b \u5728\u6bcf\u6b21\u8bf7\u6c42\u540e\u505a\u4e00\u4e9b\u4e8b \u6bd4\u5982\u6b64\u65f6\u722c\u53d6 Hacker News \uff0c\u4f60\u5e0c\u671b\u5728\u6bcf\u6b21\u8bf7\u6c42\u65f6\u5019\u81ea\u52a8\u6dfb\u52a0 Headers \u7684 User-Agent \uff0c\u53ef\u4ee5\u5c06\u4e0b\u9762\u4ee3\u7801\u590d\u5236\u5230\u4f60\u5efa\u7acb\u7684 middlewares.py \u6587\u4ef6\u4e2d\uff1a from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): ua = 'ruia user-agent' request . headers . update ({ 'User-Agent' : ua }) \u8fd9\u6837\uff0c\u7a0b\u5e8f\u4f1a\u5728\u722c\u866b\u8bf7\u6c42\u7f51\u9875\u8d44\u6e90\u4e4b\u524d\u81ea\u52a8\u52a0\u4e0a User-Agent","title":"Middleware"},{"location":"cn/first_steps/tutorials/#database","text":"\u5bf9\u4e8e\u6570\u636e\u6301\u4e45\u5316\uff0c\u4f60\u53ef\u4ee5\u6309\u7167\u81ea\u5df1\u559c\u6b22\u7684\u65b9\u5f0f\u53bb\u505a\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u4ee5 MongoDB \u4e3a\u4f8b\u5bf9\u722c\u53d6\u7684\u6570\u636e\u8fdb\u884c\u5b58\u50a8\uff0c\u521b\u5efa db.py \u6587\u4ef6\uff1a import asyncio from motor.motor_asyncio import AsyncIOMotorClient class MotorBase : \"\"\" About motor's doc: https://github.com/mongodb/motor \"\"\" _db = {} _collection = {} def __init__ ( self , loop = None ): self . motor_uri = '' self . loop = loop or asyncio . get_event_loop () def client ( self , db ): # motor self . motor_uri = f \"mongodb://localhost:27017/{db}\" return AsyncIOMotorClient ( self . motor_uri , io_loop = self . loop ) def get_db ( self , db = 'test' ): \"\"\" Get a db instance :param db: database name :return: the motor db instance \"\"\" if db not in self . _db : self . _db [ db ] = self . client ( db )[ db ] return self . _db [ db ]","title":"Database"},{"location":"cn/first_steps/tutorials/#spider","text":"Spider \u53ef\u4ee5\u8bf4\u662f\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3\uff0c\u5b83\u5c06 Item \u3001 Middleware \u3001 Request \u3001\u7b49\u6a21\u5757\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u4e3a\u4f60\u6784\u9020\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b\u7a0b\u5e8f \u8fd9\u6b21\u7684\u76ee\u7684\u4ec5\u4ec5\u662f\u4e3a\u4e86\u6f14\u793a\u5982\u4f55\u4f7f\u7528 ruia \u7f16\u5199\u722c\u866b\uff0c\u6240\u4ee5\u8fd9\u4e2a\u4f8b\u5b50\u4ec5\u4ec5\u722c\u53d6 Hacker News \u7684\u524d\u4e24\u9875\u6570\u636e\uff0c\u521b\u5efa hacker_news.py \u6587\u4ef6\uff1a from ruia import Request , Spider from items import HackerNewsItem from middlewares import middleware from db import MotorBase class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com' ] concurrency = 3 async def parse ( self , res ): self . mongo_db = MotorBase () . get_db ( 'ruia_test' ) urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] for index , url in enumerate ( urls ): yield Request ( url , callback = self . parse_item , metadata = { 'index' : index } ) async def parse_item ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : try : await self . mongo_db . news . update_one ({ 'url' : item . url }, { '$set' : { 'url' : item . url , 'title' : item . title }}, upsert = True ) except Exception as e : self . logger . exception ( e ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) HackerNewsSpider \u7ee7\u627f\u4e8e Spider \u7c7b\uff0c\u5176\u4e2d\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0 parse() \u65b9\u6cd5\uff0c\u8fd0\u884c python hacker_news.py \uff1a [2018-09-24 17:59:19,865]-ruia-INFO spider : Spider started! [2018-09-24 17:59:19,866]-Request-INFO request: <GET: https://news.ycombinator.com> [2018-09-24 17:59:23,259]-Request-INFO request: <GET: https://news.ycombinator.com/news?p=1> [2018-09-24 17:59:23,260]-Request-INFO request: <GET: https://news.ycombinator.com/news?p=2> [2018-09-24 18:03:05,562]-ruia-INFO spider : Stopping spider: ruia [2018-09-24 18:03:05,562]-ruia-INFO spider : Total requests: 3 [2018-09-24 18:03:05,562]-ruia-INFO spider : Time usage: 0:00:02.802862 [2018-09-24 18:03:05,562]-ruia-INFO spider : Spider finished! \u6570\u636e\u5e93\u4e2d\u53ef\u4ee5\u770b\u5230\uff1a \u901a\u8fc7\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u4f60\u5df2\u7ecf\u57fa\u672c\u638c\u63e1\u4e86 ruia \u7684 Item \u3001 Middleware \u3001 Request \u7b49\u6a21\u5757\u7684\u7528\u6cd5\uff0c\u7ed3\u5408\u81ea\u8eab\u9700\u6c42\uff0c\u4f60\u53ef\u4ee5\u7f16\u5199\u4efb\u4f55\u722c\u866b\uff0c\u4f8b\u5b50\u4ee3\u7801\u89c1 hacker_news_spider \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u7ed3\u5408\u5b9e\u4f8b\uff0c\u7f16\u5199\u4e00\u4e2a ruia \u7684\u7b2c\u4e09\u65b9\u6269\u5c55\uff0c\u8be6\u89c1\uff1a Plugins","title":"Spider"},{"location":"cn/topics/item/","text":"Item Item \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5b9a\u4e49\u4ee5\u53ca\u901a\u8fc7\u4e00\u5b9a\u7684\u89c4\u5219\u63d0\u53d6\u6e90\u7f51\u9875\u4e2d\u7684\u76ee\u6807\u6570\u636e\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4e00\u4e0b\u4e24\u4e2a\u65b9\u6cd5\uff1a - get_item \uff1a\u9488\u5bf9\u9875\u9762\u5355\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6 - get_items \uff1a\u9488\u5bf9\u9875\u9762\u591a\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6 Core arguments get_item \u548c get_items \u65b9\u6cd5\u63a5\u6536\u7684\u53c2\u6570\u662f\u4e00\u81f4\u7684\uff1a - html\uff1a\u7f51\u9875\u6e90\u7801 - url\uff1a\u7f51\u9875\u94fe\u63a5 - html_etree\uff1aetree._Element\u5bf9\u8c61 Usage \u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c\u4e0d\u8bba\u662f\u6e90\u7f51\u7ad9\u94fe\u63a5\u6216\u8005\u7f51\u7ad9 HTML \u6e90\u7801\uff0c\u751a\u81f3\u662f\u7ecf\u8fc7 lxml \u5904\u7406\u8fc7\u7684 etree._Element \u5bf9\u8c61\uff0c Item \u80fd\u63a5\u6536\u8fd9\u4e09\u79cd\u7c7b\u578b\u7684\u8f93\u5165\u5e76\u8fdb\u884c\u5904\u7406 import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async_func = HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ) items = asyncio . get_event_loop () . run_until_complete ( async_func ) for item in items : print ( item . title , item . url ) \u6709\u65f6\u4f60\u4f1a\u9047\u89c1\u8fd9\u6837\u4e00\u79cd\u60c5\u51b5\uff0c\u4f8b\u5982\u722c\u53d6Github\u7684Issue\u65f6\uff0c\u4f60\u4f1a\u53d1\u73b0\u4e00\u4e2aIssue\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2aTag\u3002 \u8fd9\u65f6\uff0c\u5c06Tag\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684 Item \u6765\u63d0\u53d6\u662f\u4e0d\u5212\u7b97\u7684\uff0c \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 Field \u5b57\u6bb5\u7684 many=True \u53c2\u6570\uff0c\u4f7f\u8fd9\u4e2a\u5b57\u6bb5\u8fd4\u56de\u4e00\u4e2a\u5217\u8868\u3002 import asyncio from ruia import Item , TextField , AttrField class GithiubIssueItem ( Item ): title = TextField ( css_select = 'title' ) tags = AttrField ( css_select = 'a.IssueLabel' , attr = 'data-name' , many = True ) item = asyncio . run ( GithiubIssueItem . get_item ( url = 'https://github.com/pypa/pip/issues/72' )) assert isinstance ( item . tags , list ) \u540c\u6837\uff0c TextField \u4e5f\u652f\u6301 many \u53c2\u6570\u3002 How It Works? \u6700\u7ec8 Item \u7c7b\u4f1a\u5c06\u8f93\u5165\u6700\u7ec8\u8f6c\u5316\u4e3a etree._Element \u5bf9\u8c61\u8fdb\u884c\u5904\u7406\uff0c\u7136\u540e\u5229\u7528\u5143\u7c7b\u7684\u601d\u60f3\u5c06\u6bcf\u4e00\u4e2a Field \u6784\u9020\u7684\u5c5e\u6027\u8ba1\u7b97\u4e3a\u6e90\u7f51\u9875\u4e0a\u5bf9\u5e94\u7684\u771f\u5b9e\u6570\u636e","title":"Item"},{"location":"cn/topics/item/#item","text":"Item \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5b9a\u4e49\u4ee5\u53ca\u901a\u8fc7\u4e00\u5b9a\u7684\u89c4\u5219\u63d0\u53d6\u6e90\u7f51\u9875\u4e2d\u7684\u76ee\u6807\u6570\u636e\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4e00\u4e0b\u4e24\u4e2a\u65b9\u6cd5\uff1a - get_item \uff1a\u9488\u5bf9\u9875\u9762\u5355\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6 - get_items \uff1a\u9488\u5bf9\u9875\u9762\u591a\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6","title":"Item"},{"location":"cn/topics/item/#core-arguments","text":"get_item \u548c get_items \u65b9\u6cd5\u63a5\u6536\u7684\u53c2\u6570\u662f\u4e00\u81f4\u7684\uff1a - html\uff1a\u7f51\u9875\u6e90\u7801 - url\uff1a\u7f51\u9875\u94fe\u63a5 - html_etree\uff1aetree._Element\u5bf9\u8c61","title":"Core arguments"},{"location":"cn/topics/item/#usage","text":"\u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c\u4e0d\u8bba\u662f\u6e90\u7f51\u7ad9\u94fe\u63a5\u6216\u8005\u7f51\u7ad9 HTML \u6e90\u7801\uff0c\u751a\u81f3\u662f\u7ecf\u8fc7 lxml \u5904\u7406\u8fc7\u7684 etree._Element \u5bf9\u8c61\uff0c Item \u80fd\u63a5\u6536\u8fd9\u4e09\u79cd\u7c7b\u578b\u7684\u8f93\u5165\u5e76\u8fdb\u884c\u5904\u7406 import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async_func = HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ) items = asyncio . get_event_loop () . run_until_complete ( async_func ) for item in items : print ( item . title , item . url ) \u6709\u65f6\u4f60\u4f1a\u9047\u89c1\u8fd9\u6837\u4e00\u79cd\u60c5\u51b5\uff0c\u4f8b\u5982\u722c\u53d6Github\u7684Issue\u65f6\uff0c\u4f60\u4f1a\u53d1\u73b0\u4e00\u4e2aIssue\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2aTag\u3002 \u8fd9\u65f6\uff0c\u5c06Tag\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684 Item \u6765\u63d0\u53d6\u662f\u4e0d\u5212\u7b97\u7684\uff0c \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 Field \u5b57\u6bb5\u7684 many=True \u53c2\u6570\uff0c\u4f7f\u8fd9\u4e2a\u5b57\u6bb5\u8fd4\u56de\u4e00\u4e2a\u5217\u8868\u3002 import asyncio from ruia import Item , TextField , AttrField class GithiubIssueItem ( Item ): title = TextField ( css_select = 'title' ) tags = AttrField ( css_select = 'a.IssueLabel' , attr = 'data-name' , many = True ) item = asyncio . run ( GithiubIssueItem . get_item ( url = 'https://github.com/pypa/pip/issues/72' )) assert isinstance ( item . tags , list ) \u540c\u6837\uff0c TextField \u4e5f\u652f\u6301 many \u53c2\u6570\u3002","title":"Usage"},{"location":"cn/topics/item/#how-it-works","text":"\u6700\u7ec8 Item \u7c7b\u4f1a\u5c06\u8f93\u5165\u6700\u7ec8\u8f6c\u5316\u4e3a etree._Element \u5bf9\u8c61\u8fdb\u884c\u5904\u7406\uff0c\u7136\u540e\u5229\u7528\u5143\u7c7b\u7684\u601d\u60f3\u5c06\u6bcf\u4e00\u4e2a Field \u6784\u9020\u7684\u5c5e\u6027\u8ba1\u7b97\u4e3a\u6e90\u7f51\u9875\u4e0a\u5bf9\u5e94\u7684\u771f\u5b9e\u6570\u636e","title":"How It Works?"},{"location":"cn/topics/middleware/","text":"Middleware Middleware \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5728\u8fdb\u884c\u4e00\u4e2a\u8bf7\u6c42\u7684\u524d\u540e\u8fdb\u884c\u4e00\u4e9b\u5904\u7406\uff0c\u6bd4\u5982\u76d1\u542c\u8bf7\u6c42\u6216\u8005\u54cd\u5e94\uff1a - Middleware().request \uff1a\u5728\u8bf7\u6c42\u524d\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5 - Middleware().response \uff1a\u5728\u8bf7\u6c42\u540e\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5 Usage \u4f7f\u7528\u4e2d\u95f4\u4ef6\u6709\u4e24\u70b9\u9700\u8981\u6ce8\u610f\uff0c\u4e00\u4e2a\u662f\u5904\u7406\u51fd\u6570\u9700\u8981\u5e26\u4e0a\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a\u662f\u4e0d\u9700\u8981\u8fd4\u56de\u503c\uff0c\u5177\u4f53\u4f7f\u7528\u5982\u4e0b\uff1a from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u524d\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"request: print when a request is received\" ) @middleware.response async def print_on_response ( request , response ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u540e\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 response: Response\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"response: print when a response is received\" ) How It Works? Middleware \u901a\u8fc7\u88c5\u9970\u5668\u6765\u5b9e\u73b0\u5bf9\u51fd\u6570\u7684\u56de\u8c03\uff0c\u4ece\u800c\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u4f18\u96c5\u7684\u5b9e\u73b0\u4e2d\u95f4\u4ef6\u529f\u80fd\uff0c Middleware \u7c7b\u4e2d\u7684\u4e24\u4e2a\u5c5e\u6027 request_middleware \u548c response_middleware \u5206\u522b\u7ef4\u62a4\u7740\u4e00\u4e2a\u961f\u5217\u6765\u5904\u7406\u5f00\u53d1\u8005\u5b9a\u4e49\u7684\u5904\u7406\u51fd\u6570","title":"Middleware"},{"location":"cn/topics/middleware/#middleware","text":"Middleware \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5728\u8fdb\u884c\u4e00\u4e2a\u8bf7\u6c42\u7684\u524d\u540e\u8fdb\u884c\u4e00\u4e9b\u5904\u7406\uff0c\u6bd4\u5982\u76d1\u542c\u8bf7\u6c42\u6216\u8005\u54cd\u5e94\uff1a - Middleware().request \uff1a\u5728\u8bf7\u6c42\u524d\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5 - Middleware().response \uff1a\u5728\u8bf7\u6c42\u540e\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5","title":"Middleware"},{"location":"cn/topics/middleware/#usage","text":"\u4f7f\u7528\u4e2d\u95f4\u4ef6\u6709\u4e24\u70b9\u9700\u8981\u6ce8\u610f\uff0c\u4e00\u4e2a\u662f\u5904\u7406\u51fd\u6570\u9700\u8981\u5e26\u4e0a\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a\u662f\u4e0d\u9700\u8981\u8fd4\u56de\u503c\uff0c\u5177\u4f53\u4f7f\u7528\u5982\u4e0b\uff1a from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u524d\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"request: print when a request is received\" ) @middleware.response async def print_on_response ( request , response ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u540e\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 response: Response\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"response: print when a response is received\" )","title":"Usage"},{"location":"cn/topics/middleware/#how-it-works","text":"Middleware \u901a\u8fc7\u88c5\u9970\u5668\u6765\u5b9e\u73b0\u5bf9\u51fd\u6570\u7684\u56de\u8c03\uff0c\u4ece\u800c\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u4f18\u96c5\u7684\u5b9e\u73b0\u4e2d\u95f4\u4ef6\u529f\u80fd\uff0c Middleware \u7c7b\u4e2d\u7684\u4e24\u4e2a\u5c5e\u6027 request_middleware \u548c response_middleware \u5206\u522b\u7ef4\u62a4\u7740\u4e00\u4e2a\u961f\u5217\u6765\u5904\u7406\u5f00\u53d1\u8005\u5b9a\u4e49\u7684\u5904\u7406\u51fd\u6570","title":"How It Works?"},{"location":"cn/topics/request/","text":"Request Request \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u65b9\u4fbf\u5730\u5904\u7406\u7f51\u7edc\u8bf7\u6c42\uff0c\u6700\u7ec8\u8fd4\u56de\u4e00\u4e2a Response \u5bf9\u8c61\u3002 \u4e3b\u8981\u63d0\u4f9b\u7684\u65b9\u6cd5\u6709\uff1a - Request().fetch \uff1a\u8bf7\u6c42\u4e00\u4e2a\u7f51\u9875\u8d44\u6e90\uff0c\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528 - Request().fetch_callback \uff1a\u4e3a Spider \u7c7b\u63d0\u4f9b\u7684\u548c\u6838\u5fc3\u65b9\u6cd5 Core arguments url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 method\uff1a\u8bf7\u6c42\u7684\u65b9\u6cd5\uff0c GET \u6216\u8005 POST callback\uff1a\u56de\u8c03\u51fd\u6570 headers\uff1a\u8bf7\u6c42\u5934 load_js\uff1a\u76ee\u6807\u7f51\u9875\u662f\u5426\u9700\u8981\u52a0\u8f7djs metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e request_config\uff1a\u8bf7\u6c42\u914d\u7f6e request_session\uff1a aiohttp \u7684\u8bf7\u6c42session res_type\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u7c7b\u578b\uff0c\u9ed8\u8ba4\u4e3a str \uff0c\u53ef\u4ee5\u9009\u62e9 bytes \u6216 json kwargs\uff1a\u8bf7\u6c42\u76ee\u6807\u8d44\u6e90\u53ef\u5b9a\u4e49\u7684\u5176\u4ed6\u53c2\u6570 Usage \u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c Request \u9664\u4e86\u9700\u8981\u7ed3\u5408 Spider \u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\uff1a import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}> How It Works? Request \u901a\u8fc7\u5bf9 aiohttp \u548c pyppeteer \u7684\u5c01\u88c5\u6765\u5b9e\u73b0\u5bf9\u7f51\u9875\u8d44\u6e90\u7684\u5f02\u6b65\u8bf7\u6c42","title":"Request"},{"location":"cn/topics/request/#request","text":"Request \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u65b9\u4fbf\u5730\u5904\u7406\u7f51\u7edc\u8bf7\u6c42\uff0c\u6700\u7ec8\u8fd4\u56de\u4e00\u4e2a Response \u5bf9\u8c61\u3002 \u4e3b\u8981\u63d0\u4f9b\u7684\u65b9\u6cd5\u6709\uff1a - Request().fetch \uff1a\u8bf7\u6c42\u4e00\u4e2a\u7f51\u9875\u8d44\u6e90\uff0c\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528 - Request().fetch_callback \uff1a\u4e3a Spider \u7c7b\u63d0\u4f9b\u7684\u548c\u6838\u5fc3\u65b9\u6cd5","title":"Request"},{"location":"cn/topics/request/#core-arguments","text":"url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 method\uff1a\u8bf7\u6c42\u7684\u65b9\u6cd5\uff0c GET \u6216\u8005 POST callback\uff1a\u56de\u8c03\u51fd\u6570 headers\uff1a\u8bf7\u6c42\u5934 load_js\uff1a\u76ee\u6807\u7f51\u9875\u662f\u5426\u9700\u8981\u52a0\u8f7djs metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e request_config\uff1a\u8bf7\u6c42\u914d\u7f6e request_session\uff1a aiohttp \u7684\u8bf7\u6c42session res_type\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u7c7b\u578b\uff0c\u9ed8\u8ba4\u4e3a str \uff0c\u53ef\u4ee5\u9009\u62e9 bytes \u6216 json kwargs\uff1a\u8bf7\u6c42\u76ee\u6807\u8d44\u6e90\u53ef\u5b9a\u4e49\u7684\u5176\u4ed6\u53c2\u6570","title":"Core arguments"},{"location":"cn/topics/request/#usage","text":"\u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c Request \u9664\u4e86\u9700\u8981\u7ed3\u5408 Spider \u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\uff1a import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}>","title":"Usage"},{"location":"cn/topics/request/#how-it-works","text":"Request \u901a\u8fc7\u5bf9 aiohttp \u548c pyppeteer \u7684\u5c01\u88c5\u6765\u5b9e\u73b0\u5bf9\u7f51\u9875\u8d44\u6e90\u7684\u5f02\u6b65\u8bf7\u6c42","title":"How It Works?"},{"location":"cn/topics/response/","text":"Response Response \u7684\u76ee\u7684\u662f\u8fd4\u56de\u4e00\u4e2a\u7edf\u4e00\u4e14\u53cb\u597d\u7684\u54cd\u5e94\u5bf9\u8c61\uff0c\u4e3b\u8981\u5c5e\u6027\u5982\u4e0b\uff1a - url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 - metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e - res_type\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u7c7b\u578b\uff0c\u9ed8\u8ba4\u4e3a str \uff0c\u53ef\u4ee5\u9009\u62e9 bytes \u6216 json - html\uff1a\u6e90\u7f51\u7ad9\u8fd4\u56de\u7684\u8d44\u6e90\u6570\u636e - cookies\uff1a\u7f51\u7ad9 cookies - history\uff1a\u8bbf\u95ee\u5386\u53f2 - headers\uff1a\u8bf7\u6c42\u5934 - status\uff1a\u8bf7\u6c42\u72b6\u6001\u7801","title":"Response"},{"location":"cn/topics/response/#response","text":"Response \u7684\u76ee\u7684\u662f\u8fd4\u56de\u4e00\u4e2a\u7edf\u4e00\u4e14\u53cb\u597d\u7684\u54cd\u5e94\u5bf9\u8c61\uff0c\u4e3b\u8981\u5c5e\u6027\u5982\u4e0b\uff1a - url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 - metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e - res_type\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u7c7b\u578b\uff0c\u9ed8\u8ba4\u4e3a str \uff0c\u53ef\u4ee5\u9009\u62e9 bytes \u6216 json - html\uff1a\u6e90\u7f51\u7ad9\u8fd4\u56de\u7684\u8d44\u6e90\u6570\u636e - cookies\uff1a\u7f51\u7ad9 cookies - history\uff1a\u8bbf\u95ee\u5386\u53f2 - headers\uff1a\u8bf7\u6c42\u5934 - status\uff1a\u8bf7\u6c42\u72b6\u6001\u7801","title":"Response"},{"location":"cn/topics/selector/","text":"Selector Selector \u901a\u8fc7 Field \u7c7b\u5b9e\u73b0\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86 CSS Selector \u548c XPath \u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u5177\u4f53\u7531\u4e0b\u9762\u4e24\u4e2a\u7c7b\u5b9e\u73b0\uff1a - AttrField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684\u5c5e\u6027\u6570\u636e - TextField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684text\u6570\u636e Core arguments \u6240\u6709\u7684 Field \u5171\u6709\u7684\u53c2\u6570\uff1a - default: str, \u8bbe\u7f6e\u9ed8\u8ba4\u503c - many: bool, \u8fd4\u56de\u503c\u5c06\u662f\u4e00\u4e2a\u5217\u8868 AttrField \u3001 TextField \u3001 HtmlField \u5171\u7528\u53c2\u6570\uff1a - css_select\uff1astr, \u5229\u7528 CSS Selector \u63d0\u53d6\u76ee\u6807\u6570\u636e - xpath_select\uff1astr, \u5229\u7528 XPath \u63d0\u53d6\u76ee\u6807\u6570\u636e AttrField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - attr\uff1a\u76ee\u6807\u6807\u7b7e\u5c5e\u6027 RegexField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - re_select: str, \u6b63\u5219\u8868\u8fbe\u5f0f\u5b57\u7b26\u4e32 Usage from lxml import etree from ruia import AttrField , TextField , HtmlField , RegexField HTML = \"\"\" <html> <head> <title>ruia</title> </head> <body>\u00ac <p> <a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a> </p> </body> </html> \"\"\" html = etree . HTML ( HTML ) def test_css_select (): field = TextField ( css_select = \"head title\" ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_xpath_select (): field = TextField ( xpath_select = '/html/head/title' ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_attr_field (): attr_field = AttrField ( css_select = \"p a.test_link\" , attr = 'href' ) value = attr_field . extract ( html_etree = html ) assert value == \"https://github.com/howie6879/ruia\" def test_html_field (): field = HtmlField ( css_select = \"a.test_link\" ) assert field . extract ( html_etree = html ) == '<a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a>' def test_re_field (): field = RegexField ( re_select = '<title>(.*?)</title>' ) href = field . extract ( html = HTML ) assert href == 'ruia' How It Works? \u5b9a\u597d CSS Selector \u6216 XPath \u89c4\u5219\uff0c\u7136\u540e\u5229\u7528 lxml \u5b9e\u73b0\u5bf9\u76ee\u6807 html \u8fdb\u884c\u76ee\u6807\u6570\u636e\u7684\u63d0\u53d6 \u5173\u4e8e RegexField \u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u9605 \u82f1\u6587\u6587\u6863 \u3002","title":"Selector"},{"location":"cn/topics/selector/#selector","text":"Selector \u901a\u8fc7 Field \u7c7b\u5b9e\u73b0\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86 CSS Selector \u548c XPath \u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u5177\u4f53\u7531\u4e0b\u9762\u4e24\u4e2a\u7c7b\u5b9e\u73b0\uff1a - AttrField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684\u5c5e\u6027\u6570\u636e - TextField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684text\u6570\u636e","title":"Selector"},{"location":"cn/topics/selector/#core-arguments","text":"\u6240\u6709\u7684 Field \u5171\u6709\u7684\u53c2\u6570\uff1a - default: str, \u8bbe\u7f6e\u9ed8\u8ba4\u503c - many: bool, \u8fd4\u56de\u503c\u5c06\u662f\u4e00\u4e2a\u5217\u8868 AttrField \u3001 TextField \u3001 HtmlField \u5171\u7528\u53c2\u6570\uff1a - css_select\uff1astr, \u5229\u7528 CSS Selector \u63d0\u53d6\u76ee\u6807\u6570\u636e - xpath_select\uff1astr, \u5229\u7528 XPath \u63d0\u53d6\u76ee\u6807\u6570\u636e AttrField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - attr\uff1a\u76ee\u6807\u6807\u7b7e\u5c5e\u6027 RegexField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - re_select: str, \u6b63\u5219\u8868\u8fbe\u5f0f\u5b57\u7b26\u4e32","title":"Core arguments"},{"location":"cn/topics/selector/#usage","text":"from lxml import etree from ruia import AttrField , TextField , HtmlField , RegexField HTML = \"\"\" <html> <head> <title>ruia</title> </head> <body>\u00ac <p> <a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a> </p> </body> </html> \"\"\" html = etree . HTML ( HTML ) def test_css_select (): field = TextField ( css_select = \"head title\" ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_xpath_select (): field = TextField ( xpath_select = '/html/head/title' ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_attr_field (): attr_field = AttrField ( css_select = \"p a.test_link\" , attr = 'href' ) value = attr_field . extract ( html_etree = html ) assert value == \"https://github.com/howie6879/ruia\" def test_html_field (): field = HtmlField ( css_select = \"a.test_link\" ) assert field . extract ( html_etree = html ) == '<a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a>' def test_re_field (): field = RegexField ( re_select = '<title>(.*?)</title>' ) href = field . extract ( html = HTML ) assert href == 'ruia'","title":"Usage"},{"location":"cn/topics/selector/#how-it-works","text":"\u5b9a\u597d CSS Selector \u6216 XPath \u89c4\u5219\uff0c\u7136\u540e\u5229\u7528 lxml \u5b9e\u73b0\u5bf9\u76ee\u6807 html \u8fdb\u884c\u76ee\u6807\u6570\u636e\u7684\u63d0\u53d6","title":"How It Works?"},{"location":"cn/topics/selector/#regexfield","text":"\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u9605 \u82f1\u6587\u6587\u6863 \u3002","title":"\u5173\u4e8eRegexField"},{"location":"cn/topics/spider/","text":"Spider Spider \u662f\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3\uff0c\u5b83\u5c06Item\u3001Middleware\u3001Request\u3001\u7b49\u6a21\u5757\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u4e3a\u4f60\u6784\u9020\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b\u7a0b\u5e8f\u3002\u4f60\u53ea\u9700\u8981\u5173\u6ce8\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a - Spider.start \uff1a\u722c\u866b\u7684\u542f\u52a8\u51fd\u6570 - parse \uff1a\u722c\u866b\u7684\u7b2c\u4e00\u5c42\u89e3\u6790\u51fd\u6570\uff0c\u7ee7\u627f Spider \u7684\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0\u8fd9\u4e2a\u51fd\u6570 Core arguments Spider.start \u7684\u53c2\u6570\u5982\u4e0b\uff1a - after_start\uff1a\u722c\u866b\u542f\u52a8\u540e\u7684\u94a9\u5b50\u51fd\u6570 - before_stop\uff1a\u722c\u866b\u542f\u52a8\u524d\u7684\u94a9\u5b50\u51fd\u6570 - middleware\uff1a\u4e2d\u95f4\u4ef6\u7c7b\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a\u4e2d\u95f4\u4ef6 Middleware() \u5b9e\u4f8b\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u7ec4 Middleware() \u5b9e\u4f8b\u7ec4\u6210\u7684\u5217\u8868 - loop\uff1a\u4e8b\u4ef6\u5faa\u73af Usage import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( item . title + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () How It Works? Spider \u4f1a\u81ea\u52a8\u8bfb\u53d6 start_urls \u5217\u8868\u91cc\u9762\u7684\u8bf7\u6c42\u94fe\u63a5\uff0c\u7136\u540e\u7ef4\u62a4\u4e00\u4e2a\u5f02\u6b65\u961f\u5217\uff0c\u4f7f\u7528\u751f\u4ea7\u6d88\u8d39\u8005\u6a21\u5f0f\u8fdb\u884c\u722c\u53d6\uff0c\u722c\u866b\u7a0b\u5e8f\u4e00\u76f4\u5faa\u73af\u76f4\u5230\u6ca1\u6709\u8c03\u7528\u51fd\u6570\u4e3a\u6b62","title":"Spider"},{"location":"cn/topics/spider/#spider","text":"Spider \u662f\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3\uff0c\u5b83\u5c06Item\u3001Middleware\u3001Request\u3001\u7b49\u6a21\u5757\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u4e3a\u4f60\u6784\u9020\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b\u7a0b\u5e8f\u3002\u4f60\u53ea\u9700\u8981\u5173\u6ce8\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a - Spider.start \uff1a\u722c\u866b\u7684\u542f\u52a8\u51fd\u6570 - parse \uff1a\u722c\u866b\u7684\u7b2c\u4e00\u5c42\u89e3\u6790\u51fd\u6570\uff0c\u7ee7\u627f Spider \u7684\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0\u8fd9\u4e2a\u51fd\u6570","title":"Spider"},{"location":"cn/topics/spider/#core-arguments","text":"Spider.start \u7684\u53c2\u6570\u5982\u4e0b\uff1a - after_start\uff1a\u722c\u866b\u542f\u52a8\u540e\u7684\u94a9\u5b50\u51fd\u6570 - before_stop\uff1a\u722c\u866b\u542f\u52a8\u524d\u7684\u94a9\u5b50\u51fd\u6570 - middleware\uff1a\u4e2d\u95f4\u4ef6\u7c7b\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a\u4e2d\u95f4\u4ef6 Middleware() \u5b9e\u4f8b\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u7ec4 Middleware() \u5b9e\u4f8b\u7ec4\u6210\u7684\u5217\u8868 - loop\uff1a\u4e8b\u4ef6\u5faa\u73af","title":"Core arguments"},{"location":"cn/topics/spider/#usage","text":"import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( item . title + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start ()","title":"Usage"},{"location":"cn/topics/spider/#how-it-works","text":"Spider \u4f1a\u81ea\u52a8\u8bfb\u53d6 start_urls \u5217\u8868\u91cc\u9762\u7684\u8bf7\u6c42\u94fe\u63a5\uff0c\u7136\u540e\u7ef4\u62a4\u4e00\u4e2a\u5f02\u6b65\u961f\u5217\uff0c\u4f7f\u7528\u751f\u4ea7\u6d88\u8d39\u8005\u6a21\u5f0f\u8fdb\u884c\u722c\u53d6\uff0c\u722c\u866b\u7a0b\u5e8f\u4e00\u76f4\u5faa\u73af\u76f4\u5230\u6ca1\u6709\u8c03\u7528\u51fd\u6570\u4e3a\u6b62","title":"How It Works?"},{"location":"en/plugins/","text":"Plugins Plugins are used to package some commom functions as a third-party model. Ruia allow developers to implement third-party extensions by Middleware class. In the previous section, we talked about Middleware . It is used to process before request and after response. Then, we implemeneted a function, that is to add User-Agent in request headers. Perhaps any crawler need such a function, to add User-Agent randomly, so, let's packaging this function as a third-party extension. Do it! Creating a project The project name is ruia-ua . Ruia is based on Python3.6+ , so is ruia-ua . Supposing that you're now in Python 3.6+ . # Install package management tool: pipenv pip install pipenv # Create projecet directory mkdir ruia-ua cd ruia-ua # Install virtual environment pipenv install # Install ruia pipenv install ruia # Install aiofiles pipenv install aiofiles # Create project directory in the project directory mkdir ruia_ua cd ruia_ua # Here's your implementation touch __init__.py Directory structure: ruia-ua \u251c\u2500\u2500 LICENSE # Open source license \u251c\u2500\u2500 Pipfile # pipenv management tools \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # Main code of your plugin \u2502 \u2514\u2500\u2500 user_agents.txt # some random user_agents \u2514\u2500\u2500 setup.py First plugin user_agents.txt contains all kinds of UA , then we only need to use Middleware of ruia to add a random User-Agent before every request. Here is one implementation: import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware.request async def add_random_ua ( request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } Now it's high time to upload ruia-ua to community, then all other ruia users are able to use your third-party extension. Sounds great! Usage All crawlers can use ruia-ua to add User-Agent automatically. pip install ruia - ua Here is an example: from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) The implementations of third-party plugins will make developing crawlers easier! ruia do want your developing and uploading your own third-party plugins!","title":"Plugins"},{"location":"en/plugins/#plugins","text":"Plugins are used to package some commom functions as a third-party model. Ruia allow developers to implement third-party extensions by Middleware class. In the previous section, we talked about Middleware . It is used to process before request and after response. Then, we implemeneted a function, that is to add User-Agent in request headers. Perhaps any crawler need such a function, to add User-Agent randomly, so, let's packaging this function as a third-party extension. Do it!","title":"Plugins"},{"location":"en/plugins/#creating-a-project","text":"The project name is ruia-ua . Ruia is based on Python3.6+ , so is ruia-ua . Supposing that you're now in Python 3.6+ . # Install package management tool: pipenv pip install pipenv # Create projecet directory mkdir ruia-ua cd ruia-ua # Install virtual environment pipenv install # Install ruia pipenv install ruia # Install aiofiles pipenv install aiofiles # Create project directory in the project directory mkdir ruia_ua cd ruia_ua # Here's your implementation touch __init__.py Directory structure: ruia-ua \u251c\u2500\u2500 LICENSE # Open source license \u251c\u2500\u2500 Pipfile # pipenv management tools \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # Main code of your plugin \u2502 \u2514\u2500\u2500 user_agents.txt # some random user_agents \u2514\u2500\u2500 setup.py","title":"Creating a project"},{"location":"en/plugins/#first-plugin","text":"user_agents.txt contains all kinds of UA , then we only need to use Middleware of ruia to add a random User-Agent before every request. Here is one implementation: import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware.request async def add_random_ua ( request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } Now it's high time to upload ruia-ua to community, then all other ruia users are able to use your third-party extension. Sounds great!","title":"First plugin"},{"location":"en/plugins/#usage","text":"All crawlers can use ruia-ua to add User-Agent automatically. pip install ruia - ua Here is an example: from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) The implementations of third-party plugins will make developing crawlers easier! ruia do want your developing and uploading your own third-party plugins!","title":"Usage"},{"location":"en/tutorials/","text":"Tutorials This tutorial shows how to scrape Hacker News in ruia . Creating a project Before all, you will have to set up a new ruia project, create a hacker_news_spider directory with the following contents: hacker_news_spider \u251c\u2500\u2500 db.py \u251c\u2500\u2500 hacker_news.py \u251c\u2500\u2500 items.py \u2514\u2500\u2500 middlewares.py Item Item is used to define the elements what you want to scrape from a website, and at this time, our targets are titles and urls from Hacker News . There are tow methods to get elements from a website supported by ruia , CSS Selector and XPath . PS: CSS Selector is prefered in this tutorial by default. OK, let's start our jobs. First, open the website https://news.ycombinator.com/news , and then right-click the page and choose Inspect or Inspect Element, you will see something like this: You can find that title's element is a HTML tag <a></a> with properties class=storylink and href=https://example.com , so we can get something like this: Param Selector Description target_item tr.athing each news item title a.storylink title of news url a.storylink->href url of news That's enough, let's define the news item. Edit items.py and input this code: from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) Pretty easy, right? Create a class inherit from Item and define some properties, and done\ud83d\ude0f. There is a special property target_item , is a list of items' element we want to scrape. Middleware Each middleware is responsible for doing some specific function before request or after response. If we want to add User-Agent into Headers before each request, middleware is a best way to do the job. Put this code in file middlewares.py : from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): ua = 'ruia user-agent' request . headers . update ({ 'User-Agent' : ua }) If you done, your spider will add User-Agent before each request automatically. Data persistence We gonna use MongoDB to store the data what we scrape by ruia. Open db.py and fill it with code: import asyncio from motor.motor_asyncio import AsyncIOMotorClient class MotorBase : \"\"\" About motor's doc: https://github.com/mongodb/motor \"\"\" _db = {} _collection = {} def __init__ ( self , loop = None ): self . motor_uri = '' self . loop = loop or asyncio . get_event_loop () def client ( self , db ): # motor self . motor_uri = f \"mongodb://localhost:27017/{db}\" return AsyncIOMotorClient ( self . motor_uri , io_loop = self . loop ) def get_db ( self , db = 'test' ): \"\"\" Get a db instance :param db: database name :return: the motor db instance \"\"\" if db not in self . _db : self . _db [ db ] = self . client ( db )[ db ] return self . _db [ db ] Spider Spider is the program's entry, it combines the Item , Middleware and other components togather, to make programs work better. In this example, the hacker news spider will scrape the first two pages, put the code in hacker_news.py : from ruia import Request , Spider from items import HackerNewsItem from middlewares import middleware from db import MotorBase class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com' ] concurrency = 3 async def parse ( self , res ): self . mongo_db = MotorBase () . get_db ( 'ruia_test' ) urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] for index , url in enumerate ( urls ): yield Request ( url , callback = self . parse_item , metadata = { 'index' : index } ) async def parse_item ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : try : await self . mongo_db . news . update_one ({ 'url' : item . url }, { '$set' : { 'url' : item . url , 'title' : item . title }}, upsert = True ) except Exception as e : self . logger . exception ( e ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) HackerNewsSpider subclasses Spider , and must implement method parse() . Run python hacker_news.py to launch hacker news spider: [ 2018 -09-24 17 :59:19,865 ] -ruia-INFO spider : Spider started! [ 2018 -09-24 17 :59:19,866 ] -Request-INFO request: <GET: https://news.ycombinator.com> [ 2018 -09-24 17 :59:23,259 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 1 > [ 2018 -09-24 17 :59:23,260 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 2 > [ 2018 -09-24 18 :03:05,562 ] -ruia-INFO spider : Stopping spider: ruia [ 2018 -09-24 18 :03:05,562 ] -ruia-INFO spider : Total requests: 3 [ 2018 -09-24 18 :03:05,562 ] -ruia-INFO spider : Time usage: 0 :00:02.802862 [ 2018 -09-24 18 :03:05,562 ] -ruia-INFO spider : Spider finished! After Spider finished! , open the database to have a look: As you can see, this is ruia . This example's code: hacker_news_spider . Next time, we'll learn how to write a ruia plugin","title":"Tutorials"},{"location":"en/tutorials/#tutorials","text":"This tutorial shows how to scrape Hacker News in ruia .","title":"Tutorials"},{"location":"en/tutorials/#creating-a-project","text":"Before all, you will have to set up a new ruia project, create a hacker_news_spider directory with the following contents: hacker_news_spider \u251c\u2500\u2500 db.py \u251c\u2500\u2500 hacker_news.py \u251c\u2500\u2500 items.py \u2514\u2500\u2500 middlewares.py","title":"Creating a project"},{"location":"en/tutorials/#item","text":"Item is used to define the elements what you want to scrape from a website, and at this time, our targets are titles and urls from Hacker News . There are tow methods to get elements from a website supported by ruia , CSS Selector and XPath . PS: CSS Selector is prefered in this tutorial by default. OK, let's start our jobs. First, open the website https://news.ycombinator.com/news , and then right-click the page and choose Inspect or Inspect Element, you will see something like this: You can find that title's element is a HTML tag <a></a> with properties class=storylink and href=https://example.com , so we can get something like this: Param Selector Description target_item tr.athing each news item title a.storylink title of news url a.storylink->href url of news That's enough, let's define the news item. Edit items.py and input this code: from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) Pretty easy, right? Create a class inherit from Item and define some properties, and done\ud83d\ude0f. There is a special property target_item , is a list of items' element we want to scrape.","title":"Item"},{"location":"en/tutorials/#middleware","text":"Each middleware is responsible for doing some specific function before request or after response. If we want to add User-Agent into Headers before each request, middleware is a best way to do the job. Put this code in file middlewares.py : from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): ua = 'ruia user-agent' request . headers . update ({ 'User-Agent' : ua }) If you done, your spider will add User-Agent before each request automatically.","title":"Middleware"},{"location":"en/tutorials/#data-persistence","text":"We gonna use MongoDB to store the data what we scrape by ruia. Open db.py and fill it with code: import asyncio from motor.motor_asyncio import AsyncIOMotorClient class MotorBase : \"\"\" About motor's doc: https://github.com/mongodb/motor \"\"\" _db = {} _collection = {} def __init__ ( self , loop = None ): self . motor_uri = '' self . loop = loop or asyncio . get_event_loop () def client ( self , db ): # motor self . motor_uri = f \"mongodb://localhost:27017/{db}\" return AsyncIOMotorClient ( self . motor_uri , io_loop = self . loop ) def get_db ( self , db = 'test' ): \"\"\" Get a db instance :param db: database name :return: the motor db instance \"\"\" if db not in self . _db : self . _db [ db ] = self . client ( db )[ db ] return self . _db [ db ]","title":"Data persistence"},{"location":"en/tutorials/#spider","text":"Spider is the program's entry, it combines the Item , Middleware and other components togather, to make programs work better. In this example, the hacker news spider will scrape the first two pages, put the code in hacker_news.py : from ruia import Request , Spider from items import HackerNewsItem from middlewares import middleware from db import MotorBase class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com' ] concurrency = 3 async def parse ( self , res ): self . mongo_db = MotorBase () . get_db ( 'ruia_test' ) urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] for index , url in enumerate ( urls ): yield Request ( url , callback = self . parse_item , metadata = { 'index' : index } ) async def parse_item ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : try : await self . mongo_db . news . update_one ({ 'url' : item . url }, { '$set' : { 'url' : item . url , 'title' : item . title }}, upsert = True ) except Exception as e : self . logger . exception ( e ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) HackerNewsSpider subclasses Spider , and must implement method parse() . Run python hacker_news.py to launch hacker news spider: [ 2018 -09-24 17 :59:19,865 ] -ruia-INFO spider : Spider started! [ 2018 -09-24 17 :59:19,866 ] -Request-INFO request: <GET: https://news.ycombinator.com> [ 2018 -09-24 17 :59:23,259 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 1 > [ 2018 -09-24 17 :59:23,260 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 2 > [ 2018 -09-24 18 :03:05,562 ] -ruia-INFO spider : Stopping spider: ruia [ 2018 -09-24 18 :03:05,562 ] -ruia-INFO spider : Total requests: 3 [ 2018 -09-24 18 :03:05,562 ] -ruia-INFO spider : Time usage: 0 :00:02.802862 [ 2018 -09-24 18 :03:05,562 ] -ruia-INFO spider : Spider finished! After Spider finished! , open the database to have a look: As you can see, this is ruia . This example's code: hacker_news_spider . Next time, we'll learn how to write a ruia plugin","title":"Spider"},{"location":"en/topics/item/","text":"Item item is mainly used to define data model and extract data from HTML source code. It has the following two methods: get_item : extract one data from HTML source code; get_items : extract many data from HTML source code. Core arguments get_item and get_items receives same arguments: - html: optional, HTML source code; - url: optional, HTML href link; - html_etree: optional, etree._Element object. Usage From the arguments above, we can see that, there are three ways to feed Item object: from a web link, from HTML source code, or even from etree._Element object. import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async_func = HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ) items = asyncio . get_event_loop () . run_until_complete ( async_func ) for item in items : print ( item . title , item . url ) Sometimes we may come across such a condition. When crawling github issues, we will find that there are several tags to a issue. Define TagItem as a standalone item is not that beautiful. It's time to focus on the many=True argument. Fields with many=True will return a list. from ruia import Item , TextField class GithiubIssueItem ( Item ): issue_id = TextField ( css_select = 'issue_id_class' ) title = TextField ( css_select = 'issue_title_class' ) tags = TextField ( css_select = 'tag_class' , many = True ) item = GithiubIssueItem . get_item ( html ) assert isinstance ( item . tags , list ) AttrField also has the argument many . How It Works? Inner, item class will change different kinds of inputs into etree._Element obejct, and then extract data. Meta class will help to get every property defined by Filed .","title":"Item"},{"location":"en/topics/item/#item","text":"item is mainly used to define data model and extract data from HTML source code. It has the following two methods: get_item : extract one data from HTML source code; get_items : extract many data from HTML source code.","title":"Item"},{"location":"en/topics/item/#core-arguments","text":"get_item and get_items receives same arguments: - html: optional, HTML source code; - url: optional, HTML href link; - html_etree: optional, etree._Element object.","title":"Core arguments"},{"location":"en/topics/item/#usage","text":"From the arguments above, we can see that, there are three ways to feed Item object: from a web link, from HTML source code, or even from etree._Element object. import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async_func = HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ) items = asyncio . get_event_loop () . run_until_complete ( async_func ) for item in items : print ( item . title , item . url ) Sometimes we may come across such a condition. When crawling github issues, we will find that there are several tags to a issue. Define TagItem as a standalone item is not that beautiful. It's time to focus on the many=True argument. Fields with many=True will return a list. from ruia import Item , TextField class GithiubIssueItem ( Item ): issue_id = TextField ( css_select = 'issue_id_class' ) title = TextField ( css_select = 'issue_title_class' ) tags = TextField ( css_select = 'tag_class' , many = True ) item = GithiubIssueItem . get_item ( html ) assert isinstance ( item . tags , list ) AttrField also has the argument many .","title":"Usage"},{"location":"en/topics/item/#how-it-works","text":"Inner, item class will change different kinds of inputs into etree._Element obejct, and then extract data. Meta class will help to get every property defined by Filed .","title":"How It Works?"},{"location":"en/topics/middleware/","text":"Middleware Middleware is mainly used to process before request and process after response, such as listening request and response. Middleware().request : do some operations before request; Middleware().response : do some operations after response. Usage Note: The function should receive a special argument; The function return nothing. The arguments are listed in the following example: from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): \"\"\" This function will be called before every request. request: an object of Request \"\"\" print ( \"request: print when a request is received\" ) @middleware.response async def print_on_response ( request , response ): \"\"\" This function will be called after every request. request: an object of Request response: an object of Response \"\"\" print ( \"response: print when a response is received\" ) How It Works? Middleware used decorators to implement the callback function, aims at writting middlewares easier for developers. Middleware().request_middleware and Middleware().response_middleware are two queues, stands for the user-defined functions.","title":"Middleware"},{"location":"en/topics/middleware/#middleware","text":"Middleware is mainly used to process before request and process after response, such as listening request and response. Middleware().request : do some operations before request; Middleware().response : do some operations after response.","title":"Middleware"},{"location":"en/topics/middleware/#usage","text":"Note: The function should receive a special argument; The function return nothing. The arguments are listed in the following example: from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): \"\"\" This function will be called before every request. request: an object of Request \"\"\" print ( \"request: print when a request is received\" ) @middleware.response async def print_on_response ( request , response ): \"\"\" This function will be called after every request. request: an object of Request response: an object of Response \"\"\" print ( \"response: print when a response is received\" )","title":"Usage"},{"location":"en/topics/middleware/#how-it-works","text":"Middleware used decorators to implement the callback function, aims at writting middlewares easier for developers. Middleware().request_middleware and Middleware().response_middleware are two queues, stands for the user-defined functions.","title":"How It Works?"},{"location":"en/topics/request/","text":"Request Request is used for operating web requests. It returns a Response object. Methods: Request().fetch : request a web resource, it can be used standalone. Request().fetch_callback : it is a core method for Spider class. Core arguments url: the resource link method: request method, shoud be GET or `POST callback: callback function headers: request headers load_js: bool, if the target web page needs loading JS metadata: some data that need pass to next request request_config: the configure of the request request_session: aiohttp.ClientSession res_type: response type of request, default str , optional bytes and json kwargs: other arguments for request Usage From the arguments above, we can see that Request can be used both associated with Spider and standalone. import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}> How It Works? Request class will send asynchronous http request by packaging aiohttp and pyppeteer .","title":"Request"},{"location":"en/topics/request/#request","text":"Request is used for operating web requests. It returns a Response object. Methods: Request().fetch : request a web resource, it can be used standalone. Request().fetch_callback : it is a core method for Spider class.","title":"Request"},{"location":"en/topics/request/#core-arguments","text":"url: the resource link method: request method, shoud be GET or `POST callback: callback function headers: request headers load_js: bool, if the target web page needs loading JS metadata: some data that need pass to next request request_config: the configure of the request request_session: aiohttp.ClientSession res_type: response type of request, default str , optional bytes and json kwargs: other arguments for request","title":"Core arguments"},{"location":"en/topics/request/#usage","text":"From the arguments above, we can see that Request can be used both associated with Spider and standalone. import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}>","title":"Usage"},{"location":"en/topics/request/#how-it-works","text":"Request class will send asynchronous http request by packaging aiohttp and pyppeteer .","title":"How It Works?"},{"location":"en/topics/response/","text":"Response Response is used to return a uniformed and friendly response object. Main properties: url: the href of resource metadata: some data from previous request res_type: the type of response, default str , optional bytes or json html: HTML source code from website cookies: cookies of website history: the request history headers: response headers status: response status code","title":"Response"},{"location":"en/topics/response/#response","text":"Response is used to return a uniformed and friendly response object. Main properties: url: the href of resource metadata: some data from previous request res_type: the type of response, default str , optional bytes or json html: HTML source code from website cookies: cookies of website history: the request history headers: response headers status: response status code","title":"Response"},{"location":"en/topics/selector/","text":"Selector Selector is implemented by Field class, and provides two ways for developers to extract data: CSS Selector and XPath Selector . In detail, they are implemented by the following three classes: AttrField(BaseField) : extract property of HTML tag TextField(BaseField) : extract text data of HTML tag HtmlField(BaseField) : extract raw html code from HTML tag Core arguments Arguments of all Field classes: - default: default value if no HTML tag founded. - many: bool, the return value will be a list. Arguments shard by TextField , AttrField and HtmlField : - css_select: locate the HTML tag by css selector. - xpath_select: locate the HTML tag by xpath selector. AttrField requires an extra field: - attr: the target attribute name of HTML tag. RegexField requires an extra field: - re_select: an regular expression, should be a str object. Usage from lxml import etree from ruia import AttrField , TextField , HtmlField , RegexField HTML = \"\"\" <html> <head> <title>ruia</title> </head> <body>\u00ac <p> <a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a> </p> </body> </html> \"\"\" html = etree . HTML ( HTML ) def test_css_select (): field = TextField ( css_select = \"head title\" ) value = field . extrextractact_value ( html_etree = html ) assert value == \"ruia\" def test_xpath_select (): field = TextField ( xpath_select = '/html/head/title' ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_attr_field (): attr_field = AttrField ( css_select = \"p a.test_link\" , attr = 'href' ) value = attr_field . extract ( html_etree = html ) assert value == \"https://github.com/howie6879/ruia\" def test_html_field (): field = HtmlField ( css_select = \"a.test_link\" ) assert field . extract ( html_etree = html ) == '<a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a>' def test_re_field (): field = RegexField ( re_select = '<title>(.*?)</title>' ) href = field . extract ( html = HTML ) assert href == 'ruia' How It Works? Use lxml to extract data from HTML source code, in terms of CSS Selector or XPath Selector . About RegexField RegexField is only used for better performance. It directly use python standard library re , so it's significantly faster than other fields implemented on lxml. If you only want to use regular expression to clean your crawled string, please use clean_ methods of Item, and here is an example: import re import ruia class MyItem ( ruia . Item ): title = ruia . TextField ( css_select = 'title' ) def clean_title ( self , value ): return re . match ( 'Blog: (.*?)' , value ) . group ( 0 ) RegexField do not use lxml to parse source code, so there's no css_select and xpath_select for RegexField . RegexField has a complex behaviour: if you set many= False : if you use named group in your regular expression, return a dictionary; else if you use group in your regular expression if there's only one group , return the group value as a string; if there are many groups , return a tuple of the values; else, return the whole match string. if you set many= True : return a list of the return values above. Note if you use named group, those groups without names will be lost.","title":"Selector"},{"location":"en/topics/selector/#selector","text":"Selector is implemented by Field class, and provides two ways for developers to extract data: CSS Selector and XPath Selector . In detail, they are implemented by the following three classes: AttrField(BaseField) : extract property of HTML tag TextField(BaseField) : extract text data of HTML tag HtmlField(BaseField) : extract raw html code from HTML tag","title":"Selector"},{"location":"en/topics/selector/#core-arguments","text":"Arguments of all Field classes: - default: default value if no HTML tag founded. - many: bool, the return value will be a list. Arguments shard by TextField , AttrField and HtmlField : - css_select: locate the HTML tag by css selector. - xpath_select: locate the HTML tag by xpath selector. AttrField requires an extra field: - attr: the target attribute name of HTML tag. RegexField requires an extra field: - re_select: an regular expression, should be a str object.","title":"Core arguments"},{"location":"en/topics/selector/#usage","text":"from lxml import etree from ruia import AttrField , TextField , HtmlField , RegexField HTML = \"\"\" <html> <head> <title>ruia</title> </head> <body>\u00ac <p> <a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a> </p> </body> </html> \"\"\" html = etree . HTML ( HTML ) def test_css_select (): field = TextField ( css_select = \"head title\" ) value = field . extrextractact_value ( html_etree = html ) assert value == \"ruia\" def test_xpath_select (): field = TextField ( xpath_select = '/html/head/title' ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_attr_field (): attr_field = AttrField ( css_select = \"p a.test_link\" , attr = 'href' ) value = attr_field . extract ( html_etree = html ) assert value == \"https://github.com/howie6879/ruia\" def test_html_field (): field = HtmlField ( css_select = \"a.test_link\" ) assert field . extract ( html_etree = html ) == '<a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a>' def test_re_field (): field = RegexField ( re_select = '<title>(.*?)</title>' ) href = field . extract ( html = HTML ) assert href == 'ruia'","title":"Usage"},{"location":"en/topics/selector/#how-it-works","text":"Use lxml to extract data from HTML source code, in terms of CSS Selector or XPath Selector .","title":"How It Works?"},{"location":"en/topics/selector/#about-regexfield","text":"RegexField is only used for better performance. It directly use python standard library re , so it's significantly faster than other fields implemented on lxml. If you only want to use regular expression to clean your crawled string, please use clean_ methods of Item, and here is an example: import re import ruia class MyItem ( ruia . Item ): title = ruia . TextField ( css_select = 'title' ) def clean_title ( self , value ): return re . match ( 'Blog: (.*?)' , value ) . group ( 0 ) RegexField do not use lxml to parse source code, so there's no css_select and xpath_select for RegexField . RegexField has a complex behaviour: if you set many= False : if you use named group in your regular expression, return a dictionary; else if you use group in your regular expression if there's only one group , return the group value as a string; if there are many groups , return a tuple of the values; else, return the whole match string. if you set many= True : return a list of the return values above. Note if you use named group, those groups without names will be lost.","title":"About RegexField"},{"location":"en/topics/spider/","text":"Spider Spider is the entrypoint of the crawler program. It combines Item , Middleware , Request and other models, to build a strong crawler for you. You should focus on the following two functions: Spider.start : the entrypoint parse : The first parse function, required for subclass of Spider Core arguments Spider.start arguments: after_start: a hook after starting the crawler before_stop: a hook before starting the crawler middleware: Middleware class, can be an object of Middleware() , or a list of Middleware() loop: event loop Usage import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( item . title + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () How It Works? Spider will read links in start_urls , and maintains a asynchronous queue. The queue is a producer consumer model, and the loop will run until no more request functions.","title":"Spider"},{"location":"en/topics/spider/#spider","text":"Spider is the entrypoint of the crawler program. It combines Item , Middleware , Request and other models, to build a strong crawler for you. You should focus on the following two functions: Spider.start : the entrypoint parse : The first parse function, required for subclass of Spider","title":"Spider"},{"location":"en/topics/spider/#core-arguments","text":"Spider.start arguments: after_start: a hook after starting the crawler before_stop: a hook before starting the crawler middleware: Middleware class, can be an object of Middleware() , or a list of Middleware() loop: event loop","title":"Core arguments"},{"location":"en/topics/spider/#usage","text":"import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , res ): items = await HackerNewsItem . get_items ( html = res . html ) for item in items : async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( item . title + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start ()","title":"Usage"},{"location":"en/topics/spider/#how-it-works","text":"Spider will read links in start_urls , and maintains a asynchronous queue. The queue is a producer consumer model, and the loop will run until no more request functions.","title":"How It Works?"}]}