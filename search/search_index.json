{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Ruia \ud83d\udd78\ufe0f Async Python 3.6+ web scraping micro-framework based on asyncio. \u26a1 Write less, run faster. Overview Ruia is an async web scraping micro-framework, written with asyncio and aiohttp , aims to make crawling url as convenient as possible. Write less, run faster : Documentation: \u4e2d\u6587\u6587\u6863 | documentation Organization: python-ruia Plugin: awesome-ruia (Any contributions you make are greatly appreciated !) Features Easy : Declarative programming Fast : Powered by asyncio Extensible : Middlewares and plugins Powerful : JavaScript support Installation # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia Tutorials Overview Installation Define Data Items Spider Control Request & Response Customize Middleware Write a Plugins TODO [x] Cache for debug, to decreasing request limitation, ruia-cache [x] Provide an easy way to debug the script, ruia-shell [ ] Distributed crawling/scraping Contribution Ruia is still under developing, feel free to open issues and pull requests: Report or fix bugs Require or publish plugins Write or fix documentation Add test cases !!!Notice: We use black to format the code. Thanks aiohttp demiurge","title":"Introduction"},{"location":"index.html#overview","text":"Ruia is an async web scraping micro-framework, written with asyncio and aiohttp , aims to make crawling url as convenient as possible. Write less, run faster : Documentation: \u4e2d\u6587\u6587\u6863 | documentation Organization: python-ruia Plugin: awesome-ruia (Any contributions you make are greatly appreciated !)","title":"Overview"},{"location":"index.html#features","text":"Easy : Declarative programming Fast : Powered by asyncio Extensible : Middlewares and plugins Powerful : JavaScript support","title":"Features"},{"location":"index.html#installation","text":"# For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia","title":"Installation"},{"location":"index.html#tutorials","text":"Overview Installation Define Data Items Spider Control Request & Response Customize Middleware Write a Plugins","title":"Tutorials"},{"location":"index.html#todo","text":"[x] Cache for debug, to decreasing request limitation, ruia-cache [x] Provide an easy way to debug the script, ruia-shell [ ] Distributed crawling/scraping","title":"TODO"},{"location":"index.html#contribution","text":"Ruia is still under developing, feel free to open issues and pull requests: Report or fix bugs Require or publish plugins Write or fix documentation Add test cases !!!Notice: We use black to format the code.","title":"Contribution"},{"location":"index.html#thanks","text":"aiohttp demiurge","title":"Thanks"},{"location":"cn/zh_index.html","text":"Ruia \ud83d\udd78\ufe0f Async Python 3.6+ web scraping micro-framework based on asyncio. \u26a1 Write less, run faster. \u6982\u8ff0 Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u76ee\u6807\u5728\u4e8e\u8ba9\u5f00\u53d1\u8005\u7f16\u5199\u722c\u866b\u5c3d\u53ef\u80fd\u5730\u65b9\u4fbf\u5feb\u901f\u3002 \u5199\u66f4\u5c11\u7684\u4ee3\u7801\uff0c\u83b7\u53d6\u66f4\u5feb\u7684\u8fd0\u884c\u901f\u5ea6 \uff1a \u6559\u7a0b\uff1a \u4e2d\u6587\u6587\u6863 | documentation Github \u7ec4\u7ec7\uff1a python-ruia \u63d2\u4ef6\uff1a awesome-ruia (\u4f60\u8d21\u732e\u7684\u4efb\u4f55\u63d2\u4ef6\u90fd\u662f\u503c\u5f97\u8d5e\u8d4f\u4e14\u53ef\u8d35\u7684\uff01) \u7279\u6027 \u7b80\u5355 \uff1a\u7b80\u660e\u7684\u8bed\u6cd5 \u901f\u5ea6 \uff1a \u5f00\u53d1\uff1a\u5e38\u7528\u529f\u80fd\u63d2\u4ef6\u5316\uff0c\u5982 \u52a0\u8f7d js \u3001 \u81ea\u52a8\u5207\u6362 UA \u3001 \u6570\u636e\u6301\u4e45\u5316 \u7b49\u63d2\u4ef6 \u8fd0\u884c\uff1a asyncio \u9a71\u52a8 \u63d2\u4ef6 \uff1a\u81ea\u7531\u5730\u6269\u5c55\u4e2a\u6027\u5316\u529f\u80fd \u5b89\u88c5 # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia \u5165\u95e8 Overview Installation Define Data Items Spider Control Request & Response Customize Middleware Write a Plugins \u81f4\u8c22 Ruia \u8fd8\u5904\u4e8e\u5f00\u53d1\u9636\u6bb5\uff0c\u4efb\u4f55 Issue \u548c PR(Plugin) \u90fd\u975e\u5e38\u6b22\u8fce\uff0c\u611f\u8c22\u4ee5\u4e0b\u5f00\u53d1\u8005\u5bf9 Ruia \u7684\u8d21\u732e\uff08\u6392\u540d\u4e0d\u5206\u5148\u540e\uff09\uff1a \u611f\u8c22\u4ee5\u4e0b\u6846\u67b6\uff1a aiohttp demiurge \u4ea4\u6d41 \u6709\u4efb\u4f55\u95ee\u9898\uff0c\u6b22\u8fce\u7559\u8a00\u6216\u8005\u52a0\u6211\u4e00\u8d77\u4ea4\u6d41\uff1a","title":"Zh index"},{"location":"cn/zh_index.html#_1","text":"Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u76ee\u6807\u5728\u4e8e\u8ba9\u5f00\u53d1\u8005\u7f16\u5199\u722c\u866b\u5c3d\u53ef\u80fd\u5730\u65b9\u4fbf\u5feb\u901f\u3002 \u5199\u66f4\u5c11\u7684\u4ee3\u7801\uff0c\u83b7\u53d6\u66f4\u5feb\u7684\u8fd0\u884c\u901f\u5ea6 \uff1a \u6559\u7a0b\uff1a \u4e2d\u6587\u6587\u6863 | documentation Github \u7ec4\u7ec7\uff1a python-ruia \u63d2\u4ef6\uff1a awesome-ruia (\u4f60\u8d21\u732e\u7684\u4efb\u4f55\u63d2\u4ef6\u90fd\u662f\u503c\u5f97\u8d5e\u8d4f\u4e14\u53ef\u8d35\u7684\uff01)","title":"\u6982\u8ff0"},{"location":"cn/zh_index.html#_2","text":"\u7b80\u5355 \uff1a\u7b80\u660e\u7684\u8bed\u6cd5 \u901f\u5ea6 \uff1a \u5f00\u53d1\uff1a\u5e38\u7528\u529f\u80fd\u63d2\u4ef6\u5316\uff0c\u5982 \u52a0\u8f7d js \u3001 \u81ea\u52a8\u5207\u6362 UA \u3001 \u6570\u636e\u6301\u4e45\u5316 \u7b49\u63d2\u4ef6 \u8fd0\u884c\uff1a asyncio \u9a71\u52a8 \u63d2\u4ef6 \uff1a\u81ea\u7531\u5730\u6269\u5c55\u4e2a\u6027\u5316\u529f\u80fd","title":"\u7279\u6027"},{"location":"cn/zh_index.html#_3","text":"# For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia","title":"\u5b89\u88c5"},{"location":"cn/zh_index.html#_4","text":"Overview Installation Define Data Items Spider Control Request & Response Customize Middleware Write a Plugins","title":"\u5165\u95e8"},{"location":"cn/zh_index.html#_5","text":"Ruia \u8fd8\u5904\u4e8e\u5f00\u53d1\u9636\u6bb5\uff0c\u4efb\u4f55 Issue \u548c PR(Plugin) \u90fd\u975e\u5e38\u6b22\u8fce\uff0c\u611f\u8c22\u4ee5\u4e0b\u5f00\u53d1\u8005\u5bf9 Ruia \u7684\u8d21\u732e\uff08\u6392\u540d\u4e0d\u5206\u5148\u540e\uff09\uff1a \u611f\u8c22\u4ee5\u4e0b\u6846\u67b6\uff1a aiohttp demiurge","title":"\u81f4\u8c22"},{"location":"cn/zh_index.html#_6","text":"\u6709\u4efb\u4f55\u95ee\u9898\uff0c\u6b22\u8fce\u7559\u8a00\u6216\u8005\u52a0\u6211\u4e00\u8d77\u4ea4\u6d41\uff1a","title":"\u4ea4\u6d41"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html","text":"\u5feb\u901f\u5f00\u59cb \u57fa\u4e8e Ruia \u5feb\u901f\u5b9e\u73b0\u4e00\u4e2a\u4ee5 Hacker News \u4e3a\u76ee\u6807\u7684\u722c\u866b \u672c\u6587\u4e3b\u8981\u901a\u8fc7\u5bf9 Hacker News \u7684\u722c\u53d6\u793a\u4f8b\u6765\u5c55\u793a\u5982\u4f55\u4f7f\u7528 Ruia \uff0c\u4e0b\u56fe\u7ea2\u6846\u4e2d\u7684\u6570\u636e\u5c31\u662f\u722c\u866b\u811a\u672c\u9700\u8981\u722c\u53d6\u7684\u76ee\u6807\uff1a \u5f00\u59cb\u524d\u7684\u51c6\u5907\u5de5\u4f5c\uff1a [x] \u786e\u5b9a\u5df2\u7ecf\u5b89\u88c5 Ruia \uff1a pip install ruia -U [x] \u786e\u5b9a\u53ef\u4ee5\u8bbf\u95ee Hacker News \u7b2c\u4e00\u6b65\uff1a\u5b9a\u4e49 Item Item \u7684\u76ee\u7684\u662f\u5b9a\u4e49\u76ee\u6807\u7f51\u7ad9\u4e2d\u4f60\u9700\u8981\u722c\u53d6\u7684\u6570\u636e\uff0c\u6b64\u65f6\uff0c\u722c\u866b\u7684\u76ee\u6807\u6570\u636e\u5c31\u662f\u9875\u9762\u4e2d\u7684 Title \u548c Url \uff0c\u600e\u4e48\u63d0\u53d6\u6570\u636e\uff0c Ruia \u7684 Field \u7c7b\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u4e09\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff1a XPath Re CSS Selector \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 CSS Selector \u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u7528\u6d4f\u89c8\u5668\u6253\u5f00 Hacker News \uff0c\u53f3\u952e\u5ba1\u67e5\u5143\u7d20\uff1a Notice: \u672c\u6559\u7a0b\u722c\u866b\u4f8b\u5b50\u90fd\u9ed8\u8ba4\u4f7f\u7528CSS Selector\u7684\u89c4\u5219\u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e \u663e\u800c\u6613\u89c1\uff0c\u6bcf\u9875\u5305\u542b 30 \u6761\u8d44\u8baf\uff0c\u90a3\u4e48\u76ee\u6807\u6570\u636e\u7684\u89c4\u5219\u53ef\u4ee5\u603b\u7ed3\u4e3a\uff1a Param Rule Description target_item tr.athing \u8868\u793a\u6bcf\u6761\u8d44\u8baf title a.storylink \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u7684\u6807\u9898 url a.storylink->href \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u6807\u9898\u7684\u94fe\u63a5 \u89c4\u5219\u660e\u786e\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u7528 Item \u6765\u5b9e\u73b0\u4e00\u4e2a\u9488\u5bf9\u4e8e\u76ee\u6807\u6570\u636e\u7684ORM\uff0c\u521b\u5efa\u6587\u4ef6 items.py \uff0c\u590d\u5236\u4e0b\u9762\u4ee3\u7801\uff1a from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) \u8fd9\u6bb5\u4ee3\u7801\u542b\u4e49\u662f\uff1a\u9488\u5bf9\u6211\u4eec\u63d0\u53d6\u7684\u76ee\u6807 HTML \uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a HackerNewsItem \u7c7b\uff0c\u5176\u5305\u542b\u4e86\u4e24\u4e2a field \uff1a title \uff1a\u76f4\u63a5\u4ece\u6587\u672c\u63d0\u53d6 url \uff1a\u4ece\u5c5e\u6027\u63d0\u53d6 \u7b49\u7b49\uff01 target_item \u662f\u4ec0\u4e48\uff1f\u5bf9\u4e8e\u4e00\u4e2a Item \u7c7b\u6765\u8bf4\uff0c\u5f53\u5176\u5b9a\u4e49\u597d\u7f51\u9875\u76ee\u6807\u6570\u636e\u540e\uff0c Ruia \u63d0\u4f9b\u4e24\u79cd\u65b9\u5f0f\u8fdb\u884c\u83b7\u53d6 Item \uff1a get_item\uff1a\u83b7\u53d6\u7f51\u9875\u7684\u5355\u76ee\u6807\uff0c\u6bd4\u5982\u76ee\u6807\u7f51\u9875\u7684\u6807\u9898\uff0c\u6b64\u65f6\u65e0\u9700\u5b9a\u4e49 target_item \uff1b get_items\uff1a\u83b7\u53d6\u7f51\u9875\u7684\u591a\u76ee\u6807\uff0c\u6bd4\u5982\u5f53\u524d\u76ee\u6807\u7f51\u9875 Hacker News \u4e2d\u7684 title \u548c url \u4e00\u5171\u6709 30 \u4e2a\uff0c\u8fd9\u65f6\u5c31\u5fc5\u987b\u5b9a\u4e49 target_item \u6765\u5bfb\u627e\u591a\u4e2a\u76ee\u6807\u5757\uff1b target_item \u7684\u4f5c\u7528\u5c31\u662f\u9488\u5bf9\u8fd9\u6837\u7684\u5de5\u4f5c\u800c\u8bde\u751f\u7684\uff0c\u5f00\u53d1\u8005\u53ea\u8981\u5b9a\u4e49\u597d\u8fd9\u4e2a\u5c5e\u6027\uff08\u6b64\u65f6Ruia\u4f1a\u81ea\u52a8\u83b7\u53d6\u7f51\u9875\u4e2d 30 \u4e2a target_item \uff09\uff0c\u7136\u540e\u6bcf\u4e2a target_item \u91cc\u9762\u5305\u542b\u7684 title \u548c url \u5c31\u4f1a\u88ab\u63d0\u53d6\u51fa\u6765\u3002 \u7b2c\u4e8c\u6b65\uff1a\u6d4b\u8bd5 Item Ruia \u4e3a\u4e86\u65b9\u4fbf\u6269\u5c55\u4ee5\u53ca\u81ea\u7531\u5730\u7ec4\u5408\u4f7f\u7528\uff0c\u672c\u8eab\u5404\u4e2a\u6a21\u5757\u4e4b\u95f4\u8026\u5408\u5ea6\u662f\u6781\u4f4e\u7684\uff0c\u6bcf\u4e2a\u6a21\u5757\u90fd\u53ef\u4ee5\u5728\u4f60\u7684\u9879\u76ee\u4e2d\u5355\u72ec\u4f7f\u7528\uff1b\u4f60\u751a\u81f3\u53ea\u4f7f\u7528 ruia.Item \u3001 Ruia.TextField \u548c ruia.AttrField \u6765\u7f16\u5199\u4e00\u4e2a\u7b80\u5355\u7684\u722c\u866b\u3002 \u811a\u672c\u8c03\u8bd5 \u57fa\u4e8e\u8fd9\u4e2a\u7279\u6027\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u5728\u811a\u672c\u91cc\u9762\u6d4b\u8bd5 HackerNewsItem \uff1a import asyncio from ruia import Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def test_item (): url = 'https://news.ycombinator.com/news?p=1' async for item in HackerNewsItem . get_items ( url = url ): print ( ' {} : {} ' . format ( item . title , item . url )) if __name__ == '__main__' : # Python 3.7 Required. asyncio . run ( test_item ()) # For Python 3.6 # loop = asyncio.get_event_loop() # loop.run_until_complete(test_item()) \u63a5\u4e0b\u6765\uff0c\u7ec8\u7aef\u4f1a\u8f93\u51fa\u4ee5\u4e0b\u65e5\u5fd7\uff1a [ 2021 :04:04 21 :37:23 ] INFO Request <GET: https://news.ycombinator.com/news?p = 1 > How to bypass Cloudflare bot protection: https://jychp.medium.com/how-to-bypass-cloudflare-bot-protection-1f2c6c0c36fb The EU has archived all of the \u201cEuromyths\u201d printed in UK media: https://www.thelondoneconomic.com/news/the-eu-have-archived-all-of-the-euromyths-printed-in-uk-media-and-it-makes-for-some-disturbing-reading-108942/ Laser: Learning a Latent Action Space for Efficient Reinforcement Learning: https://arxiv.org/abs/2103.15793 StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery: https://github.com/orpatashnik/StyleCLIP \u547d\u4ee4\u884c\u8c03\u8bd5 \u4e3a\u4e86\u4f7f Ruia \u7684\u811a\u672c\u8c03\u8bd5\u8fc7\u7a0b\u66f4\u52a0\u65b9\u4fbf\u4f18\u96c5\uff0c\u5f00\u53d1\u8005\u8fd8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 ruia-shell \u63d2\u4ef6\u8fdb\u884c\u8c03\u8bd5\uff0c\u9996\u5148\u8fdb\u884c\u5b89\u88c5\uff1a pip install -U ruia-shell pip install ipython \u5177\u4f53\u4f7f\u7528\u5982\u4e0b\uff1a \u279c ~ ruia_shell https://news.ycombinator.com/news \\? p \\= 1 \u2728 Write less, run faster ( 0 .8.2 ) . __________ .__ .__ .__ .__ \\_ _____ \\_ _ __ | __ | ____ _____ | | __ ____ | | | | | _/ | \\ \\_ _ \\ / ___/ | \\_ / __ \\| | | | | | \\ | / | / __ \\_ \\_ __ \\| Y \\ ___/ | | _ | | __ | ____ | _ /____/ | __ ( ____ / /____ >___ | / \\_ __ >____/____/ \\/ \\/ \\/ \\/ \\/ Available Objects : response : ruia.Response request : ruia.Request Available Functions : attr_field : Extract attribute elements by using css selector or xpath text_field : Extract text elements by using css selector or xpath fetch : Fetch a URL or ruia.Request In [ 1 ] : request Out [ 1 ] : <GET https://news.ycombinator.com/news?p = 1 > In [ 2 ] : response Out [ 2 ] : <Response url [ GET ] : https://news.ycombinator.com/news?p = 1 status:200> In [ 3 ] : text_field ( css_select = \"a.storylink\" ) Out [ 3 ] : 'The EU has archived all of the \u201cEuromyths\u201d printed in UK media' In [ 4 ] : attr_field ( css_select = \"a.storylink\" , attr = \"href\" ) Out [ 4 ] : 'https://www.thelondoneconomic.com/news/the-eu-have-archived-all-of-the-euromyths-printed-in-uk-media-and-it-makes-for-some-disturbing-reading-108942/' \u5982\u679c\u6587\u5b57\u4e0d\u6e05\u695a\uff0c\u53ef\u770b\u4e0b\u56fe\uff1a \u7b2c\u4e09\u6b65\uff1a\u7f16\u5199 Spider Ruia.Spider \u662f Ruia \u6846\u67b6\u91cc\u9762\u7684\u6838\u5fc3\u63a7\u5236\u7c7b\uff0c\u5b83\u4f5c\u7528\u5728\u4e8e\uff1a \u63a7\u5236\u76ee\u6807\u7f51\u9875\u7684\u8bf7\u6c42 Ruia.Request \u548c\u54cd\u5e94 Ruia.Response \u53ef\u52a0\u8f7d\u81ea\u5b9a\u4e49\u94a9\u5b50\u3001\u63d2\u4ef6\u3001\u4ee5\u53ca\u76f8\u5173\u914d\u7f6e\u7b49\uff0c\u8ba9\u5f00\u53d1\u6548\u7387\u66f4\u9ad8 \u63a5\u4e0b\u6765\u4f1a\u57fa\u4e8e\u524d\u9762\u7684 Item \u811a\u672c\u7ee7\u7eed\u5f00\u53d1\uff0c\u5177\u4f53\u4ee3\u7801\u5982\u4e0b\uff1a \"\"\" Target: https://news.ycombinator.com/ pip install aiofiles \"\"\" import aiofiles from ruia import Item , TextField , AttrField , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p= { index } ' for index in range ( 3 )] concurrency = 3 # \u8bbe\u7f6e\u4ee3\u7406 # aiohttp_kwargs = {\"proxy\": \"http://0.0.0.0:8765\"} async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) \u672c\u6bb5\u4ee3\u7801\u7684\u4f5c\u7528\u662f\uff1a \u722c\u53d6 Hacker News \u7684\u524d\u4e09\u9875\u5185\u5bb9\uff0c\u8bbe\u7f6e\u5e76\u53d1\u6570\u4e3a 3 \uff0c\u7136\u540e\u5168\u90e8\u6301\u4e45\u5316\u5230\u6587\u4ef6 hacker_news.txt \u5f00\u53d1\u8005\u5b9e\u73b0 HackerNewsSpider \u5fc5\u987b\u662f Spider \u7684\u5b50\u7c7b\uff0c\u4ee3\u7801\u4e2d\u51fa\u73b0\u7684\u4e24\u4e2a\u65b9\u6cd5\u90fd\u662f Spider \u5185\u7f6e\u7684\uff1a parse\uff1a\u6b64\u65b9\u6cd5\u662f Spider \u7684\u5165\u53e3\uff0c\u6bcf\u4e00\u4e2a start_urls \u7684\u54cd\u5e94\u5fc5\u7136\u4f1a\u88ab parse \u65b9\u6cd5\u6355\u6349\u5e76\u6267\u884c\uff1b process_item\uff1a\u6b64\u65b9\u6cd5\u4f5c\u7528\u662f\u62bd\u79bb\u51fa\u5bf9 Item \u63d0\u53d6\u7ed3\u679c\u7684\u5904\u7406\u8fc7\u7a0b\uff0c\u6bd4\u5982\u8fd9\u91cc\u4f1a\u63a5\u53d7\u81ea\u5b9a\u4e49 Item \u7c7b\u4f5c\u4e3a\u8f93\u5165\uff0c\u7136\u540e\u8fdb\u884c\u5904\u7406\u6301\u4e45\u5316\u5230\u6587\u4ef6\u3002 \u7b2c\u56db\u6b65\uff1a\u8fd0\u884c Start \u4e00\u5207\u51c6\u5907\u5c31\u7eea\uff0c\u542f\u52a8\u4f60\u7684\u722c\u866b\u811a\u672c\u5427\uff01 import aiofiles from ruia import AttrField , Item , Spider , TextField class HackerNewsItem ( Item ): target_item = TextField ( css_select = \"tr.athing\" ) title = TextField ( css_select = \"a.storylink\" ) url = AttrField ( css_select = \"a.storylink\" , attr = \"href\" ) class HackerNewsSpider ( Spider ): start_urls = [ f \"https://news.ycombinator.com/news?p= { index } \" for index in range ( 3 )] concurrency = 3 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( \"./hacker_news.txt\" , \"a\" ) as f : await f . write ( str ( item . title ) + \" \\n \" ) if __name__ == \"__main__\" : HackerNewsSpider . start () Tips\uff1a\u5982\u679c\u4f60\u60f3\u5728\u5f02\u6b65\u51fd\u6570\u91cc\u9762\u8c03\u7528\uff0c\u6267\u884c await HackerNewsSpider.start() \u5373\u53ef \u4e0d\u5230 30 \u884c\u4ee3\u7801\uff0c\u4f60\u5c31\u5b9e\u73b0\u4e86\u5bf9 Hacker News \u7684\u722c\u866b\u811a\u672c\uff0c\u5e76\u4e14\u811a\u672c\u5e26\u6709\u81ea\u52a8\u91cd\u8bd5\u3001\u5e76\u53d1\u63a7\u5236\u3001\u8bed\u6cd5\u7b80\u5355\u7b49\u7279\u6027\u3002 \u901a\u8fc7\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u4f60\u5df2\u7ecf\u57fa\u672c\u638c\u63e1\u4e86 Ruia \u4e2d Item \u3001 Middleware \u3001 Request \u7b49\u6a21\u5757\u7684\u7528\u6cd5\uff0c\u7ed3\u5408\u81ea\u8eab\u9700\u6c42\uff0c\u4f60\u53ef\u4ee5\u7f16\u5199\u4efb\u4f55\u722c\u866b\uff0c\u4f8b\u5b50\u4ee3\u7801\u89c1 hacker_news_spider \u3002 \u7b2c\u4e94\u6b65\uff1a\u6269\u5c55 Middleware Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u5206\u4e0b\u9762\u4e24\u79cd\u60c5\u51b5\uff1a \u5728\u6bcf\u6b21\u8bf7\u6c42\u4e4b\u524d\u505a\u4e00\u4e9b\u4e8b \u5728\u6bcf\u6b21\u8bf7\u6c42\u540e\u505a\u4e00\u4e9b\u4e8b \u6bd4\u5982\u6b64\u65f6\u722c\u53d6 Hacker News \uff0c\u82e5\u5e0c\u671b\u5728\u6bcf\u6b21\u8bf7\u6c42\u65f6\u5019\u81ea\u52a8\u6dfb\u52a0 Headers \u7684 User-Agent \uff0c\u53ef\u4ee5\u6dfb\u52a0\u4ee5\u4e0b\u4ee3\u7801\u5f15\u5165\u4e2d\u95f4\u4ef6\uff1a from ruia import AttrField , Item , Middleware , Spider , TextField middleware = Middleware () @middleware . request async def print_on_request ( spider_ins , request ): ua = \"ruia user-agent\" request . headers . update ({ \"User-Agent\" : ua }) print ( request . headers ) class HackerNewsItem ( Item ): target_item = TextField ( css_select = \"tr.athing\" ) title = TextField ( css_select = \"a.storylink\" ) url = AttrField ( css_select = \"a.storylink\" , attr = \"href\" ) class HackerNewsSpider ( Spider ): start_urls = [ f \"https://news.ycombinator.com/news?p= { index } \" for index in range ( 3 )] concurrency = 3 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item if __name__ == \"__main__\" : HackerNewsSpider . start ( middleware = middleware ) \u8fd9\u6837\uff0c\u7a0b\u5e8f\u4f1a\u5728\u722c\u866b\u8bf7\u6c42\u7f51\u9875\u8d44\u6e90\u4e4b\u524d\u81ea\u52a8\u52a0\u4e0a User-Agent \uff0c\u9488\u5bf9\u81ea\u52a8 UA \u7684\u529f\u80fd\u70b9\uff0c Ruia \u5df2\u7ecf\u4e13\u95e8\u7f16\u5199\u4e86\u4e00\u4e2a\u540d\u4e3a ruia-ua \u7684\u63d2\u4ef6\u6765\u4e3a\u5f00\u53d1\u8005\u63d0\u5347\u6548\u7387\uff0c\u4f7f\u7528\u975e\u5e38\u7b80\u5355\uff0c\u4ee3\u7801\u793a\u4f8b\u5982\u4e0b\uff1a from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , response ): # Do something... print ( response . url ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) MongoDB \u5bf9\u4e8e\u6570\u636e\u6301\u4e45\u5316\uff0c\u4f60\u53ef\u4ee5\u6309\u7167\u81ea\u5df1\u559c\u6b22\u7684\u65b9\u5f0f\u53bb\u505a\uff0c\u524d\u9762\u5b9e\u4f8b\u4e2d\u4ecb\u7ecd\u4e86\u5982\u4f55\u5c06\u76ee\u6807 Item \u6301\u4e45\u5316\u5230\u6587\u4ef6\u4e2d\u3002 \u5982\u679c\u60f3\u5c06\u6570\u636e\u6301\u4e45\u5316\u5230\u6570\u636e\u5e93\uff08MongoDB\uff09\u4e2d\uff0c\u8be5\u600e\u4e48\u505a\uff1f\u6b64\u65f6\u5c31\u5230\u4e86\u51f8\u663e Ruia \u63d2\u4ef6\u4f18\u52bf\u7684\u65f6\u5019\u4e86\uff0c\u4f60\u53ea\u9700\u8981\u5b89\u88c5 ruia-motor \uff1a pip install -U ruia-motor \u7136\u540e\u518d\u4ee3\u7801\u4e2d\u5f15\u5165 ruia-motor \uff1a from ruia_motor import RuiaMotorInsert , init_spider from ruia import AttrField , Item , Spider , TextField class HackerNewsItem ( Item ): target_item = TextField ( css_select = \"tr.athing\" ) title = TextField ( css_select = \"a.storylink\" ) url = AttrField ( css_select = \"a.storylink\" , attr = \"href\" ) class HackerNewsSpider ( Spider ): start_urls = [ f \"https://news.ycombinator.com/news?p= { index } \" for index in range ( 3 )] concurrency = 3 # aiohttp_kwargs = {\"proxy\": \"http://0.0.0.0:1087\"} async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield RuiaMotorInsert ( collection = \"news\" , data = item . results ) async def init_plugins_after_start ( spider_ins ): spider_ins . mongodb_config = { \"host\" : \"127.0.0.1\" , \"port\" : 27017 , \"db\" : \"ruia_motor\" } init_spider ( spider_ins = spider_ins ) if __name__ == \"__main__\" : HackerNewsSpider . start ( after_start = init_plugins_after_start ) \u6570\u636e\u5e93\u4e2d\u53ef\u4ee5\u770b\u5230\u76ee\u6807\u5b57\u6bb5\uff1a \u662f\u4e0d\u662f\u66f4\u7b80\u5355\u4e86\u5462\uff1f \u7b2c\u516d\u6b65\uff1a\u6df1\u5165\u4e86\u89e3Ruia \u5e0c\u671b Ruia \u53ef\u4ee5\u4e3a\u4f60\u5e26\u6765\u7f16\u5199\u722c\u866b\u7684\u4e50\u8da3 \uff1a) \u672c\u7ae0\u7b80\u5355\u4ecb\u7ecd\u4e86\u4e0b Ruia \u7684\u5feb\u901f\u5b9e\u8df5\uff0c\u5982\u679c\u60f3\u8981\u6df1\u5165\u4e86\u89e3 Ruia \u6b22\u8fce\u7ee7\u7eed\u5f80\u4e0b\u9605\u8bfb\uff0c\u5982\u679c\u6709\u60f3\u52a0\u5165\u4ea4\u6d41\u7fa4\uff0c\u8bf7\u5173\u6ce8\u516c\u4f17\u53f7\u52a0\u6211\u5fae\u4fe1\uff1a","title":"\u5feb\u901f\u5f00\u59cb"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#_1","text":"\u57fa\u4e8e Ruia \u5feb\u901f\u5b9e\u73b0\u4e00\u4e2a\u4ee5 Hacker News \u4e3a\u76ee\u6807\u7684\u722c\u866b \u672c\u6587\u4e3b\u8981\u901a\u8fc7\u5bf9 Hacker News \u7684\u722c\u53d6\u793a\u4f8b\u6765\u5c55\u793a\u5982\u4f55\u4f7f\u7528 Ruia \uff0c\u4e0b\u56fe\u7ea2\u6846\u4e2d\u7684\u6570\u636e\u5c31\u662f\u722c\u866b\u811a\u672c\u9700\u8981\u722c\u53d6\u7684\u76ee\u6807\uff1a \u5f00\u59cb\u524d\u7684\u51c6\u5907\u5de5\u4f5c\uff1a [x] \u786e\u5b9a\u5df2\u7ecf\u5b89\u88c5 Ruia \uff1a pip install ruia -U [x] \u786e\u5b9a\u53ef\u4ee5\u8bbf\u95ee Hacker News","title":"\u5feb\u901f\u5f00\u59cb"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#item","text":"Item \u7684\u76ee\u7684\u662f\u5b9a\u4e49\u76ee\u6807\u7f51\u7ad9\u4e2d\u4f60\u9700\u8981\u722c\u53d6\u7684\u6570\u636e\uff0c\u6b64\u65f6\uff0c\u722c\u866b\u7684\u76ee\u6807\u6570\u636e\u5c31\u662f\u9875\u9762\u4e2d\u7684 Title \u548c Url \uff0c\u600e\u4e48\u63d0\u53d6\u6570\u636e\uff0c Ruia \u7684 Field \u7c7b\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u4e09\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff1a XPath Re CSS Selector \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 CSS Selector \u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u7528\u6d4f\u89c8\u5668\u6253\u5f00 Hacker News \uff0c\u53f3\u952e\u5ba1\u67e5\u5143\u7d20\uff1a Notice: \u672c\u6559\u7a0b\u722c\u866b\u4f8b\u5b50\u90fd\u9ed8\u8ba4\u4f7f\u7528CSS Selector\u7684\u89c4\u5219\u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e \u663e\u800c\u6613\u89c1\uff0c\u6bcf\u9875\u5305\u542b 30 \u6761\u8d44\u8baf\uff0c\u90a3\u4e48\u76ee\u6807\u6570\u636e\u7684\u89c4\u5219\u53ef\u4ee5\u603b\u7ed3\u4e3a\uff1a Param Rule Description target_item tr.athing \u8868\u793a\u6bcf\u6761\u8d44\u8baf title a.storylink \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u7684\u6807\u9898 url a.storylink->href \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u6807\u9898\u7684\u94fe\u63a5 \u89c4\u5219\u660e\u786e\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u7528 Item \u6765\u5b9e\u73b0\u4e00\u4e2a\u9488\u5bf9\u4e8e\u76ee\u6807\u6570\u636e\u7684ORM\uff0c\u521b\u5efa\u6587\u4ef6 items.py \uff0c\u590d\u5236\u4e0b\u9762\u4ee3\u7801\uff1a from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) \u8fd9\u6bb5\u4ee3\u7801\u542b\u4e49\u662f\uff1a\u9488\u5bf9\u6211\u4eec\u63d0\u53d6\u7684\u76ee\u6807 HTML \uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a HackerNewsItem \u7c7b\uff0c\u5176\u5305\u542b\u4e86\u4e24\u4e2a field \uff1a title \uff1a\u76f4\u63a5\u4ece\u6587\u672c\u63d0\u53d6 url \uff1a\u4ece\u5c5e\u6027\u63d0\u53d6 \u7b49\u7b49\uff01 target_item \u662f\u4ec0\u4e48\uff1f\u5bf9\u4e8e\u4e00\u4e2a Item \u7c7b\u6765\u8bf4\uff0c\u5f53\u5176\u5b9a\u4e49\u597d\u7f51\u9875\u76ee\u6807\u6570\u636e\u540e\uff0c Ruia \u63d0\u4f9b\u4e24\u79cd\u65b9\u5f0f\u8fdb\u884c\u83b7\u53d6 Item \uff1a get_item\uff1a\u83b7\u53d6\u7f51\u9875\u7684\u5355\u76ee\u6807\uff0c\u6bd4\u5982\u76ee\u6807\u7f51\u9875\u7684\u6807\u9898\uff0c\u6b64\u65f6\u65e0\u9700\u5b9a\u4e49 target_item \uff1b get_items\uff1a\u83b7\u53d6\u7f51\u9875\u7684\u591a\u76ee\u6807\uff0c\u6bd4\u5982\u5f53\u524d\u76ee\u6807\u7f51\u9875 Hacker News \u4e2d\u7684 title \u548c url \u4e00\u5171\u6709 30 \u4e2a\uff0c\u8fd9\u65f6\u5c31\u5fc5\u987b\u5b9a\u4e49 target_item \u6765\u5bfb\u627e\u591a\u4e2a\u76ee\u6807\u5757\uff1b target_item \u7684\u4f5c\u7528\u5c31\u662f\u9488\u5bf9\u8fd9\u6837\u7684\u5de5\u4f5c\u800c\u8bde\u751f\u7684\uff0c\u5f00\u53d1\u8005\u53ea\u8981\u5b9a\u4e49\u597d\u8fd9\u4e2a\u5c5e\u6027\uff08\u6b64\u65f6Ruia\u4f1a\u81ea\u52a8\u83b7\u53d6\u7f51\u9875\u4e2d 30 \u4e2a target_item \uff09\uff0c\u7136\u540e\u6bcf\u4e2a target_item \u91cc\u9762\u5305\u542b\u7684 title \u548c url \u5c31\u4f1a\u88ab\u63d0\u53d6\u51fa\u6765\u3002","title":"\u7b2c\u4e00\u6b65\uff1a\u5b9a\u4e49 Item"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#item_1","text":"Ruia \u4e3a\u4e86\u65b9\u4fbf\u6269\u5c55\u4ee5\u53ca\u81ea\u7531\u5730\u7ec4\u5408\u4f7f\u7528\uff0c\u672c\u8eab\u5404\u4e2a\u6a21\u5757\u4e4b\u95f4\u8026\u5408\u5ea6\u662f\u6781\u4f4e\u7684\uff0c\u6bcf\u4e2a\u6a21\u5757\u90fd\u53ef\u4ee5\u5728\u4f60\u7684\u9879\u76ee\u4e2d\u5355\u72ec\u4f7f\u7528\uff1b\u4f60\u751a\u81f3\u53ea\u4f7f\u7528 ruia.Item \u3001 Ruia.TextField \u548c ruia.AttrField \u6765\u7f16\u5199\u4e00\u4e2a\u7b80\u5355\u7684\u722c\u866b\u3002","title":"\u7b2c\u4e8c\u6b65\uff1a\u6d4b\u8bd5 Item"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#_2","text":"\u57fa\u4e8e\u8fd9\u4e2a\u7279\u6027\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u5728\u811a\u672c\u91cc\u9762\u6d4b\u8bd5 HackerNewsItem \uff1a import asyncio from ruia import Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def test_item (): url = 'https://news.ycombinator.com/news?p=1' async for item in HackerNewsItem . get_items ( url = url ): print ( ' {} : {} ' . format ( item . title , item . url )) if __name__ == '__main__' : # Python 3.7 Required. asyncio . run ( test_item ()) # For Python 3.6 # loop = asyncio.get_event_loop() # loop.run_until_complete(test_item()) \u63a5\u4e0b\u6765\uff0c\u7ec8\u7aef\u4f1a\u8f93\u51fa\u4ee5\u4e0b\u65e5\u5fd7\uff1a [ 2021 :04:04 21 :37:23 ] INFO Request <GET: https://news.ycombinator.com/news?p = 1 > How to bypass Cloudflare bot protection: https://jychp.medium.com/how-to-bypass-cloudflare-bot-protection-1f2c6c0c36fb The EU has archived all of the \u201cEuromyths\u201d printed in UK media: https://www.thelondoneconomic.com/news/the-eu-have-archived-all-of-the-euromyths-printed-in-uk-media-and-it-makes-for-some-disturbing-reading-108942/ Laser: Learning a Latent Action Space for Efficient Reinforcement Learning: https://arxiv.org/abs/2103.15793 StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery: https://github.com/orpatashnik/StyleCLIP","title":"\u811a\u672c\u8c03\u8bd5"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#_3","text":"\u4e3a\u4e86\u4f7f Ruia \u7684\u811a\u672c\u8c03\u8bd5\u8fc7\u7a0b\u66f4\u52a0\u65b9\u4fbf\u4f18\u96c5\uff0c\u5f00\u53d1\u8005\u8fd8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 ruia-shell \u63d2\u4ef6\u8fdb\u884c\u8c03\u8bd5\uff0c\u9996\u5148\u8fdb\u884c\u5b89\u88c5\uff1a pip install -U ruia-shell pip install ipython \u5177\u4f53\u4f7f\u7528\u5982\u4e0b\uff1a \u279c ~ ruia_shell https://news.ycombinator.com/news \\? p \\= 1 \u2728 Write less, run faster ( 0 .8.2 ) . __________ .__ .__ .__ .__ \\_ _____ \\_ _ __ | __ | ____ _____ | | __ ____ | | | | | _/ | \\ \\_ _ \\ / ___/ | \\_ / __ \\| | | | | | \\ | / | / __ \\_ \\_ __ \\| Y \\ ___/ | | _ | | __ | ____ | _ /____/ | __ ( ____ / /____ >___ | / \\_ __ >____/____/ \\/ \\/ \\/ \\/ \\/ Available Objects : response : ruia.Response request : ruia.Request Available Functions : attr_field : Extract attribute elements by using css selector or xpath text_field : Extract text elements by using css selector or xpath fetch : Fetch a URL or ruia.Request In [ 1 ] : request Out [ 1 ] : <GET https://news.ycombinator.com/news?p = 1 > In [ 2 ] : response Out [ 2 ] : <Response url [ GET ] : https://news.ycombinator.com/news?p = 1 status:200> In [ 3 ] : text_field ( css_select = \"a.storylink\" ) Out [ 3 ] : 'The EU has archived all of the \u201cEuromyths\u201d printed in UK media' In [ 4 ] : attr_field ( css_select = \"a.storylink\" , attr = \"href\" ) Out [ 4 ] : 'https://www.thelondoneconomic.com/news/the-eu-have-archived-all-of-the-euromyths-printed-in-uk-media-and-it-makes-for-some-disturbing-reading-108942/' \u5982\u679c\u6587\u5b57\u4e0d\u6e05\u695a\uff0c\u53ef\u770b\u4e0b\u56fe\uff1a","title":"\u547d\u4ee4\u884c\u8c03\u8bd5"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#spider","text":"Ruia.Spider \u662f Ruia \u6846\u67b6\u91cc\u9762\u7684\u6838\u5fc3\u63a7\u5236\u7c7b\uff0c\u5b83\u4f5c\u7528\u5728\u4e8e\uff1a \u63a7\u5236\u76ee\u6807\u7f51\u9875\u7684\u8bf7\u6c42 Ruia.Request \u548c\u54cd\u5e94 Ruia.Response \u53ef\u52a0\u8f7d\u81ea\u5b9a\u4e49\u94a9\u5b50\u3001\u63d2\u4ef6\u3001\u4ee5\u53ca\u76f8\u5173\u914d\u7f6e\u7b49\uff0c\u8ba9\u5f00\u53d1\u6548\u7387\u66f4\u9ad8 \u63a5\u4e0b\u6765\u4f1a\u57fa\u4e8e\u524d\u9762\u7684 Item \u811a\u672c\u7ee7\u7eed\u5f00\u53d1\uff0c\u5177\u4f53\u4ee3\u7801\u5982\u4e0b\uff1a \"\"\" Target: https://news.ycombinator.com/ pip install aiofiles \"\"\" import aiofiles from ruia import Item , TextField , AttrField , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p= { index } ' for index in range ( 3 )] concurrency = 3 # \u8bbe\u7f6e\u4ee3\u7406 # aiohttp_kwargs = {\"proxy\": \"http://0.0.0.0:8765\"} async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) \u672c\u6bb5\u4ee3\u7801\u7684\u4f5c\u7528\u662f\uff1a \u722c\u53d6 Hacker News \u7684\u524d\u4e09\u9875\u5185\u5bb9\uff0c\u8bbe\u7f6e\u5e76\u53d1\u6570\u4e3a 3 \uff0c\u7136\u540e\u5168\u90e8\u6301\u4e45\u5316\u5230\u6587\u4ef6 hacker_news.txt \u5f00\u53d1\u8005\u5b9e\u73b0 HackerNewsSpider \u5fc5\u987b\u662f Spider \u7684\u5b50\u7c7b\uff0c\u4ee3\u7801\u4e2d\u51fa\u73b0\u7684\u4e24\u4e2a\u65b9\u6cd5\u90fd\u662f Spider \u5185\u7f6e\u7684\uff1a parse\uff1a\u6b64\u65b9\u6cd5\u662f Spider \u7684\u5165\u53e3\uff0c\u6bcf\u4e00\u4e2a start_urls \u7684\u54cd\u5e94\u5fc5\u7136\u4f1a\u88ab parse \u65b9\u6cd5\u6355\u6349\u5e76\u6267\u884c\uff1b process_item\uff1a\u6b64\u65b9\u6cd5\u4f5c\u7528\u662f\u62bd\u79bb\u51fa\u5bf9 Item \u63d0\u53d6\u7ed3\u679c\u7684\u5904\u7406\u8fc7\u7a0b\uff0c\u6bd4\u5982\u8fd9\u91cc\u4f1a\u63a5\u53d7\u81ea\u5b9a\u4e49 Item \u7c7b\u4f5c\u4e3a\u8f93\u5165\uff0c\u7136\u540e\u8fdb\u884c\u5904\u7406\u6301\u4e45\u5316\u5230\u6587\u4ef6\u3002","title":"\u7b2c\u4e09\u6b65\uff1a\u7f16\u5199 Spider"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#start","text":"\u4e00\u5207\u51c6\u5907\u5c31\u7eea\uff0c\u542f\u52a8\u4f60\u7684\u722c\u866b\u811a\u672c\u5427\uff01 import aiofiles from ruia import AttrField , Item , Spider , TextField class HackerNewsItem ( Item ): target_item = TextField ( css_select = \"tr.athing\" ) title = TextField ( css_select = \"a.storylink\" ) url = AttrField ( css_select = \"a.storylink\" , attr = \"href\" ) class HackerNewsSpider ( Spider ): start_urls = [ f \"https://news.ycombinator.com/news?p= { index } \" for index in range ( 3 )] concurrency = 3 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( \"./hacker_news.txt\" , \"a\" ) as f : await f . write ( str ( item . title ) + \" \\n \" ) if __name__ == \"__main__\" : HackerNewsSpider . start () Tips\uff1a\u5982\u679c\u4f60\u60f3\u5728\u5f02\u6b65\u51fd\u6570\u91cc\u9762\u8c03\u7528\uff0c\u6267\u884c await HackerNewsSpider.start() \u5373\u53ef \u4e0d\u5230 30 \u884c\u4ee3\u7801\uff0c\u4f60\u5c31\u5b9e\u73b0\u4e86\u5bf9 Hacker News \u7684\u722c\u866b\u811a\u672c\uff0c\u5e76\u4e14\u811a\u672c\u5e26\u6709\u81ea\u52a8\u91cd\u8bd5\u3001\u5e76\u53d1\u63a7\u5236\u3001\u8bed\u6cd5\u7b80\u5355\u7b49\u7279\u6027\u3002 \u901a\u8fc7\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u4f60\u5df2\u7ecf\u57fa\u672c\u638c\u63e1\u4e86 Ruia \u4e2d Item \u3001 Middleware \u3001 Request \u7b49\u6a21\u5757\u7684\u7528\u6cd5\uff0c\u7ed3\u5408\u81ea\u8eab\u9700\u6c42\uff0c\u4f60\u53ef\u4ee5\u7f16\u5199\u4efb\u4f55\u722c\u866b\uff0c\u4f8b\u5b50\u4ee3\u7801\u89c1 hacker_news_spider \u3002","title":"\u7b2c\u56db\u6b65\uff1a\u8fd0\u884c Start"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#_4","text":"","title":"\u7b2c\u4e94\u6b65\uff1a\u6269\u5c55"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#middleware","text":"Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u5206\u4e0b\u9762\u4e24\u79cd\u60c5\u51b5\uff1a \u5728\u6bcf\u6b21\u8bf7\u6c42\u4e4b\u524d\u505a\u4e00\u4e9b\u4e8b \u5728\u6bcf\u6b21\u8bf7\u6c42\u540e\u505a\u4e00\u4e9b\u4e8b \u6bd4\u5982\u6b64\u65f6\u722c\u53d6 Hacker News \uff0c\u82e5\u5e0c\u671b\u5728\u6bcf\u6b21\u8bf7\u6c42\u65f6\u5019\u81ea\u52a8\u6dfb\u52a0 Headers \u7684 User-Agent \uff0c\u53ef\u4ee5\u6dfb\u52a0\u4ee5\u4e0b\u4ee3\u7801\u5f15\u5165\u4e2d\u95f4\u4ef6\uff1a from ruia import AttrField , Item , Middleware , Spider , TextField middleware = Middleware () @middleware . request async def print_on_request ( spider_ins , request ): ua = \"ruia user-agent\" request . headers . update ({ \"User-Agent\" : ua }) print ( request . headers ) class HackerNewsItem ( Item ): target_item = TextField ( css_select = \"tr.athing\" ) title = TextField ( css_select = \"a.storylink\" ) url = AttrField ( css_select = \"a.storylink\" , attr = \"href\" ) class HackerNewsSpider ( Spider ): start_urls = [ f \"https://news.ycombinator.com/news?p= { index } \" for index in range ( 3 )] concurrency = 3 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item if __name__ == \"__main__\" : HackerNewsSpider . start ( middleware = middleware ) \u8fd9\u6837\uff0c\u7a0b\u5e8f\u4f1a\u5728\u722c\u866b\u8bf7\u6c42\u7f51\u9875\u8d44\u6e90\u4e4b\u524d\u81ea\u52a8\u52a0\u4e0a User-Agent \uff0c\u9488\u5bf9\u81ea\u52a8 UA \u7684\u529f\u80fd\u70b9\uff0c Ruia \u5df2\u7ecf\u4e13\u95e8\u7f16\u5199\u4e86\u4e00\u4e2a\u540d\u4e3a ruia-ua \u7684\u63d2\u4ef6\u6765\u4e3a\u5f00\u53d1\u8005\u63d0\u5347\u6548\u7387\uff0c\u4f7f\u7528\u975e\u5e38\u7b80\u5355\uff0c\u4ee3\u7801\u793a\u4f8b\u5982\u4e0b\uff1a from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , response ): # Do something... print ( response . url ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware )","title":"Middleware"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#mongodb","text":"\u5bf9\u4e8e\u6570\u636e\u6301\u4e45\u5316\uff0c\u4f60\u53ef\u4ee5\u6309\u7167\u81ea\u5df1\u559c\u6b22\u7684\u65b9\u5f0f\u53bb\u505a\uff0c\u524d\u9762\u5b9e\u4f8b\u4e2d\u4ecb\u7ecd\u4e86\u5982\u4f55\u5c06\u76ee\u6807 Item \u6301\u4e45\u5316\u5230\u6587\u4ef6\u4e2d\u3002 \u5982\u679c\u60f3\u5c06\u6570\u636e\u6301\u4e45\u5316\u5230\u6570\u636e\u5e93\uff08MongoDB\uff09\u4e2d\uff0c\u8be5\u600e\u4e48\u505a\uff1f\u6b64\u65f6\u5c31\u5230\u4e86\u51f8\u663e Ruia \u63d2\u4ef6\u4f18\u52bf\u7684\u65f6\u5019\u4e86\uff0c\u4f60\u53ea\u9700\u8981\u5b89\u88c5 ruia-motor \uff1a pip install -U ruia-motor \u7136\u540e\u518d\u4ee3\u7801\u4e2d\u5f15\u5165 ruia-motor \uff1a from ruia_motor import RuiaMotorInsert , init_spider from ruia import AttrField , Item , Spider , TextField class HackerNewsItem ( Item ): target_item = TextField ( css_select = \"tr.athing\" ) title = TextField ( css_select = \"a.storylink\" ) url = AttrField ( css_select = \"a.storylink\" , attr = \"href\" ) class HackerNewsSpider ( Spider ): start_urls = [ f \"https://news.ycombinator.com/news?p= { index } \" for index in range ( 3 )] concurrency = 3 # aiohttp_kwargs = {\"proxy\": \"http://0.0.0.0:1087\"} async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield RuiaMotorInsert ( collection = \"news\" , data = item . results ) async def init_plugins_after_start ( spider_ins ): spider_ins . mongodb_config = { \"host\" : \"127.0.0.1\" , \"port\" : 27017 , \"db\" : \"ruia_motor\" } init_spider ( spider_ins = spider_ins ) if __name__ == \"__main__\" : HackerNewsSpider . start ( after_start = init_plugins_after_start ) \u6570\u636e\u5e93\u4e2d\u53ef\u4ee5\u770b\u5230\u76ee\u6807\u5b57\u6bb5\uff1a \u662f\u4e0d\u662f\u66f4\u7b80\u5355\u4e86\u5462\uff1f","title":"MongoDB"},{"location":"cn/00_%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/_index.html#ruia","text":"\u5e0c\u671b Ruia \u53ef\u4ee5\u4e3a\u4f60\u5e26\u6765\u7f16\u5199\u722c\u866b\u7684\u4e50\u8da3 \uff1a) \u672c\u7ae0\u7b80\u5355\u4ecb\u7ecd\u4e86\u4e0b Ruia \u7684\u5feb\u901f\u5b9e\u8df5\uff0c\u5982\u679c\u60f3\u8981\u6df1\u5165\u4e86\u89e3 Ruia \u6b22\u8fce\u7ee7\u7eed\u5f80\u4e0b\u9605\u8bfb\uff0c\u5982\u679c\u6709\u60f3\u52a0\u5165\u4ea4\u6d41\u7fa4\uff0c\u8bf7\u5173\u6ce8\u516c\u4f17\u53f7\u52a0\u6211\u5fae\u4fe1\uff1a","title":"\u7b2c\u516d\u6b65\uff1a\u6df1\u5165\u4e86\u89e3Ruia"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/1.%E6%A6%82%E8%A7%88.html","text":"\u6982\u89c8 Why Ruia Write less, Run faster \u2764\ufe0f Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u5176\u8bde\u751f\u7684\u6838\u5fc3\u7406\u5ff5\u5982\u4e0b\uff1a \u66f4\u5c11\u7684\u4ee3\u7801\uff1a\u901a\u7528\u7684\u529f\u80fd\u5c31\u63d2\u4ef6\u5316\uff0c\u8ba9\u5f00\u53d1\u8005\u76f4\u63a5\u5f15\u7528\u5373\u53ef \u66f4\u5feb\u7684\u901f\u5ea6\uff1a\u5f02\u6b65\u9a71\u52a8 \u4ecb\u7ecd Ruia \u5f02\u6b65\u722c\u866b\u6846\u67b6\u4e3b\u8981\u7531\u4ee5\u4e0b\u56db\u5927\u90e8\u5206\u6784\u6210\uff1a Ruia Part Is Required Description Data Items Required \u57fa\u4e8eField\u7c7b\u5b9a\u4e49\u76ee\u6807\u6570\u636e\uff0c\u5b9a\u4e49\u5373\u53ef\u83b7\u53d6\uff0c\u7c7b\u4f3cORM Spider Recommended \u6838\u5fc3\u722c\u866b\u63a7\u5236\u7c7b\uff0c\u8ba9\u722c\u866b\u66f4\u5f3a\u58ee\u4e14\u6613\u7ba1\u7406 Request & Response Optional \u8bf7\u6c42\u548c\u54cd\u5e94\u7c7b\uff0c\u5c01\u88c5\u4e86\u4e00\u4e9b\u901a\u7528\u529f\u80fd\uff0c\u5f00\u53d1\u8005\u4e5f\u53ef\u5355\u72ec\u4f7f\u7528 Middleware Optional \u722c\u866b\u4e2d\u95f4\u4ef6\uff0c\u5bf9\u6bcf\u4e2a\u8bf7\u6c42\u548c\u54cd\u5e94\u8fdb\u884c\u81ea\u5b9a\u4e49 \u542f\u7a0b","title":"\u6982\u89c8"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/1.%E6%A6%82%E8%A7%88.html#_1","text":"","title":"\u6982\u89c8"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/1.%E6%A6%82%E8%A7%88.html#why-ruia","text":"Write less, Run faster \u2764\ufe0f Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u5176\u8bde\u751f\u7684\u6838\u5fc3\u7406\u5ff5\u5982\u4e0b\uff1a \u66f4\u5c11\u7684\u4ee3\u7801\uff1a\u901a\u7528\u7684\u529f\u80fd\u5c31\u63d2\u4ef6\u5316\uff0c\u8ba9\u5f00\u53d1\u8005\u76f4\u63a5\u5f15\u7528\u5373\u53ef \u66f4\u5feb\u7684\u901f\u5ea6\uff1a\u5f02\u6b65\u9a71\u52a8","title":"Why Ruia"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/1.%E6%A6%82%E8%A7%88.html#_2","text":"Ruia \u5f02\u6b65\u722c\u866b\u6846\u67b6\u4e3b\u8981\u7531\u4ee5\u4e0b\u56db\u5927\u90e8\u5206\u6784\u6210\uff1a Ruia Part Is Required Description Data Items Required \u57fa\u4e8eField\u7c7b\u5b9a\u4e49\u76ee\u6807\u6570\u636e\uff0c\u5b9a\u4e49\u5373\u53ef\u83b7\u53d6\uff0c\u7c7b\u4f3cORM Spider Recommended \u6838\u5fc3\u722c\u866b\u63a7\u5236\u7c7b\uff0c\u8ba9\u722c\u866b\u66f4\u5f3a\u58ee\u4e14\u6613\u7ba1\u7406 Request & Response Optional \u8bf7\u6c42\u548c\u54cd\u5e94\u7c7b\uff0c\u5c01\u88c5\u4e86\u4e00\u4e9b\u901a\u7528\u529f\u80fd\uff0c\u5f00\u53d1\u8005\u4e5f\u53ef\u5355\u72ec\u4f7f\u7528 Middleware Optional \u722c\u866b\u4e2d\u95f4\u4ef6\uff0c\u5bf9\u6bcf\u4e2a\u8bf7\u6c42\u548c\u54cd\u5e94\u8fdb\u884c\u81ea\u5b9a\u4e49","title":"\u4ecb\u7ecd"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/1.%E6%A6%82%E8%A7%88.html#_3","text":"","title":"\u542f\u7a0b"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/2.%E5%AE%89%E8%A3%85.html","text":"\u5b89\u88c5 \u5b89\u88c5Python \u5b89\u88c5 Ruia \u524d\uff0c\u9700\u8981\u4f60\u7684\u7cfb\u7edf\u73af\u5883\u5b89\u88c5\u6709 Python3.6+ \u73af\u5883\uff0c\u7531\u4e8e Ruia \u662f\u7b2c\u4e09\u65b9\u5305\uff0c\u6240\u4ee5\u8fd8\u9700\u8981\u4f60\u63d0\u524d\u88c5\u6709 Python \u7684\u5305\u7ba1\u7406\u5de5\u5177 pip \u3002 \u5982\u679c\u786e\u8ba4\u51c6\u5907\u597d\u73af\u5883\uff0c\u8bf7\u8fdb\u5165\u7ec8\u7aef\uff0c\u505a\u73af\u5883\u68c0\u67e5\uff1a [ ~ ] python --version Python 3 .7.3 [ ~ ] pip --version pip 21 .0.1 from ~/anaconda3/lib/python3.7/site-packages/pip ( python 3 .7 ) \u5b89\u88c5Ruia \u8bf7\u8fdb\u5165\u6240\u5728\u9879\u76ee\u73af\u5883\uff0c\u5982\u679c\u6ca1\u6709\u7279\u5b9a\u73af\u5883\u5c31\u9ed8\u8ba4\u4f7f\u7528\u7684\u662f\u7cfb\u7edf\u5168\u5c40 Python \u73af\u5883\uff0c\u7136\u540e\u5229\u7528 pip \u8fdb\u884c\u5b89\u88c5\uff1a # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia \u6821\u9a8c \u8ba9\u6211\u4eec\u770b\u770b ruia \u662f\u5426\u5b89\u88c5\u6210\u529f\uff1a [ ~ ] python Python 3 .7.3 ( default, Mar 27 2019 , 16 :54:48 ) [ Clang 4 .0.1 ( tags/RELEASE_401/final )] :: Anaconda, Inc. on darwin Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> import ruia >>> ruia.__version '0.8.0' \u4e0a\u8ff0 ruia \u5177\u4f53\u7248\u672c\u53f7\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u6821\u9a8c\u3002","title":"\u5b89\u88c5"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/2.%E5%AE%89%E8%A3%85.html#_1","text":"","title":"\u5b89\u88c5"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/2.%E5%AE%89%E8%A3%85.html#python","text":"\u5b89\u88c5 Ruia \u524d\uff0c\u9700\u8981\u4f60\u7684\u7cfb\u7edf\u73af\u5883\u5b89\u88c5\u6709 Python3.6+ \u73af\u5883\uff0c\u7531\u4e8e Ruia \u662f\u7b2c\u4e09\u65b9\u5305\uff0c\u6240\u4ee5\u8fd8\u9700\u8981\u4f60\u63d0\u524d\u88c5\u6709 Python \u7684\u5305\u7ba1\u7406\u5de5\u5177 pip \u3002 \u5982\u679c\u786e\u8ba4\u51c6\u5907\u597d\u73af\u5883\uff0c\u8bf7\u8fdb\u5165\u7ec8\u7aef\uff0c\u505a\u73af\u5883\u68c0\u67e5\uff1a [ ~ ] python --version Python 3 .7.3 [ ~ ] pip --version pip 21 .0.1 from ~/anaconda3/lib/python3.7/site-packages/pip ( python 3 .7 )","title":"\u5b89\u88c5Python"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/2.%E5%AE%89%E8%A3%85.html#ruia","text":"\u8bf7\u8fdb\u5165\u6240\u5728\u9879\u76ee\u73af\u5883\uff0c\u5982\u679c\u6ca1\u6709\u7279\u5b9a\u73af\u5883\u5c31\u9ed8\u8ba4\u4f7f\u7528\u7684\u662f\u7cfb\u7edf\u5168\u5c40 Python \u73af\u5883\uff0c\u7136\u540e\u5229\u7528 pip \u8fdb\u884c\u5b89\u88c5\uff1a # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia","title":"\u5b89\u88c5Ruia"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/2.%E5%AE%89%E8%A3%85.html#_2","text":"\u8ba9\u6211\u4eec\u770b\u770b ruia \u662f\u5426\u5b89\u88c5\u6210\u529f\uff1a [ ~ ] python Python 3 .7.3 ( default, Mar 27 2019 , 16 :54:48 ) [ Clang 4 .0.1 ( tags/RELEASE_401/final )] :: Anaconda, Inc. on darwin Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> import ruia >>> ruia.__version '0.8.0' \u4e0a\u8ff0 ruia \u5177\u4f53\u7248\u672c\u53f7\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u6821\u9a8c\u3002","title":"\u6821\u9a8c"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/3.%E5%AE%9A%E4%B9%89%20Item.html","text":"\u5b9a\u4e49Item","title":"\u5b9a\u4e49Item"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/3.%E5%AE%9A%E4%B9%89%20Item.html#item","text":"","title":"\u5b9a\u4e49Item"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/4.%E8%BF%90%E8%A1%8C%20Spider.html","text":"\u8fd0\u884cSpider","title":"\u8fd0\u884cSpider"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/4.%E8%BF%90%E8%A1%8C%20Spider.html#spider","text":"","title":"\u8fd0\u884cSpider"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/5.%E4%B8%AA%E6%80%A7%E5%8C%96.html","text":"\u4e2a\u6027\u5316","title":"\u4e2a\u6027\u5316"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/5.%E4%B8%AA%E6%80%A7%E5%8C%96.html#_1","text":"","title":"\u4e2a\u6027\u5316"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/6.%E6%8F%92%E4%BB%B6.html","text":"\u63d2\u4ef6","title":"\u63d2\u4ef6"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/6.%E6%8F%92%E4%BB%B6.html#_1","text":"","title":"\u63d2\u4ef6"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/7.%E5%B8%AE%E5%8A%A9.html","text":"\u5e2e\u52a9 \u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u6709\u95ee\u9898\uff1f\u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u4efb\u610f\u5f62\u5f0f\u5bfb\u6c42\u5e2e\u52a9\uff1a \u76f4\u63a5\u5728\u672c\u6587\u4e0b\u65b9\u8fdb\u884c\u7559\u8a00 \u76f4\u63a5\u63d0 Issue \u67e5\u770b Ruia \u66f4\u591a \u57fa\u7840\u6982\u5ff5 \u8054\u7cfb\u6211\uff1a \u5fae\u4fe1 \u5e0c\u671b\u4f60\u5728\u4f7f\u7528 Ruia \u5f00\u53d1\u7684\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u63d0\u5347\u6548\u7387\u3001\u8282\u7701\u65f6\u95f4\uff0c\u54ea\u6015\u53ea\u6709\u4e00\u70b9\u70b9\uff0c\u90a3\u4e5f\u662f\u503c\u5f97\u9ad8\u5174\u7684\u4e00\u4ef6\u4e8b\u3002","title":"\u5e2e\u52a9"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/7.%E5%B8%AE%E5%8A%A9.html#_1","text":"\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u6709\u95ee\u9898\uff1f\u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u4efb\u610f\u5f62\u5f0f\u5bfb\u6c42\u5e2e\u52a9\uff1a \u76f4\u63a5\u5728\u672c\u6587\u4e0b\u65b9\u8fdb\u884c\u7559\u8a00 \u76f4\u63a5\u63d0 Issue \u67e5\u770b Ruia \u66f4\u591a \u57fa\u7840\u6982\u5ff5 \u8054\u7cfb\u6211\uff1a \u5fae\u4fe1 \u5e0c\u671b\u4f60\u5728\u4f7f\u7528 Ruia \u5f00\u53d1\u7684\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u63d0\u5347\u6548\u7387\u3001\u8282\u7701\u65f6\u95f4\uff0c\u54ea\u6015\u53ea\u6709\u4e00\u70b9\u70b9\uff0c\u90a3\u4e5f\u662f\u503c\u5f97\u9ad8\u5174\u7684\u4e00\u4ef6\u4e8b\u3002","title":"\u5e2e\u52a9"},{"location":"cn/01_%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/_index.html","text":"weight: 2 bookFlatSection: false title: \"\u5165\u95e8\u6307\u5357\"","title":" index"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/1.Request.html","text":"Request Request \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u65b9\u4fbf\u5730\u5904\u7406\u7f51\u7edc\u8bf7\u6c42\uff0c\u6700\u7ec8\u8fd4\u56de\u4e00\u4e2a Response \u5bf9\u8c61\u3002 \u4e3b\u8981\u63d0\u4f9b\u7684\u65b9\u6cd5\u6709\uff1a - Request().fetch \uff1a\u8bf7\u6c42\u4e00\u4e2a\u7f51\u9875\u8d44\u6e90\uff0c\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528 - Request().fetch_callback \uff1a\u4e3a Spider \u7c7b\u63d0\u4f9b\u7684\u548c\u6838\u5fc3\u65b9\u6cd5 Core arguments url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 method\uff1a\u8bf7\u6c42\u7684\u65b9\u6cd5\uff0c GET \u6216\u8005 POST callback\uff1a\u56de\u8c03\u51fd\u6570 headers\uff1a\u8bf7\u6c42\u5934 load_js\uff1a\u76ee\u6807\u7f51\u9875\u662f\u5426\u9700\u8981\u52a0\u8f7djs metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e request_config\uff1a\u8bf7\u6c42\u914d\u7f6e request_session\uff1a aiohttp \u7684\u8bf7\u6c42session aiohttp_kwargs\uff1a\u8bf7\u6c42\u76ee\u6807\u8d44\u6e90\u53ef\u5b9a\u4e49\u7684\u5176\u4ed6\u53c2\u6570 Usage \u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c Request \u9664\u4e86\u9700\u8981\u7ed3\u5408 Spider \u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\uff1a import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}> How It Works? Request \u901a\u8fc7\u5bf9 aiohttp \u548c pyppeteer \u7684\u5c01\u88c5\u6765\u5b9e\u73b0\u5bf9\u7f51\u9875\u8d44\u6e90\u7684\u5f02\u6b65\u8bf7\u6c42","title":"Request"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/1.Request.html#request","text":"Request \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u65b9\u4fbf\u5730\u5904\u7406\u7f51\u7edc\u8bf7\u6c42\uff0c\u6700\u7ec8\u8fd4\u56de\u4e00\u4e2a Response \u5bf9\u8c61\u3002 \u4e3b\u8981\u63d0\u4f9b\u7684\u65b9\u6cd5\u6709\uff1a - Request().fetch \uff1a\u8bf7\u6c42\u4e00\u4e2a\u7f51\u9875\u8d44\u6e90\uff0c\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528 - Request().fetch_callback \uff1a\u4e3a Spider \u7c7b\u63d0\u4f9b\u7684\u548c\u6838\u5fc3\u65b9\u6cd5","title":"Request"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/1.Request.html#core-arguments","text":"url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 method\uff1a\u8bf7\u6c42\u7684\u65b9\u6cd5\uff0c GET \u6216\u8005 POST callback\uff1a\u56de\u8c03\u51fd\u6570 headers\uff1a\u8bf7\u6c42\u5934 load_js\uff1a\u76ee\u6807\u7f51\u9875\u662f\u5426\u9700\u8981\u52a0\u8f7djs metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e request_config\uff1a\u8bf7\u6c42\u914d\u7f6e request_session\uff1a aiohttp \u7684\u8bf7\u6c42session aiohttp_kwargs\uff1a\u8bf7\u6c42\u76ee\u6807\u8d44\u6e90\u53ef\u5b9a\u4e49\u7684\u5176\u4ed6\u53c2\u6570","title":"Core arguments"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/1.Request.html#usage","text":"\u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c Request \u9664\u4e86\u9700\u8981\u7ed3\u5408 Spider \u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\uff1a import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}>","title":"Usage"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/1.Request.html#how-it-works","text":"Request \u901a\u8fc7\u5bf9 aiohttp \u548c pyppeteer \u7684\u5c01\u88c5\u6765\u5b9e\u73b0\u5bf9\u7f51\u9875\u8d44\u6e90\u7684\u5f02\u6b65\u8bf7\u6c42","title":"How It Works?"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/2.Response.html","text":"Response Response \u7684\u76ee\u7684\u662f\u8fd4\u56de\u4e00\u4e2a\u7edf\u4e00\u4e14\u53cb\u597d\u7684\u54cd\u5e94\u5bf9\u8c61\uff0c\u4e3b\u8981\u5c5e\u6027\u5982\u4e0b\uff1a - url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 - metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e - html\uff1a\u6e90\u7f51\u7ad9\u8fd4\u56de\u7684\u8d44\u6e90\u6570\u636e - cookies\uff1a\u7f51\u7ad9 cookies - history\uff1a\u8bbf\u95ee\u5386\u53f2 - headers\uff1a\u8bf7\u6c42\u5934 - status\uff1a\u8bf7\u6c42\u72b6\u6001\u7801","title":"Response"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/2.Response.html#response","text":"Response \u7684\u76ee\u7684\u662f\u8fd4\u56de\u4e00\u4e2a\u7edf\u4e00\u4e14\u53cb\u597d\u7684\u54cd\u5e94\u5bf9\u8c61\uff0c\u4e3b\u8981\u5c5e\u6027\u5982\u4e0b\uff1a - url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 - metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e - html\uff1a\u6e90\u7f51\u7ad9\u8fd4\u56de\u7684\u8d44\u6e90\u6570\u636e - cookies\uff1a\u7f51\u7ad9 cookies - history\uff1a\u8bbf\u95ee\u5386\u53f2 - headers\uff1a\u8bf7\u6c42\u5934 - status\uff1a\u8bf7\u6c42\u72b6\u6001\u7801","title":"Response"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/3.Item.html","text":"Item Item \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5b9a\u4e49\u4ee5\u53ca\u901a\u8fc7\u4e00\u5b9a\u7684\u89c4\u5219\u63d0\u53d6\u6e90\u7f51\u9875\u4e2d\u7684\u76ee\u6807\u6570\u636e\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4e00\u4e0b\u4e24\u4e2a\u65b9\u6cd5\uff1a - get_item \uff1a\u9488\u5bf9\u9875\u9762\u5355\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6 - get_items \uff1a\u9488\u5bf9\u9875\u9762\u591a\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6 Core arguments get_item \u548c get_items \u65b9\u6cd5\u63a5\u6536\u7684\u53c2\u6570\u662f\u4e00\u81f4\u7684\uff1a - html\uff1a\u7f51\u9875\u6e90\u7801 - url\uff1a\u7f51\u9875\u94fe\u63a5 - html_etree\uff1aetree._Element\u5bf9\u8c61 Usage \u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c\u4e0d\u8bba\u662f\u6e90\u7f51\u7ad9\u94fe\u63a5\u6216\u8005\u7f51\u7ad9 HTML \u6e90\u7801\uff0c\u751a\u81f3\u662f\u7ecf\u8fc7 lxml \u5904\u7406\u8fc7\u7684 etree._Element \u5bf9\u8c61\uff0c Item \u80fd\u63a5\u6536\u8fd9\u4e09\u79cd\u7c7b\u578b\u7684\u8f93\u5165\u5e76\u8fdb\u884c\u5904\u7406 import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async_func = HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ) items = asyncio . get_event_loop () . run_until_complete ( async_func ) for item in items : print ( item . title , item . url ) \u6709\u65f6\u4f60\u4f1a\u9047\u89c1\u8fd9\u6837\u4e00\u79cd\u60c5\u51b5\uff0c\u4f8b\u5982\u722c\u53d6Github\u7684Issue\u65f6\uff0c\u4f60\u4f1a\u53d1\u73b0\u4e00\u4e2aIssue\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2aTag\u3002 \u8fd9\u65f6\uff0c\u5c06Tag\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684 Item \u6765\u63d0\u53d6\u662f\u4e0d\u5212\u7b97\u7684\uff0c \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 Field \u5b57\u6bb5\u7684 many=True \u53c2\u6570\uff0c\u4f7f\u8fd9\u4e2a\u5b57\u6bb5\u8fd4\u56de\u4e00\u4e2a\u5217\u8868\u3002 import asyncio from ruia import Item , TextField , AttrField class GithiubIssueItem ( Item ): title = TextField ( css_select = 'title' ) tags = AttrField ( css_select = 'a.IssueLabel' , attr = 'data-name' , many = True ) item = asyncio . run ( GithiubIssueItem . get_item ( url = 'https://github.com/pypa/pip/issues/72' )) assert isinstance ( item . tags , list ) \u540c\u6837\uff0c TextField \u4e5f\u652f\u6301 many \u53c2\u6570\u3002 How It Works? \u6700\u7ec8 Item \u7c7b\u4f1a\u5c06\u8f93\u5165\u6700\u7ec8\u8f6c\u5316\u4e3a etree._Element \u5bf9\u8c61\u8fdb\u884c\u5904\u7406\uff0c\u7136\u540e\u5229\u7528\u5143\u7c7b\u7684\u601d\u60f3\u5c06\u6bcf\u4e00\u4e2a Field \u6784\u9020\u7684\u5c5e\u6027\u8ba1\u7b97\u4e3a\u6e90\u7f51\u9875\u4e0a\u5bf9\u5e94\u7684\u771f\u5b9e\u6570\u636e","title":"Item"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/3.Item.html#item","text":"Item \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5b9a\u4e49\u4ee5\u53ca\u901a\u8fc7\u4e00\u5b9a\u7684\u89c4\u5219\u63d0\u53d6\u6e90\u7f51\u9875\u4e2d\u7684\u76ee\u6807\u6570\u636e\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4e00\u4e0b\u4e24\u4e2a\u65b9\u6cd5\uff1a - get_item \uff1a\u9488\u5bf9\u9875\u9762\u5355\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6 - get_items \uff1a\u9488\u5bf9\u9875\u9762\u591a\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6","title":"Item"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/3.Item.html#core-arguments","text":"get_item \u548c get_items \u65b9\u6cd5\u63a5\u6536\u7684\u53c2\u6570\u662f\u4e00\u81f4\u7684\uff1a - html\uff1a\u7f51\u9875\u6e90\u7801 - url\uff1a\u7f51\u9875\u94fe\u63a5 - html_etree\uff1aetree._Element\u5bf9\u8c61","title":"Core arguments"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/3.Item.html#usage","text":"\u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c\u4e0d\u8bba\u662f\u6e90\u7f51\u7ad9\u94fe\u63a5\u6216\u8005\u7f51\u7ad9 HTML \u6e90\u7801\uff0c\u751a\u81f3\u662f\u7ecf\u8fc7 lxml \u5904\u7406\u8fc7\u7684 etree._Element \u5bf9\u8c61\uff0c Item \u80fd\u63a5\u6536\u8fd9\u4e09\u79cd\u7c7b\u578b\u7684\u8f93\u5165\u5e76\u8fdb\u884c\u5904\u7406 import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async_func = HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ) items = asyncio . get_event_loop () . run_until_complete ( async_func ) for item in items : print ( item . title , item . url ) \u6709\u65f6\u4f60\u4f1a\u9047\u89c1\u8fd9\u6837\u4e00\u79cd\u60c5\u51b5\uff0c\u4f8b\u5982\u722c\u53d6Github\u7684Issue\u65f6\uff0c\u4f60\u4f1a\u53d1\u73b0\u4e00\u4e2aIssue\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2aTag\u3002 \u8fd9\u65f6\uff0c\u5c06Tag\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684 Item \u6765\u63d0\u53d6\u662f\u4e0d\u5212\u7b97\u7684\uff0c \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 Field \u5b57\u6bb5\u7684 many=True \u53c2\u6570\uff0c\u4f7f\u8fd9\u4e2a\u5b57\u6bb5\u8fd4\u56de\u4e00\u4e2a\u5217\u8868\u3002 import asyncio from ruia import Item , TextField , AttrField class GithiubIssueItem ( Item ): title = TextField ( css_select = 'title' ) tags = AttrField ( css_select = 'a.IssueLabel' , attr = 'data-name' , many = True ) item = asyncio . run ( GithiubIssueItem . get_item ( url = 'https://github.com/pypa/pip/issues/72' )) assert isinstance ( item . tags , list ) \u540c\u6837\uff0c TextField \u4e5f\u652f\u6301 many \u53c2\u6570\u3002","title":"Usage"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/3.Item.html#how-it-works","text":"\u6700\u7ec8 Item \u7c7b\u4f1a\u5c06\u8f93\u5165\u6700\u7ec8\u8f6c\u5316\u4e3a etree._Element \u5bf9\u8c61\u8fdb\u884c\u5904\u7406\uff0c\u7136\u540e\u5229\u7528\u5143\u7c7b\u7684\u601d\u60f3\u5c06\u6bcf\u4e00\u4e2a Field \u6784\u9020\u7684\u5c5e\u6027\u8ba1\u7b97\u4e3a\u6e90\u7f51\u9875\u4e0a\u5bf9\u5e94\u7684\u771f\u5b9e\u6570\u636e","title":"How It Works?"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/4.Field.html","text":"Selector Selector \u901a\u8fc7 Field \u7c7b\u5b9e\u73b0\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86 CSS Selector \u548c XPath \u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u5177\u4f53\u7531\u4e0b\u9762\u4e24\u4e2a\u7c7b\u5b9e\u73b0\uff1a - AttrField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684\u5c5e\u6027\u6570\u636e - TextField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684text\u6570\u636e Core arguments \u6240\u6709\u7684 Field \u5171\u6709\u7684\u53c2\u6570\uff1a - default: str, \u8bbe\u7f6e\u9ed8\u8ba4\u503c\uff0c\u5efa\u8bae\u5b9a\u4e49\uff0c\u5426\u5219\u627e\u4e0d\u5230\u5b57\u6bb5\u65f6\u4f1a\u62a5\u9519 - many: bool, \u8fd4\u56de\u503c\u5c06\u662f\u4e00\u4e2a\u5217\u8868 AttrField \u3001 TextField \u3001 HtmlField \u5171\u7528\u53c2\u6570\uff1a - css_select\uff1astr, \u5229\u7528 CSS Selector \u63d0\u53d6\u76ee\u6807\u6570\u636e - xpath_select\uff1astr, \u5229\u7528 XPath \u63d0\u53d6\u76ee\u6807\u6570\u636e AttrField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - attr\uff1a\u76ee\u6807\u6807\u7b7e\u5c5e\u6027 RegexField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - re_select: str, \u6b63\u5219\u8868\u8fbe\u5f0f\u5b57\u7b26\u4e32 Usage from lxml import etree from ruia import AttrField , TextField , HtmlField , RegexField HTML = \"\"\" <html> <head> <title>ruia</title> </head> <body>\u00ac <p> <a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a> </p> </body> </html> \"\"\" html = etree . HTML ( HTML ) def test_css_select (): field = TextField ( css_select = \"head title\" ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_xpath_select (): field = TextField ( xpath_select = '/html/head/title' ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_attr_field (): attr_field = AttrField ( css_select = \"p a.test_link\" , attr = 'href' ) value = attr_field . extract ( html_etree = html ) assert value == \"https://github.com/howie6879/ruia\" def test_html_field (): field = HtmlField ( css_select = \"a.test_link\" ) assert field . extract ( html_etree = html ) == '<a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a>' def test_re_field (): field = RegexField ( re_select = '<title>(.*?)</title>' ) href = field . extract ( html = HTML ) assert href == 'ruia' How It Works? \u5b9a\u597d CSS Selector \u6216 XPath \u89c4\u5219\uff0c\u7136\u540e\u5229\u7528 lxml \u5b9e\u73b0\u5bf9\u76ee\u6807 html \u8fdb\u884c\u76ee\u6807\u6570\u636e\u7684\u63d0\u53d6 \u5173\u4e8e RegexField \u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u9605 \u82f1\u6587\u6587\u6863 \u3002","title":"Selector"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/4.Field.html#selector","text":"Selector \u901a\u8fc7 Field \u7c7b\u5b9e\u73b0\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86 CSS Selector \u548c XPath \u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u5177\u4f53\u7531\u4e0b\u9762\u4e24\u4e2a\u7c7b\u5b9e\u73b0\uff1a - AttrField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684\u5c5e\u6027\u6570\u636e - TextField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684text\u6570\u636e","title":"Selector"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/4.Field.html#core-arguments","text":"\u6240\u6709\u7684 Field \u5171\u6709\u7684\u53c2\u6570\uff1a - default: str, \u8bbe\u7f6e\u9ed8\u8ba4\u503c\uff0c\u5efa\u8bae\u5b9a\u4e49\uff0c\u5426\u5219\u627e\u4e0d\u5230\u5b57\u6bb5\u65f6\u4f1a\u62a5\u9519 - many: bool, \u8fd4\u56de\u503c\u5c06\u662f\u4e00\u4e2a\u5217\u8868 AttrField \u3001 TextField \u3001 HtmlField \u5171\u7528\u53c2\u6570\uff1a - css_select\uff1astr, \u5229\u7528 CSS Selector \u63d0\u53d6\u76ee\u6807\u6570\u636e - xpath_select\uff1astr, \u5229\u7528 XPath \u63d0\u53d6\u76ee\u6807\u6570\u636e AttrField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - attr\uff1a\u76ee\u6807\u6807\u7b7e\u5c5e\u6027 RegexField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - re_select: str, \u6b63\u5219\u8868\u8fbe\u5f0f\u5b57\u7b26\u4e32","title":"Core arguments"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/4.Field.html#usage","text":"from lxml import etree from ruia import AttrField , TextField , HtmlField , RegexField HTML = \"\"\" <html> <head> <title>ruia</title> </head> <body>\u00ac <p> <a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a> </p> </body> </html> \"\"\" html = etree . HTML ( HTML ) def test_css_select (): field = TextField ( css_select = \"head title\" ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_xpath_select (): field = TextField ( xpath_select = '/html/head/title' ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_attr_field (): attr_field = AttrField ( css_select = \"p a.test_link\" , attr = 'href' ) value = attr_field . extract ( html_etree = html ) assert value == \"https://github.com/howie6879/ruia\" def test_html_field (): field = HtmlField ( css_select = \"a.test_link\" ) assert field . extract ( html_etree = html ) == '<a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a>' def test_re_field (): field = RegexField ( re_select = '<title>(.*?)</title>' ) href = field . extract ( html = HTML ) assert href == 'ruia'","title":"Usage"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/4.Field.html#how-it-works","text":"\u5b9a\u597d CSS Selector \u6216 XPath \u89c4\u5219\uff0c\u7136\u540e\u5229\u7528 lxml \u5b9e\u73b0\u5bf9\u76ee\u6807 html \u8fdb\u884c\u76ee\u6807\u6570\u636e\u7684\u63d0\u53d6","title":"How It Works?"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/4.Field.html#regexfield","text":"\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u9605 \u82f1\u6587\u6587\u6863 \u3002","title":"\u5173\u4e8eRegexField"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/5.Spider.html","text":"Spider Spider \u662f\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3\uff0c\u5b83\u5c06Item\u3001Middleware\u3001Request\u3001\u7b49\u6a21\u5757\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u4e3a\u4f60\u6784\u9020\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b\u7a0b\u5e8f\u3002\u4f60\u53ea\u9700\u8981\u5173\u6ce8\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a - Spider.start \uff1a\u722c\u866b\u7684\u542f\u52a8\u51fd\u6570 - parse \uff1a\u722c\u866b\u7684\u7b2c\u4e00\u5c42\u89e3\u6790\u51fd\u6570\uff0c\u7ee7\u627f Spider \u7684\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0\u8fd9\u4e2a\u51fd\u6570 Core arguments Spider.start \u7684\u53c2\u6570\u5982\u4e0b\uff1a - after_start\uff1a\u722c\u866b\u542f\u52a8\u540e\u7684\u94a9\u5b50\u51fd\u6570 - before_stop\uff1a\u722c\u866b\u542f\u52a8\u524d\u7684\u94a9\u5b50\u51fd\u6570 - middleware\uff1a\u4e2d\u95f4\u4ef6\u7c7b\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a\u4e2d\u95f4\u4ef6 Middleware() \u5b9e\u4f8b\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u7ec4 Middleware() \u5b9e\u4f8b\u7ec4\u6210\u7684\u5217\u8868 - loop\uff1a\u4e8b\u4ef6\u5faa\u73af Usage import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () How It Works? Spider \u4f1a\u81ea\u52a8\u8bfb\u53d6 start_urls \u5217\u8868\u91cc\u9762\u7684\u8bf7\u6c42\u94fe\u63a5\uff0c\u7136\u540e\u7ef4\u62a4\u4e00\u4e2a\u5f02\u6b65\u961f\u5217\uff0c\u4f7f\u7528\u751f\u4ea7\u6d88\u8d39\u8005\u6a21\u5f0f\u8fdb\u884c\u722c\u53d6\uff0c\u722c\u866b\u7a0b\u5e8f\u4e00\u76f4\u5faa\u73af\u76f4\u5230\u6ca1\u6709\u8c03\u7528\u51fd\u6570\u4e3a\u6b62","title":"Spider"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/5.Spider.html#spider","text":"Spider \u662f\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3\uff0c\u5b83\u5c06Item\u3001Middleware\u3001Request\u3001\u7b49\u6a21\u5757\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u4e3a\u4f60\u6784\u9020\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b\u7a0b\u5e8f\u3002\u4f60\u53ea\u9700\u8981\u5173\u6ce8\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a - Spider.start \uff1a\u722c\u866b\u7684\u542f\u52a8\u51fd\u6570 - parse \uff1a\u722c\u866b\u7684\u7b2c\u4e00\u5c42\u89e3\u6790\u51fd\u6570\uff0c\u7ee7\u627f Spider \u7684\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0\u8fd9\u4e2a\u51fd\u6570","title":"Spider"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/5.Spider.html#core-arguments","text":"Spider.start \u7684\u53c2\u6570\u5982\u4e0b\uff1a - after_start\uff1a\u722c\u866b\u542f\u52a8\u540e\u7684\u94a9\u5b50\u51fd\u6570 - before_stop\uff1a\u722c\u866b\u542f\u52a8\u524d\u7684\u94a9\u5b50\u51fd\u6570 - middleware\uff1a\u4e2d\u95f4\u4ef6\u7c7b\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a\u4e2d\u95f4\u4ef6 Middleware() \u5b9e\u4f8b\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u7ec4 Middleware() \u5b9e\u4f8b\u7ec4\u6210\u7684\u5217\u8868 - loop\uff1a\u4e8b\u4ef6\u5faa\u73af","title":"Core arguments"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/5.Spider.html#usage","text":"import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start ()","title":"Usage"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/5.Spider.html#how-it-works","text":"Spider \u4f1a\u81ea\u52a8\u8bfb\u53d6 start_urls \u5217\u8868\u91cc\u9762\u7684\u8bf7\u6c42\u94fe\u63a5\uff0c\u7136\u540e\u7ef4\u62a4\u4e00\u4e2a\u5f02\u6b65\u961f\u5217\uff0c\u4f7f\u7528\u751f\u4ea7\u6d88\u8d39\u8005\u6a21\u5f0f\u8fdb\u884c\u722c\u53d6\uff0c\u722c\u866b\u7a0b\u5e8f\u4e00\u76f4\u5faa\u73af\u76f4\u5230\u6ca1\u6709\u8c03\u7528\u51fd\u6570\u4e3a\u6b62","title":"How It Works?"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/6.Middleware.html","text":"Middleware Middleware \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5728\u8fdb\u884c\u4e00\u4e2a\u8bf7\u6c42\u7684\u524d\u540e\u8fdb\u884c\u4e00\u4e9b\u5904\u7406\uff0c\u6bd4\u5982\u76d1\u542c\u8bf7\u6c42\u6216\u8005\u54cd\u5e94\uff1a - Middleware().request \uff1a\u5728\u8bf7\u6c42\u524d\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5 - Middleware().response \uff1a\u5728\u8bf7\u6c42\u540e\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5 Usage \u4f7f\u7528\u4e2d\u95f4\u4ef6\u6709\u4e24\u70b9\u9700\u8981\u6ce8\u610f\uff0c\u4e00\u4e2a\u662f\u5904\u7406\u51fd\u6570\u9700\u8981\u5e26\u4e0a\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a\u662f\u4e0d\u9700\u8981\u8fd4\u56de\u503c\uff0c\u5177\u4f53\u4f7f\u7528\u5982\u4e0b\uff1a from ruia import Middleware middleware = Middleware () @middleware . request async def print_on_request ( spider_ins , request ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u524d\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"request: print when a request is received\" ) @middleware . response async def print_on_response ( spider_ins , request , response ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u540e\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 response: Response\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"response: print when a response is received\" ) How It Works? Middleware \u901a\u8fc7\u88c5\u9970\u5668\u6765\u5b9e\u73b0\u5bf9\u51fd\u6570\u7684\u56de\u8c03\uff0c\u4ece\u800c\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u4f18\u96c5\u7684\u5b9e\u73b0\u4e2d\u95f4\u4ef6\u529f\u80fd\uff0c Middleware \u7c7b\u4e2d\u7684\u4e24\u4e2a\u5c5e\u6027 request_middleware \u548c response_middleware \u5206\u522b\u7ef4\u62a4\u7740\u4e00\u4e2a\u961f\u5217\u6765\u5904\u7406\u5f00\u53d1\u8005\u5b9a\u4e49\u7684\u5904\u7406\u51fd\u6570","title":"Middleware"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/6.Middleware.html#middleware","text":"Middleware \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5728\u8fdb\u884c\u4e00\u4e2a\u8bf7\u6c42\u7684\u524d\u540e\u8fdb\u884c\u4e00\u4e9b\u5904\u7406\uff0c\u6bd4\u5982\u76d1\u542c\u8bf7\u6c42\u6216\u8005\u54cd\u5e94\uff1a - Middleware().request \uff1a\u5728\u8bf7\u6c42\u524d\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5 - Middleware().response \uff1a\u5728\u8bf7\u6c42\u540e\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5","title":"Middleware"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/6.Middleware.html#usage","text":"\u4f7f\u7528\u4e2d\u95f4\u4ef6\u6709\u4e24\u70b9\u9700\u8981\u6ce8\u610f\uff0c\u4e00\u4e2a\u662f\u5904\u7406\u51fd\u6570\u9700\u8981\u5e26\u4e0a\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a\u662f\u4e0d\u9700\u8981\u8fd4\u56de\u503c\uff0c\u5177\u4f53\u4f7f\u7528\u5982\u4e0b\uff1a from ruia import Middleware middleware = Middleware () @middleware . request async def print_on_request ( spider_ins , request ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u524d\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"request: print when a request is received\" ) @middleware . response async def print_on_response ( spider_ins , request , response ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u540e\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 response: Response\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"response: print when a response is received\" )","title":"Usage"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/6.Middleware.html#how-it-works","text":"Middleware \u901a\u8fc7\u88c5\u9970\u5668\u6765\u5b9e\u73b0\u5bf9\u51fd\u6570\u7684\u56de\u8c03\uff0c\u4ece\u800c\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u4f18\u96c5\u7684\u5b9e\u73b0\u4e2d\u95f4\u4ef6\u529f\u80fd\uff0c Middleware \u7c7b\u4e2d\u7684\u4e24\u4e2a\u5c5e\u6027 request_middleware \u548c response_middleware \u5206\u522b\u7ef4\u62a4\u7740\u4e00\u4e2a\u961f\u5217\u6765\u5904\u7406\u5f00\u53d1\u8005\u5b9a\u4e49\u7684\u5904\u7406\u51fd\u6570","title":"How It Works?"},{"location":"cn/02_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/_index.html","text":"","title":"\u57fa\u7840\u6982\u5ff5"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/1.%E6%90%AD%E5%BB%BA%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83.html","text":"\u642d\u5efa\u5f00\u53d1\u73af\u5883","title":"\u642d\u5efa\u5f00\u53d1\u73af\u5883"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/1.%E6%90%AD%E5%BB%BA%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83.html#_1","text":"","title":"\u642d\u5efa\u5f00\u53d1\u73af\u5883"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/2.%E6%B5%85%E8%B0%88%20Ruia%20%E6%9E%B6%E6%9E%84.html","text":"\u6d45\u8c08Ruia\u67b6\u6784","title":"\u6d45\u8c08Ruia\u67b6\u6784"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/2.%E6%B5%85%E8%B0%88%20Ruia%20%E6%9E%B6%E6%9E%84.html#ruia","text":"","title":"\u6d45\u8c08Ruia\u67b6\u6784"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/3.%E4%B8%BA%20Ruia%20%E7%BC%96%E5%86%99%E6%8F%92%E4%BB%B6.html","text":"\u4e3aRuia\u7f16\u5199\u63d2\u4ef6 \u6269\u5c55\u7684\u76ee\u7684\u662f\u5c06\u4e00\u4e9b\u5728\u722c\u866b\u7a0b\u5e8f\u4e2d\u9891\u7e41\u4f7f\u7528\u7684\u529f\u80fd\u5c01\u88c5\u8d77\u6765\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u5757\u4f9b\u7b2c\u4e09\u65b9\u8c03\u7528\uff0c Ruia \u901a\u8fc7 Middleware \u6765\u8ba9\u5f00\u53d1\u8005\u5feb\u901f\u5730\u5b9e\u73b0\u7b2c\u4e09\u65b9\u6269\u5c55 \u524d\u9762\u4e00\u8282\u5df2\u7ecf\u8bf4\u8fc7\uff0c Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u7136\u540e\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u529f\u80fd\uff0c\u5c31\u662f\u5728\u8bf7\u6c42\u5934\u91cc\u9762\u52a0\u5165 User-Agent \u53ef\u80fd\u4efb\u610f\u4e00\u4e2a\u722c\u866b\u90fd\u4f1a\u9700\u8981\u81ea\u52a8\u6dfb\u52a0\u968f\u673a User-Agent \u7684\u529f\u80fd\uff0c\u8ba9\u6211\u5c06\u8fd9\u4e2a\u529f\u80fd\u5c01\u88c5\u4e0b\uff0c\u4f7f\u5176\u6210\u4e3a Ruia \u7684\u4e00\u4e2a\u7b2c\u4e09\u65b9\u6269\u5c55\u5427\uff0c\u8ba9\u6211\u4eec\u73b0\u5728\u5c31\u5f00\u59cb\u5427 Creating a project \u9879\u76ee\u540d\u79f0\u4e3a\uff1a ruia-ua \uff0c\u56e0\u4e3a Ruia \u57fa\u4e8e Python3.6+ \uff0c\u6240\u4ee5\u6269\u5c55 ruia-ua \u4e5f\u4ea6\u7136\uff0c\u5047\u8bbe\u4f60\u6b64\u65f6\u4f7f\u7528\u7684\u662f Python3.6+ \uff0c\u8bf7\u6309\u7167\u5982\u4e0b\u64cd\u4f5c\uff1a # \u5b89\u88c5\u5305\u7ba1\u7406\u5de5\u5177 pipenv pip install pipenv # \u521b\u5efa\u9879\u76ee\u6587\u4ef6\u5939 mkdir ruia-ua cd ruia-ua # \u5b89\u88c5\u865a\u62df\u73af\u5883 pipenv install # \u5b89\u88c5 ruia pipenv install ruia # \u5b89\u88c5 aiofiles pipenv install aiofiles # \u521b\u5efa\u9879\u76ee\u76ee\u5f55 mkdir ruia_ua cd ruia_ua # \u5b9e\u73b0\u4ee3\u7801\u653e\u5728\u8fd9\u91cc touch __init__.py \u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a ruia-ua \u251c\u2500\u2500 LICENSE # \u5f00\u6e90\u534f\u8bae \u251c\u2500\u2500 Pipfile # pipenv \u7ba1\u7406\u5de5\u5177\u751f\u6210\u6587\u4ef6 \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # \u4ee3\u7801\u5b9e\u73b0 \u2502 \u2514\u2500\u2500 user_agents.txt # \u968f\u673aua\u96c6\u5408 \u2514\u2500\u2500 setup.py First extension user_agents.txt \u6587\u4ef6\u5305\u542b\u4e86\u5404\u79cd ua \uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u53ea\u8981\u5229\u7528 ruia \u7684 Middleware \u5b9e\u73b0\u5728\u6bcf\u6b21\u8bf7\u6c42\u524d\u968f\u673a\u6dfb\u52a0\u4e00\u4e2a User-Agent \u5373\u53ef\uff0c\u5b9e\u73b0\u4ee3\u7801\u5982\u4e0b\uff1a import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware . request async def add_random_ua ( spider_ins , request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } \u7f16\u5199\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5c06 ruia-ua \u4e0a\u4f20\u81f3\u793e\u533a\uff0c\u8fd9\u6837\u6240\u6709\u7684 ruia \u4f7f\u7528\u8005\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u4f60\u7f16\u5199\u7684\u7b2c\u4e09\u65b9\u6269\u5c55\uff0c\u591a\u4e48\u7f8e\u597d\u7684\u4e00\u4ef6\u4e8b Usage \u6240\u6709\u7684\u722c\u866b\u7a0b\u5e8f\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 ruia-ua \u6765\u5b9e\u73b0\u81ea\u52a8\u6dfb\u52a0 User-Agent pip install ruia-ua \u4e3e\u4e2a\u5b9e\u9645\u4f7f\u7528\u7684\u4f8b\u5b50\uff1a from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) \u7b2c\u4e09\u65b9\u6269\u5c55\u7684\u5b9e\u73b0\u5c06\u4f1a\u5927\u5927\u51cf\u5c11\u722c\u866b\u5de5\u7a0b\u5e08\u7684\u5f00\u53d1\u5468\u671f\uff0c ruia \u975e\u5e38\u5e0c\u671b\u4f60\u53ef\u4ee5\u5f00\u53d1\u5e76\u63d0\u4ea4\u81ea\u5df1\u7684\u7b2c\u4e09\u65b9\u6269\u5c55","title":"\u4e3aRuia\u7f16\u5199\u63d2\u4ef6"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/3.%E4%B8%BA%20Ruia%20%E7%BC%96%E5%86%99%E6%8F%92%E4%BB%B6.html#ruia","text":"\u6269\u5c55\u7684\u76ee\u7684\u662f\u5c06\u4e00\u4e9b\u5728\u722c\u866b\u7a0b\u5e8f\u4e2d\u9891\u7e41\u4f7f\u7528\u7684\u529f\u80fd\u5c01\u88c5\u8d77\u6765\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u5757\u4f9b\u7b2c\u4e09\u65b9\u8c03\u7528\uff0c Ruia \u901a\u8fc7 Middleware \u6765\u8ba9\u5f00\u53d1\u8005\u5feb\u901f\u5730\u5b9e\u73b0\u7b2c\u4e09\u65b9\u6269\u5c55 \u524d\u9762\u4e00\u8282\u5df2\u7ecf\u8bf4\u8fc7\uff0c Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u7136\u540e\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u529f\u80fd\uff0c\u5c31\u662f\u5728\u8bf7\u6c42\u5934\u91cc\u9762\u52a0\u5165 User-Agent \u53ef\u80fd\u4efb\u610f\u4e00\u4e2a\u722c\u866b\u90fd\u4f1a\u9700\u8981\u81ea\u52a8\u6dfb\u52a0\u968f\u673a User-Agent \u7684\u529f\u80fd\uff0c\u8ba9\u6211\u5c06\u8fd9\u4e2a\u529f\u80fd\u5c01\u88c5\u4e0b\uff0c\u4f7f\u5176\u6210\u4e3a Ruia \u7684\u4e00\u4e2a\u7b2c\u4e09\u65b9\u6269\u5c55\u5427\uff0c\u8ba9\u6211\u4eec\u73b0\u5728\u5c31\u5f00\u59cb\u5427","title":"\u4e3aRuia\u7f16\u5199\u63d2\u4ef6"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/3.%E4%B8%BA%20Ruia%20%E7%BC%96%E5%86%99%E6%8F%92%E4%BB%B6.html#creating-a-project","text":"\u9879\u76ee\u540d\u79f0\u4e3a\uff1a ruia-ua \uff0c\u56e0\u4e3a Ruia \u57fa\u4e8e Python3.6+ \uff0c\u6240\u4ee5\u6269\u5c55 ruia-ua \u4e5f\u4ea6\u7136\uff0c\u5047\u8bbe\u4f60\u6b64\u65f6\u4f7f\u7528\u7684\u662f Python3.6+ \uff0c\u8bf7\u6309\u7167\u5982\u4e0b\u64cd\u4f5c\uff1a # \u5b89\u88c5\u5305\u7ba1\u7406\u5de5\u5177 pipenv pip install pipenv # \u521b\u5efa\u9879\u76ee\u6587\u4ef6\u5939 mkdir ruia-ua cd ruia-ua # \u5b89\u88c5\u865a\u62df\u73af\u5883 pipenv install # \u5b89\u88c5 ruia pipenv install ruia # \u5b89\u88c5 aiofiles pipenv install aiofiles # \u521b\u5efa\u9879\u76ee\u76ee\u5f55 mkdir ruia_ua cd ruia_ua # \u5b9e\u73b0\u4ee3\u7801\u653e\u5728\u8fd9\u91cc touch __init__.py \u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a ruia-ua \u251c\u2500\u2500 LICENSE # \u5f00\u6e90\u534f\u8bae \u251c\u2500\u2500 Pipfile # pipenv \u7ba1\u7406\u5de5\u5177\u751f\u6210\u6587\u4ef6 \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # \u4ee3\u7801\u5b9e\u73b0 \u2502 \u2514\u2500\u2500 user_agents.txt # \u968f\u673aua\u96c6\u5408 \u2514\u2500\u2500 setup.py","title":"Creating a project"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/3.%E4%B8%BA%20Ruia%20%E7%BC%96%E5%86%99%E6%8F%92%E4%BB%B6.html#first-extension","text":"user_agents.txt \u6587\u4ef6\u5305\u542b\u4e86\u5404\u79cd ua \uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u53ea\u8981\u5229\u7528 ruia \u7684 Middleware \u5b9e\u73b0\u5728\u6bcf\u6b21\u8bf7\u6c42\u524d\u968f\u673a\u6dfb\u52a0\u4e00\u4e2a User-Agent \u5373\u53ef\uff0c\u5b9e\u73b0\u4ee3\u7801\u5982\u4e0b\uff1a import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware . request async def add_random_ua ( spider_ins , request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } \u7f16\u5199\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5c06 ruia-ua \u4e0a\u4f20\u81f3\u793e\u533a\uff0c\u8fd9\u6837\u6240\u6709\u7684 ruia \u4f7f\u7528\u8005\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u4f60\u7f16\u5199\u7684\u7b2c\u4e09\u65b9\u6269\u5c55\uff0c\u591a\u4e48\u7f8e\u597d\u7684\u4e00\u4ef6\u4e8b","title":"First extension"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/3.%E4%B8%BA%20Ruia%20%E7%BC%96%E5%86%99%E6%8F%92%E4%BB%B6.html#usage","text":"\u6240\u6709\u7684\u722c\u866b\u7a0b\u5e8f\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 ruia-ua \u6765\u5b9e\u73b0\u81ea\u52a8\u6dfb\u52a0 User-Agent pip install ruia-ua \u4e3e\u4e2a\u5b9e\u9645\u4f7f\u7528\u7684\u4f8b\u5b50\uff1a from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) \u7b2c\u4e09\u65b9\u6269\u5c55\u7684\u5b9e\u73b0\u5c06\u4f1a\u5927\u5927\u51cf\u5c11\u722c\u866b\u5de5\u7a0b\u5e08\u7684\u5f00\u53d1\u5468\u671f\uff0c ruia \u975e\u5e38\u5e0c\u671b\u4f60\u53ef\u4ee5\u5f00\u53d1\u5e76\u63d0\u4ea4\u81ea\u5df1\u7684\u7b2c\u4e09\u65b9\u6269\u5c55","title":"Usage"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/4.%E8%B4%A1%E7%8C%AE%E4%BB%A3%E7%A0%81.html","text":"\u8d21\u732e\u4ee3\u7801","title":"\u8d21\u732e\u4ee3\u7801"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/4.%E8%B4%A1%E7%8C%AE%E4%BB%A3%E7%A0%81.html#_1","text":"","title":"\u8d21\u732e\u4ee3\u7801"},{"location":"cn/03_%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/_index.html","text":"","title":"\u5f00\u53d1\u6307\u5357"},{"location":"cn/04_%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/1.%E8%B0%88%E8%B0%88%E5%AF%B9%20Python%20%E7%88%AC%E8%99%AB%E7%9A%84%E7%90%86%E8%A7%A3.html","text":"\u8c08\u8c08\u5bf9Python\u722c\u866b\u7684\u7406\u89e3 \u722c\u866b\u4e5f\u53ef\u4ee5\u79f0\u4e3aPython\u722c\u866b \u4e0d\u77e5\u4ece\u4f55\u65f6\u8d77\uff0cPython\u8fd9\u95e8\u8bed\u8a00\u548c\u722c\u866b\u5c31\u50cf\u4e00\u5bf9\u604b\u4eba\uff0c\u4e8c\u8005\u5982\u80f6\u4f3c\u6f06 \uff0c\u5f62\u5f71\u4e0d\u79bb\uff0c\u4f60\u4e2d\u6709\u6211\u3001\u6211\u4e2d\u6709\u4f60\uff0c\u4e00\u63d0\u8d77\u722c\u866b\uff0c\u5c31\u4f1a\u60f3\u5230Python\uff0c\u4e00\u8bf4\u8d77Python\uff0c\u5c31\u4f1a\u60f3\u5230\u4eba\u5de5\u667a\u80fd\u2026\u2026\u548c\u722c\u866b \u6240\u4ee5\uff0c\u4e00\u822c\u8bf4\u722c\u866b\u7684\u65f6\u5019\uff0c\u5927\u90e8\u5206\u7a0b\u5e8f\u5458\u6f5c\u610f\u8bc6\u91cc\u90fd\u4f1a\u8054\u60f3\u4e3aPython\u722c\u866b\uff0c\u4e3a\u4ec0\u4e48\u4f1a\u8fd9\u6837\uff0c\u6211\u89c9\u5f97\u6709\u4e24\u4e2a\u539f\u56e0\uff1a Python\u751f\u6001\u6781\u5176\u4e30\u5bcc\uff0c\u8bf8\u5982Request\u3001Beautiful Soup\u3001Scrapy\u3001PySpider\u7b49\u7b2c\u4e09\u65b9\u5e93\u5b9e\u5728\u5f3a\u5927 Python\u8bed\u6cd5\u7b80\u6d01\u6613\u4e0a\u624b\uff0c\u5206\u5206\u949f\u5c31\u80fd\u5199\u51fa\u4e00\u4e2a\u722c\u866b\uff08\u6709\u4eba\u5410\u69fdPython\u6162\uff0c\u4f46\u662f\u722c\u866b\u7684\u74f6\u9888\u548c\u8bed\u8a00\u5173\u7cfb\u4e0d\u5927\uff09 \u4efb\u4f55\u4e00\u4e2a\u5b66\u4e60Python\u7684\u7a0b\u5e8f\u5458\uff0c\u5e94\u8be5\u90fd\u6216\u591a\u6216\u5c11\u5730\u89c1\u8fc7\u751a\u81f3\u7814\u7a76\u8fc7\u722c\u866b\uff0c\u6211\u5f53\u65f6\u5199Python\u7684\u76ee\u7684\u5c31\u975e\u5e38\u7eaf\u7cb9\u2014\u2014\u4e3a\u4e86\u5199\u722c\u866b\u3002\u6240\u4ee5\u672c\u6587\u7684\u76ee\u7684\u5f88\u7b80\u5355\uff0c\u5c31\u662f\u8bf4\u8bf4\u6211\u4e2a\u4eba\u5bf9Python\u722c\u866b\u7684\u7406\u89e3\u4e0e\u5b9e\u8df5\uff0c\u4f5c\u4e3a\u4e00\u540d\u7a0b\u5e8f\u5458\uff0c\u6211\u89c9\u5f97\u4e86\u89e3\u4e00\u4e0b\u722c\u866b\u7684\u76f8\u5173\u77e5\u8bc6\u5bf9\u4f60\u53ea\u6709\u597d\u5904\uff0c\u6240\u4ee5\u8bfb\u5b8c\u8fd9\u7bc7\u6587\u7ae0\u540e\uff0c\u5982\u679c\u80fd\u5bf9\u4f60\u6709\u5e2e\u52a9\uff0c\u90a3\u4fbf\u518d\u597d\u4e0d\u8fc7 \u4ec0\u4e48\u662f\u722c\u866b \u722c\u866b\u662f\u4e00\u4e2a\u7a0b\u5e8f\uff0c\u8fd9\u4e2a\u7a0b\u5e8f\u7684\u76ee\u7684\u5c31\u662f\u4e3a\u4e86\u6293\u53d6\u4e07\u7ef4\u7f51\u4fe1\u606f\u8d44\u6e90\uff0c\u6bd4\u5982\u4f60\u65e5\u5e38\u4f7f\u7528\u7684\u8c37\u6b4c\u7b49\u641c\u7d22\u5f15\u64ce\uff0c\u641c\u7d22\u7ed3\u679c\u5c31\u5168\u90fd\u4f9d\u8d56\u722c\u866b\u6765\u5b9a\u65f6\u83b7\u53d6 \u770b\u4e0a\u8ff0\u641c\u7d22\u7ed3\u679c\uff0c\u9664\u4e86wiki\u76f8\u5173\u4ecb\u7ecd\u5916\uff0c\u722c\u866b\u6709\u5173\u7684\u641c\u7d22\u7ed3\u679c\u5168\u90fd\u5e26\u4e0a\u4e86Python\uff0c\u524d\u4eba\u8bf4Python\u722c\u866b\uff0c\u73b0\u5728\u770b\u6765\u679c\u7136\u8bda\u4e0d\u6b3a\u6211\uff5e \u722c\u866b\u7684\u76ee\u6807\u5bf9\u8c61\u4e5f\u5f88\u4e30\u5bcc\uff0c\u4e0d\u8bba\u662f\u6587\u5b57\u3001\u56fe\u7247\u3001\u89c6\u9891\uff0c\u4efb\u4f55\u7ed3\u6784\u5316\u975e\u7ed3\u6784\u5316\u7684\u6570\u636e\u722c\u866b\u90fd\u53ef\u4ee5\u722c\u53d6\uff0c\u722c\u866b\u7ecf\u8fc7\u53d1\u5c55\uff0c\u4e5f\u884d\u751f\u51fa\u4e86\u5404\u79cd\u722c\u866b\u7c7b\u578b\uff1a \u901a\u7528\u7f51\u7edc\u722c\u866b\uff1a\u722c\u53d6\u5bf9\u8c61\u4ece\u4e00\u4e9b\u79cd\u5b50 URL \u6269\u5145\u5230\u6574\u4e2a Web\uff0c\u641c\u7d22\u5f15\u64ce\u5e72\u7684\u5c31\u662f\u8fd9\u4e9b\u4e8b \u5782\u76f4\u7f51\u7edc\u722c\u866b\uff1a\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u4e3b\u9898\u8fdb\u884c\u722c\u53d6\uff0c\u6bd4\u5982\u4e13\u95e8\u722c\u53d6\u5c0f\u8bf4\u76ee\u5f55\u4ee5\u53ca\u7ae0\u8282\u7684\u5782\u76f4\u722c\u866b \u589e\u91cf\u7f51\u7edc\u722c\u866b\uff1a\u5bf9\u5df2\u7ecf\u6293\u53d6\u7684\u7f51\u9875\u8fdb\u884c\u5b9e\u65f6\u66f4\u65b0 \u6df1\u5c42\u7f51\u7edc\u722c\u866b\uff1a\u722c\u53d6\u4e00\u4e9b\u9700\u8981\u7528\u6237\u63d0\u4ea4\u5173\u952e\u8bcd\u624d\u80fd\u83b7\u5f97\u7684 Web \u9875\u9762 \u4e0d\u60f3\u8bf4\u8fd9\u4e9b\u5927\u65b9\u5411\u7684\u6982\u5ff5\uff0c\u8ba9\u6211\u4eec\u4ee5\u4e00\u4e2a\u83b7\u53d6\u7f51\u9875\u5185\u5bb9\u4e3a\u4f8b\uff0c\u4ece\u722c\u866b\u6280\u672f\u672c\u8eab\u51fa\u53d1\uff0c\u6765\u8bf4\u8bf4\u7f51\u9875\u722c\u866b\uff0c\u6b65\u9aa4\u5982\u4e0b\uff1a \u6a21\u62df\u8bf7\u6c42\u7f51\u9875\u8d44\u6e90 \u4eceHTML\u63d0\u53d6\u76ee\u6807\u5143\u7d20 \u6570\u636e\u6301\u4e45\u5316 \u4ec0\u4e48\u662f\u722c\u866b\uff0c\u8fd9\u5c31\u662f\u722c\u866b\uff1a \"\"\"\u8ba9\u6211\u4eec\u6839\u636e\u4e0a\u9762\u8bf4\u7684\u6b65\u9aa4\u6765\u5b8c\u6210\u4e00\u4e2a\u7b80\u5355\u7684\u722c\u866b\u7a0b\u5e8f\"\"\" import requests from bs4 import BeautifulSoup target_url = 'http://www.baidu.com/s?wd=\u722c\u866b' # \u7b2c\u4e00\u6b65 \u53d1\u8d77\u4e00\u4e2aGET\u8bf7\u6c42 res = requests . get ( target_url ) # \u7b2c\u4e8c\u6b65 \u63d0\u53d6HTML\u5e76\u89e3\u6790\u60f3\u83b7\u53d6\u7684\u6570\u636e \u6bd4\u5982\u83b7\u53d6 title soup = BeautifulSoup ( res . text , \"lxml\" ) # \u8f93\u51fa soup.title.text title = soup . title . text # \u7b2c\u4e09\u6b65 \u6301\u4e45\u5316 \u6bd4\u5982\u4fdd\u5b58\u5230\u672c\u5730 with open ( 'title.txt' , 'w' ) as fp : fp . write ( title ) \u52a0\u4e0a\u6ce8\u91ca\u4e0d\u523020\u884c\u4ee3\u7801\uff0c\u4f60\u5c31\u5b8c\u6210\u4e86\u4e00\u4e2a\u722c\u866b\uff0c\u7b80\u5355\u5427 \u600e\u4e48\u5199\u722c\u866b \u7f51\u9875\u4e16\u754c\u591a\u59ff\u591a\u5f69\u3001\u4ebf\u4e07\u7f51\u9875\u8d44\u6e90\u4f9b\u4f60\u9009\u62e9\uff0c\u9762\u5bf9\u4e0d\u540c\u7684\u9875\u9762\uff0c\u600e\u4e48\u4f7f\u81ea\u5df1\u7f16\u5199\u7684\u722c\u866b\u7a0b\u5e8f\u591f\u7a33\u5065\u3001\u6301\u4e45\uff0c\u8fd9\u662f\u4e00\u4e2a\u503c\u5f97\u8ba8\u8bba\u7684\u95ee\u9898 \u4fd7\u8bdd\u8bf4\uff0c\u78e8\u5200\u4e0d\u8bef\u780d\u67f4\u5de5\uff0c\u5728\u5f00\u59cb\u7f16\u5199\u722c\u866b\u4e4b\u524d\uff0c\u5f88\u6709\u5fc5\u8981\u638c\u63e1\u4e00\u4e9b\u57fa\u672c\u77e5\u8bc6\uff1a \u7f51\u9875\u7684\u7ed3\u6784\u662fHTML\uff0c\u722c\u866b\u7684\u76ee\u6807\u5c31\u662f\u89e3\u6790HTML\uff0c\u83b7\u53d6\u76ee\u6807\u5b57\u6bb5\u5e76\u4fdd\u5b58 \u5ba2\u6237\u7aef\u5c55\u73b0\u7684\u7f51\u9875\u7531\u6d4f\u89c8\u5668\u6e32\u67d3\uff0c\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u7aef\u7684\u4fe1\u606f\u4ea4\u4e92\u4f9d\u9760HTTP\u534f\u8bae \u8fd9\u4e24\u53e5\u63cf\u8ff0\u4f53\u73b0\u4e86\u4e00\u540d\u722c\u866b\u5f00\u53d1\u4eba\u5458\u9700\u8981\u638c\u63e1\u7684\u57fa\u672c\u77e5\u8bc6\uff0c\u4e0d\u8fc7\u4e00\u540d\u57fa\u672c\u7684\u540e\u7aef\u6216\u8005\u524d\u7aef\u5de5\u7a0b\u5e08\u90fd\u4f1a\u8fd9\u4e9b\u54c8\u54c8\uff0c\u8fd9\u4e5f\u8bf4\u660e\u4e86\u722c\u866b\u7684\u5165\u95e8\u96be\u5ea6\u6781\u4f4e\uff0c\u4ece\u8fd9\u4e24\u53e5\u8bdd\uff0c\u4f60\u80fd\u601d\u8003\u51fa\u54ea\u4e9b\u722c\u866b\u5fc5\u5907\u7684\u77e5\u8bc6\u70b9\u5462\uff1f \u57fa\u672c\u7684HTML\u77e5\u8bc6\uff0c\u4e86\u89e3HTML\u624d\u65b9\u4fbf\u76ee\u6807\u4fe1\u606f\u63d0\u53d6 \u57fa\u672c\u7684JS\u77e5\u8bc6 \uff0cJS\u53ef\u4ee5\u5f02\u6b65\u52a0\u8f7dHTML \u4e86\u89e3CSS Selector\u3001XPath\u4ee5\u53ca\u6b63\u5219\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u63d0\u53d6\u6570\u636e \u4e86\u89e3HTTP\u534f\u8bae\uff0c\u4e3a\u540e\u9762\u7684\u53cd\u722c\u866b\u6597\u4e89\u6253\u4e0b\u57fa\u7840 \u4e86\u89e3\u57fa\u672c\u7684\u6570\u636e\u5e93\u64cd\u4f5c\uff0c\u4e3a\u4e86\u6570\u636e\u6301\u4e45\u5316 \u6709\u4e86\u8fd9\u4e9b\u77e5\u8bc6\u50a8\u5907\uff0c\u63a5\u4e0b\u6765\u5c31\u53ef\u4ee5\u9009\u62e9\u4e00\u95e8\u8bed\u8a00\uff0c\u5f00\u59cb\u7f16\u5199\u81ea\u5df1\u7684\u722c\u866b\u7a0b\u5e8f\u4e86\uff0c\u8fd8\u662f\u6309\u7167\u4e0a\u4e00\u8282\u8bf4\u7684\u4e09\u4e2a\u6b65\u9aa4\uff0c\u7136\u540e\u4ee5Python\u4e3a\u4f8b\uff0c\u8bf4\u4e00\u8bf4\u8981\u5728\u7f16\u7a0b\u8bed\u8a00\u65b9\u9762\u505a\u90a3\u4e9b\u51c6\u5907\uff1a \u7f51\u9875\u8bf7\u6c42\uff1a\u5185\u7f6e\u6709urllib\u5e93\uff0c\u7b2c\u4e09\u65b9\u5e93\u7684\u8bdd\uff0c\u540c\u6b65\u8bf7\u6c42\u53ef\u4ee5\u4f7f\u7528requests\uff0c\u5f02\u6b65\u8bf7\u6c42\u4f7f\u7528aiohttp \u5206\u6790HTML\u7ed3\u6784\u5e76\u63d0\u53d6\u76ee\u6807\u5143\u7d20\uff1aCSS Selector\u548cXPath\u662f\u76ee\u524d\u4e3b\u6d41\u7684\u63d0\u53d6\u65b9\u5f0f\uff0c\u7b2c\u4e09\u65b9\u5e93\u53ef\u4ee5\u4f7f\u7528Beautiful Soup\u6216\u8005PyQuery \u6570\u636e\u6301\u4e45\u5316\uff1a\u76ee\u6807\u6570\u636e\u63d0\u53d6\u4e4b\u540e\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u4fdd\u5b58\u5230\u6570\u636e\u5e93\u4e2d\u8fdb\u884c\u6301\u4e45\u5316\uff0cMySQL\u3001MongoDB\u7b49\uff0c\u8fd9\u4e9b\u90fd\u6709\u5bf9\u5e94\u7684\u5e93\u652f\u6301\uff0c\u5f53\u7136\u4f60\u4e5f\u53ef\u4ee5\u4fdd\u5b58\u5728\u786c\u76d8\uff0c\u8c01\u786c\u76d8\u6ca1\u70b9\u4e1c\u897f\u5bf9\u5427\uff08\u6ed1\u7a3d\u8138\uff09 \u638c\u63e1\u4e86\u4e0a\u9762\u8fd9\u4e9b\uff0c\u4f60\u5927\u53ef\u653e\u5f00\u624b\u811a\u5927\u5e72\u4e00\u573a\uff0c\u4e07\u7ef4\u7f51\u5c31\u662f\u4f60\u7684\u540d\u5229\u573a\uff0c\u53bb\u5427\uff5e \u6211\u89c9\u5f97\u5bf9\u4e8e\u4e00\u4e2a\u76ee\u6807\u7f51\u7ad9\u7684\u7f51\u9875\uff0c\u53ef\u4ee5\u5206\u4e0b\u9762\u56db\u4e2a\u7c7b\u578b\uff1a \u5355\u9875\u9762\u5355\u76ee\u6807 \u5355\u9875\u9762\u591a\u76ee\u6807 \u591a\u9875\u9762\u5355\u76ee\u6807 \u591a\u9875\u9762\u591a\u76ee\u6807 \u5177\u4f53\u662f\u4ec0\u4e48\u610f\u601d\u5462\uff0c\u53ef\u80fd\u770b\u8d77\u6765\u6709\u70b9\u7ed5\uff0c\u4f46\u660e\u767d\u8fd9\u4e9b\uff0c\u4f60\u4e4b\u540e\u5199\u722c\u866b\uff0c\u53ea\u8981\u5728\u8111\u5b50\u91cc\u9762\u8fc7\u4e00\u904d\u7740\u7f51\u9875\u5bf9\u5e94\u4ec0\u4e48\u7c7b\u578b\uff0c\u7136\u540e\u5957\u4e0a\u5bf9\u5e94\u7c7b\u578b\u7684\u7a0b\u5e8f\uff08\u5199\u591a\u4e86\u90fd\u5e94\u8be5\u6709\u4e00\u5957\u81ea\u5df1\u7684\u5e38\u7528\u4ee3\u7801\u5e93\uff09\uff0c\u90a3\u5199\u722c\u866b\u7684\u901f\u5ea6\uff0c\u81ea\u7136\u4e0d\u4f1a\u6162 \u5355\u9875\u9762\u5355\u76ee\u6807 \u901a\u4fd7\u6765\u8bf4\uff0c\u5c31\u662f\u5728\u8fd9\u4e2a\u7f51\u9875\u91cc\u9762\uff0c\u6211\u4eec\u7684\u76ee\u6807\u5c31\u53ea\u6709\u4e00\u4e2a\uff0c\u5047\u8bbe\u6211\u4eec\u7684\u9700\u6c42\u662f\u6293\u53d6\u8fd9\u90e8 \u7535\u5f71-\u8096\u7533\u514b\u7684\u6551\u8d4e \u7684\u540d\u79f0\uff0c\u9996\u5148\u6253\u5f00\u7f51\u9875\u53f3\u952e\u5ba1\u67e5\u5143\u7d20\uff0c\u627e\u5230\u7535\u5f71\u540d\u79f0\u5bf9\u5e94\u7684\u5143\u7d20\u4f4d\u7f6e\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u5728\u67d0\u4e2a\u5355\u4e00\u9875\u9762\u5185\uff0c\u770b\u76ee\u6807\u662f\u4e0d\u662f\u53ea\u6709\u4e00\u4e2a\uff0c\u4e00\u773c\u5c31\u80fd\u770b\u51fa\u6807\u9898\u7684CSS Selector\u89c4\u5219\u4e3a\uff1a #content > h1 > span:nth-child(1) \uff0c\u7136\u540e\u7528\u6211\u81ea\u5df1\u5199\u7684\u5e38\u7528\u5e93\uff0c\u6211\u7528\u4e0d\u5230\u5341\u884c\u4ee3\u7801\u5c31\u80fd\u5199\u5b8c\u6293\u53d6\u8fd9\u4e2a\u9875\u9762\u7535\u5f71\u540d\u79f0\u7684\u722c\u866b\uff1a import asyncio from ruia import Item , TextField class DoubanItem ( Item ): \"\"\" \u5b9a\u4e49\u722c\u866b\u7684\u76ee\u6807\u5b57\u6bb5 \"\"\" title = TextField ( css_select = '#content > h1 > span:nth-child(1)' ) async_func = DoubanItem . get_item ( url = \"https://movie.douban.com/subject/1292052/\" ) item = asyncio . get_event_loop () . run_until_complete ( async_func ) print ( item ) \u591a\u9875\u9762\u591a\u76ee\u6807\u5c31\u662f\u6b64\u60c5\u51b5\u4e0b\u591a\u4e2aurl\u7684\u884d\u751f\u60c5\u51b5 \u5355\u9875\u9762\u591a\u76ee\u6807 \u5047\u8bbe\u73b0\u5728\u7684\u9700\u6c42\u662f\u6293\u53d6 \u8c46\u74e3\u7535\u5f71250 \u7b2c\u4e00\u9875\u4e2d\u7684\u6240\u6709\u7535\u5f71\u540d\u79f0\uff0c\u4f60\u9700\u8981\u63d0\u53d625\u4e2a\u7535\u5f71\u540d\u79f0\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u76ee\u6807\u9875\u7684\u76ee\u6807\u6570\u636e\u662f\u591a\u4e2aitem\u7684\uff0c\u6240\u4ee5\u76ee\u6807\u9700\u8981\u5faa\u73af\u83b7\u53d6\u3002 \u5bf9\u4e8e\u8fd9\u4e2a\u60c5\u51b5\uff0c\u6211\u5728 Item \u4e2d\u9650\u5236\u4e86\u4e00\u70b9\uff0c\u5f53\u4f60\u5b9a\u4e49\u7684\u722c\u866b\u9700\u8981\u5728\u67d0\u4e00\u9875\u9762\u5faa\u73af\u83b7\u53d6\u4f60\u7684\u76ee\u6807\u65f6\uff0c\u5219\u9700\u8981\u5b9a\u4e49 target_item \u5c5e\u6027\uff08\u5c31\u662f\u622a\u56fe\u4e2d\u7684\u7ea2\u6846\uff09 \u5bf9\u4e8e\u8c46\u74e3250\u8fd9\u4e2a\u9875\u9762\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f25\u90e8\u7535\u5f71\u4fe1\u606f\uff0c\u6240\u4ee5\u8be5\u8fd9\u6837\u5b9a\u4e49\uff1a field css_select target_item\uff08\u5fc5\u987b\uff09 div.item title span.title \u4ee3\u7801\u5b9e\u73b0\u5982\u4e0b\uff1a import asyncio from ruia import Item , TextField class DoubanItem ( Item ): \"\"\" \u5b9a\u4e49\u722c\u866b\u7684\u76ee\u6807\u5b57\u6bb5 \"\"\" target_item = TextField ( css_select = \"div.item\" ) title = TextField ( css_select = \"span.title\" ) async def clean_title ( self , title ): \"\"\" \u5bf9\u63d0\u53d6\u7684\u76ee\u6807\u6570\u636e\u8fdb\u884c\u6e05\u6d17 \u53ef\u9009 :param title: \u521d\u6b65\u63d0\u53d6\u7684\u76ee\u6807\u6570\u636e :return: \"\"\" if isinstance ( title , str ): return title else : return \"\" . join ([ i . text . strip () . replace ( \" \\xa0 \" , \"\" ) for i in title ]) async def run_item ( url : str ): async for item in DoubanItem . get_items ( url = url ): print ( item ) items = asyncio . get_event_loop () . run_until_complete ( run_item ( \"https://movie.douban.com/top250\" ) ) \u591a\u9875\u9762\u591a\u76ee\u6807 \u591a\u9875\u9762\u591a\u76ee\u6807\u662f\u4e0a\u8ff0\u5355\u9875\u9762\u591a\u76ee\u6807\u60c5\u51b5\u7684\u884d\u751f\uff0c\u5728\u8fd9\u4e2a\u95ee\u9898\u4e0a\u6765\u770b\uff0c\u6b64\u65f6\u5c31\u662f\u83b7\u53d6\u6240\u6709\u5206\u9875\u7684\u7535\u5f71\u540d\u79f0 from ruia import Item , Request , Spider , TextField class DoubanItem ( Item ): \"\"\" \u5b9a\u4e49\u722c\u866b\u7684\u76ee\u6807\u5b57\u6bb5 \"\"\" target_item = TextField ( css_select = \"div.item\" ) title = TextField ( css_select = \"span.title\" ) async def clean_title ( self , title ): if isinstance ( title , str ): return title else : return \"\" . join ([ i . text . strip () . replace ( \" \\xa0 \" , \"\" ) for i in title ]) class DoubanSpider ( Spider ): start_urls = [ \"https://movie.douban.com/top250\" ] concurrency = 10 async def parse ( self , res ): etree = res . html_etree ( html = await res . text ()) pages = [ \"?start=0&filter=\" ] + [ i . get ( \"href\" ) for i in etree . cssselect ( \".paginator>a\" ) ] for index , page in enumerate ( pages ): url = self . start_urls [ 0 ] + page yield Request ( url , callback = self . parse_item , metadata = { \"index\" : index }, request_config = self . request_config , ) async def parse_item ( self , res ): res_list = [] async for item in DoubanItem . get_items ( html = await res . text ()): res_list . append ( item . title ) return res_list if __name__ == \"__main__\" : DoubanSpider . start () \u5982\u679c\u7f51\u7edc\u6ca1\u95ee\u9898\u7684\u8bdd\uff0c\u4f1a\u5f97\u5230\u5982\u4e0b\u8f93\u51fa\uff1a \u6ce8\u610f\u722c\u866b\u8fd0\u884c\u65f6\u95f4\uff0c1s\u4e0d\u5230\uff0c\u8fd9\u5c31\u662f\u5f02\u6b65\u7684\u9b45\u529b \u7528Python\u5199\u722c\u866b\uff0c\u5c31\u662f\u8fd9\u4e48\u7b80\u5355\u4f18\u96c5\uff0c\u8bf8\u4f4d\uff0c\u770b\u7740\u7f51\u9875\u5c31\u601d\u8003\u4e0b\uff1a \u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u76ee\u6807\u7c7b\u578b \u7528\u4ec0\u4e48\u5e93\u6a21\u62df\u8bf7\u6c42 \u600e\u4e48\u89e3\u6790\u76ee\u6807\u5b57\u6bb5 \u600e\u4e48\u5b58\u50a8 \u4e00\u4e2a\u722c\u866b\u7a0b\u5e8f\u5c31\u6210\u578b\u4e86\uff0c\u987a\u4fbf\u4e00\u63d0\uff0c\u722c\u866b\u8fd9\u4e1c\u897f\uff0c\u53ef\u4ee5\u8bf4\u662f\u9632\u541b\u5b50\u4e0d\u9632\u5c0f\u4eba\uff0c robots.txt \u5927\u90e8\u5206\u7f51\u7ad9\u90fd\u6709\uff08\u5b83\u7684\u76ee\u7684\u662f\u544a\u8bc9\u722c\u866b\u4ec0\u4e48\u53ef\u4ee5\u722c\u53d6\u4ec0\u4e48\u4e0d\u53ef\u4ee5\u722c\u53d6\uff0c\u6bd4\u5982\uff1a https://www.baidu.com/robots.txt \uff09\uff0c\u5404\u4f4d\u60f3\u600e\u4e48\u722c\u53d6\uff0c\u81ea\u5df1\u8861\u91cf \u5982\u4f55\u8fdb\u9636 \u4e0d\u8981\u4ee5\u4e3a\u5199\u597d\u4e00\u4e2a\u722c\u866b\u7a0b\u5e8f\u5c31\u53ef\u4ee5\u51fa\u5e08\u4e86\uff0c\u6b64\u65f6\u8fd8\u6709\u66f4\u591a\u7684\u95ee\u9898\u5728\u524d\u9762\u7b49\u7740\u4f60\uff0c\u4f60\u8981\u542b\u60c5\u8109\u8109\u5730\u770b\u7740\u4f60\u7684\u722c\u866b\u7a0b\u5e8f\uff0c\u95ee\u81ea\u5df1\u4e09\u4e2a\u95ee\u9898\uff1a \u722c\u866b\u6293\u53d6\u6570\u636e\u540e\u662f\u6b63\u5f53\u7528\u9014\u4e48\uff1f \u722c\u866b\u4f1a\u628a\u76ee\u6807\u7f51\u7ad9\u5e72\u6389\u4e48\uff1f \u722c\u866b\u4f1a\u88ab\u53cd\u722c\u866b\u5e72\u6389\u4e48\uff1f \u524d\u4e24\u4e2a\u5173\u4e8e\u4eba\u6027\u7684\u95ee\u9898\u5728\u6b64\u4e0d\u505a\u8fc7\u591a\u53d9\u8ff0\uff0c\u56e0\u6b64\u8df3\u8fc7\uff0c\u4f46\u4f60\u4eec\u5982\u679c\u4f5c\u4e3a\u722c\u866b\u5de5\u7a0b\u5e08\u7684\u8bdd\uff0c\u5207\u4e0d\u53ef\u8df3\u8fc7 \u4f1a\u88ab\u53cd\u722c\u866b\u5e72\u6389\u4e48\uff1f \u6700\u540e\u5173\u4e8e\u53cd\u722c\u866b\u7684\u95ee\u9898\u624d\u662f\u4f60\u722c\u866b\u7a0b\u5e8f\u5f3a\u58ee\u4e0e\u5426\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4ec0\u4e48\u662f\u53cd\u722c\u866b\uff1f \u5f53\u8d8a\u6765\u8d8a\u591a\u7684\u722c\u866b\u5728\u4e92\u8054\u7f51\u4e0a\u6a2a\u51b2\u76f4\u649e\u540e\uff0c\u7f51\u9875\u8d44\u6e90\u7ef4\u62a4\u8005\u4e3a\u4e86\u9632\u6b62\u81ea\u8eab\u6570\u636e\u88ab\u6293\u53d6\uff0c\u5f00\u59cb\u8fdb\u884c\u4e00\u7cfb\u5217\u7684\u63aa\u65bd\u6765\u4f7f\u5f97\u81ea\u8eab\u6570\u636e\u4e0d\u6613\u88ab\u522b\u7684\u7a0b\u5e8f\u722c\u53d6\uff0c\u8fd9\u4e9b\u63aa\u65bd\u5c31\u662f\u53cd\u722c\u866b \u6bd4\u5982\u68c0\u6d4bIP\u8bbf\u95ee\u9891\u7387\u3001\u8d44\u6e90\u8bbf\u95ee\u901f\u5ea6\u3001\u94fe\u63a5\u662f\u5426\u5e26\u6709\u5173\u952e\u53c2\u6570\u3001\u9a8c\u8bc1\u7801\u68c0\u6d4b\u673a\u5668\u4eba\u3001ajax\u6df7\u6dc6\u3001js\u52a0\u5bc6\u7b49\u7b49 \u5bf9\u4e8e\u76ee\u524d\u5e02\u573a\u4e0a\u7684\u53cd\u722c\u866b\uff0c\u722c\u866b\u5de5\u7a0b\u5e08\u5e38\u6709\u7684\u53cd\u53cd\u722c\u866b\u65b9\u6848\u662f\u4e0b\u9762\u8fd9\u6837\u7684\uff1a \u4e0d\u65ad\u8bd5\u63a2\u76ee\u6807\u5e95\u7ebf\uff0c\u8bd5\u51fa\u5355IP\u4e0b\u6700\u4f18\u7684\u8bbf\u95ee\u9891\u7387 \u6784\u5efa\u81ea\u5df1\u7684IP\u4ee3\u7406\u6c60 \u7ef4\u62a4\u4e00\u4efd\u81ea\u5df1\u5e38\u7528\u7684UA\u5e93 \u9488\u5bf9\u76ee\u6807\u7f51\u9875\u7684Cookie\u6c60 \u9700\u8981JS\u6e32\u67d3\u7684\u7f51\u9875\u4f7f\u7528\u65e0\u5934\u6d4f\u89c8\u5668\u8fdb\u884c\u4ee3\u7801\u6e32\u67d3\u518d\u6293\u53d6 \u4e00\u5957\u7834\u89e3\u9a8c\u8bc1\u7801\u7a0b\u5e8f \u624e\u5b9e\u7684JS\u77e5\u8bc6\u6765\u7834\u89e3\u6df7\u6dc6\u51fd\u6570 \u722c\u866b\u5de5\u7a0b\u5e08\u7684\u8fdb\u9636\u4e4b\u8def\u5176\u5b9e\u5c31\u662f\u4e0d\u65ad\u53cd\u53cd\u722c\u866b\uff0c\u53ef\u8c13\u8270\u8f9b\uff0c\u4f46\u6362\u4e2a\u89d2\u5ea6\u60f3\u4e5f\u662f\u4e50\u8da3\u6240\u5728 \u5173\u4e8e\u6846\u67b6 \u722c\u866b\u6709\u81ea\u5df1\u7684\u7f16\u5199\u6d41\u7a0b\u548c\u6807\u51c6\uff0c\u6709\u4e86\u6807\u51c6\uff0c\u81ea\u7136\u5c31\u6709\u4e86\u6846\u67b6\uff0c\u50cfPython\u8fd9\u79cd\u751f\u6001\u5f3a\u5927\u7684\u8bed\u8a00\uff0c\u6846\u67b6\u81ea\u7136\u662f\u591a\u4e0d\u80dc\u6570\uff0c\u76ee\u524d\u4e16\u9762\u4e0a\u7528\u7684\u6bd4\u8f83\u591a\u7684\u6709\uff1a Scrapy PySpider Portia Ruia \uff1a\u5bb9\u6211\u81ea\u8350\u4e00\u6ce2 \u8fd9\u91cc\u4e0d\u8fc7\u591a\u4ecb\u7ecd\uff0c\u6846\u67b6\u53ea\u662f\u5de5\u5177\uff0c\u662f\u4e00\u79cd\u63d0\u5347\u6548\u7387\u7684\u65b9\u5f0f\uff0c\u770b\u4f60\u9009\u62e9 \u8bf4\u660e \u4efb\u4f55\u4e8b\u7269\u90fd\u6709\u4e24\u9762\u6027\uff0c\u722c\u866b\u81ea\u7136\u4e5f\u4e0d\u4f8b\u5916\uff0c\u56e0\u6b64\u6211\u9001\u8bf8\u4f4d\u4e00\u5f20\u56fe\uff0c\u5173\u952e\u65f6\u523b\u597d\u597d\u60f3\u60f3 \u6700\u540e\uff0c\u6b22\u8fce\u4e00\u8d77\u4ea4\u6d41\uff1a","title":"\u8c08\u8c08\u5bf9Python\u722c\u866b\u7684\u7406\u89e3"},{"location":"cn/04_%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/1.%E8%B0%88%E8%B0%88%E5%AF%B9%20Python%20%E7%88%AC%E8%99%AB%E7%9A%84%E7%90%86%E8%A7%A3.html#python","text":"\u722c\u866b\u4e5f\u53ef\u4ee5\u79f0\u4e3aPython\u722c\u866b \u4e0d\u77e5\u4ece\u4f55\u65f6\u8d77\uff0cPython\u8fd9\u95e8\u8bed\u8a00\u548c\u722c\u866b\u5c31\u50cf\u4e00\u5bf9\u604b\u4eba\uff0c\u4e8c\u8005\u5982\u80f6\u4f3c\u6f06 \uff0c\u5f62\u5f71\u4e0d\u79bb\uff0c\u4f60\u4e2d\u6709\u6211\u3001\u6211\u4e2d\u6709\u4f60\uff0c\u4e00\u63d0\u8d77\u722c\u866b\uff0c\u5c31\u4f1a\u60f3\u5230Python\uff0c\u4e00\u8bf4\u8d77Python\uff0c\u5c31\u4f1a\u60f3\u5230\u4eba\u5de5\u667a\u80fd\u2026\u2026\u548c\u722c\u866b \u6240\u4ee5\uff0c\u4e00\u822c\u8bf4\u722c\u866b\u7684\u65f6\u5019\uff0c\u5927\u90e8\u5206\u7a0b\u5e8f\u5458\u6f5c\u610f\u8bc6\u91cc\u90fd\u4f1a\u8054\u60f3\u4e3aPython\u722c\u866b\uff0c\u4e3a\u4ec0\u4e48\u4f1a\u8fd9\u6837\uff0c\u6211\u89c9\u5f97\u6709\u4e24\u4e2a\u539f\u56e0\uff1a Python\u751f\u6001\u6781\u5176\u4e30\u5bcc\uff0c\u8bf8\u5982Request\u3001Beautiful Soup\u3001Scrapy\u3001PySpider\u7b49\u7b2c\u4e09\u65b9\u5e93\u5b9e\u5728\u5f3a\u5927 Python\u8bed\u6cd5\u7b80\u6d01\u6613\u4e0a\u624b\uff0c\u5206\u5206\u949f\u5c31\u80fd\u5199\u51fa\u4e00\u4e2a\u722c\u866b\uff08\u6709\u4eba\u5410\u69fdPython\u6162\uff0c\u4f46\u662f\u722c\u866b\u7684\u74f6\u9888\u548c\u8bed\u8a00\u5173\u7cfb\u4e0d\u5927\uff09 \u4efb\u4f55\u4e00\u4e2a\u5b66\u4e60Python\u7684\u7a0b\u5e8f\u5458\uff0c\u5e94\u8be5\u90fd\u6216\u591a\u6216\u5c11\u5730\u89c1\u8fc7\u751a\u81f3\u7814\u7a76\u8fc7\u722c\u866b\uff0c\u6211\u5f53\u65f6\u5199Python\u7684\u76ee\u7684\u5c31\u975e\u5e38\u7eaf\u7cb9\u2014\u2014\u4e3a\u4e86\u5199\u722c\u866b\u3002\u6240\u4ee5\u672c\u6587\u7684\u76ee\u7684\u5f88\u7b80\u5355\uff0c\u5c31\u662f\u8bf4\u8bf4\u6211\u4e2a\u4eba\u5bf9Python\u722c\u866b\u7684\u7406\u89e3\u4e0e\u5b9e\u8df5\uff0c\u4f5c\u4e3a\u4e00\u540d\u7a0b\u5e8f\u5458\uff0c\u6211\u89c9\u5f97\u4e86\u89e3\u4e00\u4e0b\u722c\u866b\u7684\u76f8\u5173\u77e5\u8bc6\u5bf9\u4f60\u53ea\u6709\u597d\u5904\uff0c\u6240\u4ee5\u8bfb\u5b8c\u8fd9\u7bc7\u6587\u7ae0\u540e\uff0c\u5982\u679c\u80fd\u5bf9\u4f60\u6709\u5e2e\u52a9\uff0c\u90a3\u4fbf\u518d\u597d\u4e0d\u8fc7","title":"\u8c08\u8c08\u5bf9Python\u722c\u866b\u7684\u7406\u89e3"},{"location":"cn/04_%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/1.%E8%B0%88%E8%B0%88%E5%AF%B9%20Python%20%E7%88%AC%E8%99%AB%E7%9A%84%E7%90%86%E8%A7%A3.html#_1","text":"\u722c\u866b\u662f\u4e00\u4e2a\u7a0b\u5e8f\uff0c\u8fd9\u4e2a\u7a0b\u5e8f\u7684\u76ee\u7684\u5c31\u662f\u4e3a\u4e86\u6293\u53d6\u4e07\u7ef4\u7f51\u4fe1\u606f\u8d44\u6e90\uff0c\u6bd4\u5982\u4f60\u65e5\u5e38\u4f7f\u7528\u7684\u8c37\u6b4c\u7b49\u641c\u7d22\u5f15\u64ce\uff0c\u641c\u7d22\u7ed3\u679c\u5c31\u5168\u90fd\u4f9d\u8d56\u722c\u866b\u6765\u5b9a\u65f6\u83b7\u53d6 \u770b\u4e0a\u8ff0\u641c\u7d22\u7ed3\u679c\uff0c\u9664\u4e86wiki\u76f8\u5173\u4ecb\u7ecd\u5916\uff0c\u722c\u866b\u6709\u5173\u7684\u641c\u7d22\u7ed3\u679c\u5168\u90fd\u5e26\u4e0a\u4e86Python\uff0c\u524d\u4eba\u8bf4Python\u722c\u866b\uff0c\u73b0\u5728\u770b\u6765\u679c\u7136\u8bda\u4e0d\u6b3a\u6211\uff5e \u722c\u866b\u7684\u76ee\u6807\u5bf9\u8c61\u4e5f\u5f88\u4e30\u5bcc\uff0c\u4e0d\u8bba\u662f\u6587\u5b57\u3001\u56fe\u7247\u3001\u89c6\u9891\uff0c\u4efb\u4f55\u7ed3\u6784\u5316\u975e\u7ed3\u6784\u5316\u7684\u6570\u636e\u722c\u866b\u90fd\u53ef\u4ee5\u722c\u53d6\uff0c\u722c\u866b\u7ecf\u8fc7\u53d1\u5c55\uff0c\u4e5f\u884d\u751f\u51fa\u4e86\u5404\u79cd\u722c\u866b\u7c7b\u578b\uff1a \u901a\u7528\u7f51\u7edc\u722c\u866b\uff1a\u722c\u53d6\u5bf9\u8c61\u4ece\u4e00\u4e9b\u79cd\u5b50 URL \u6269\u5145\u5230\u6574\u4e2a Web\uff0c\u641c\u7d22\u5f15\u64ce\u5e72\u7684\u5c31\u662f\u8fd9\u4e9b\u4e8b \u5782\u76f4\u7f51\u7edc\u722c\u866b\uff1a\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u4e3b\u9898\u8fdb\u884c\u722c\u53d6\uff0c\u6bd4\u5982\u4e13\u95e8\u722c\u53d6\u5c0f\u8bf4\u76ee\u5f55\u4ee5\u53ca\u7ae0\u8282\u7684\u5782\u76f4\u722c\u866b \u589e\u91cf\u7f51\u7edc\u722c\u866b\uff1a\u5bf9\u5df2\u7ecf\u6293\u53d6\u7684\u7f51\u9875\u8fdb\u884c\u5b9e\u65f6\u66f4\u65b0 \u6df1\u5c42\u7f51\u7edc\u722c\u866b\uff1a\u722c\u53d6\u4e00\u4e9b\u9700\u8981\u7528\u6237\u63d0\u4ea4\u5173\u952e\u8bcd\u624d\u80fd\u83b7\u5f97\u7684 Web \u9875\u9762 \u4e0d\u60f3\u8bf4\u8fd9\u4e9b\u5927\u65b9\u5411\u7684\u6982\u5ff5\uff0c\u8ba9\u6211\u4eec\u4ee5\u4e00\u4e2a\u83b7\u53d6\u7f51\u9875\u5185\u5bb9\u4e3a\u4f8b\uff0c\u4ece\u722c\u866b\u6280\u672f\u672c\u8eab\u51fa\u53d1\uff0c\u6765\u8bf4\u8bf4\u7f51\u9875\u722c\u866b\uff0c\u6b65\u9aa4\u5982\u4e0b\uff1a \u6a21\u62df\u8bf7\u6c42\u7f51\u9875\u8d44\u6e90 \u4eceHTML\u63d0\u53d6\u76ee\u6807\u5143\u7d20 \u6570\u636e\u6301\u4e45\u5316 \u4ec0\u4e48\u662f\u722c\u866b\uff0c\u8fd9\u5c31\u662f\u722c\u866b\uff1a \"\"\"\u8ba9\u6211\u4eec\u6839\u636e\u4e0a\u9762\u8bf4\u7684\u6b65\u9aa4\u6765\u5b8c\u6210\u4e00\u4e2a\u7b80\u5355\u7684\u722c\u866b\u7a0b\u5e8f\"\"\" import requests from bs4 import BeautifulSoup target_url = 'http://www.baidu.com/s?wd=\u722c\u866b' # \u7b2c\u4e00\u6b65 \u53d1\u8d77\u4e00\u4e2aGET\u8bf7\u6c42 res = requests . get ( target_url ) # \u7b2c\u4e8c\u6b65 \u63d0\u53d6HTML\u5e76\u89e3\u6790\u60f3\u83b7\u53d6\u7684\u6570\u636e \u6bd4\u5982\u83b7\u53d6 title soup = BeautifulSoup ( res . text , \"lxml\" ) # \u8f93\u51fa soup.title.text title = soup . title . text # \u7b2c\u4e09\u6b65 \u6301\u4e45\u5316 \u6bd4\u5982\u4fdd\u5b58\u5230\u672c\u5730 with open ( 'title.txt' , 'w' ) as fp : fp . write ( title ) \u52a0\u4e0a\u6ce8\u91ca\u4e0d\u523020\u884c\u4ee3\u7801\uff0c\u4f60\u5c31\u5b8c\u6210\u4e86\u4e00\u4e2a\u722c\u866b\uff0c\u7b80\u5355\u5427","title":"\u4ec0\u4e48\u662f\u722c\u866b"},{"location":"cn/04_%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/1.%E8%B0%88%E8%B0%88%E5%AF%B9%20Python%20%E7%88%AC%E8%99%AB%E7%9A%84%E7%90%86%E8%A7%A3.html#_2","text":"\u7f51\u9875\u4e16\u754c\u591a\u59ff\u591a\u5f69\u3001\u4ebf\u4e07\u7f51\u9875\u8d44\u6e90\u4f9b\u4f60\u9009\u62e9\uff0c\u9762\u5bf9\u4e0d\u540c\u7684\u9875\u9762\uff0c\u600e\u4e48\u4f7f\u81ea\u5df1\u7f16\u5199\u7684\u722c\u866b\u7a0b\u5e8f\u591f\u7a33\u5065\u3001\u6301\u4e45\uff0c\u8fd9\u662f\u4e00\u4e2a\u503c\u5f97\u8ba8\u8bba\u7684\u95ee\u9898 \u4fd7\u8bdd\u8bf4\uff0c\u78e8\u5200\u4e0d\u8bef\u780d\u67f4\u5de5\uff0c\u5728\u5f00\u59cb\u7f16\u5199\u722c\u866b\u4e4b\u524d\uff0c\u5f88\u6709\u5fc5\u8981\u638c\u63e1\u4e00\u4e9b\u57fa\u672c\u77e5\u8bc6\uff1a \u7f51\u9875\u7684\u7ed3\u6784\u662fHTML\uff0c\u722c\u866b\u7684\u76ee\u6807\u5c31\u662f\u89e3\u6790HTML\uff0c\u83b7\u53d6\u76ee\u6807\u5b57\u6bb5\u5e76\u4fdd\u5b58 \u5ba2\u6237\u7aef\u5c55\u73b0\u7684\u7f51\u9875\u7531\u6d4f\u89c8\u5668\u6e32\u67d3\uff0c\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u7aef\u7684\u4fe1\u606f\u4ea4\u4e92\u4f9d\u9760HTTP\u534f\u8bae \u8fd9\u4e24\u53e5\u63cf\u8ff0\u4f53\u73b0\u4e86\u4e00\u540d\u722c\u866b\u5f00\u53d1\u4eba\u5458\u9700\u8981\u638c\u63e1\u7684\u57fa\u672c\u77e5\u8bc6\uff0c\u4e0d\u8fc7\u4e00\u540d\u57fa\u672c\u7684\u540e\u7aef\u6216\u8005\u524d\u7aef\u5de5\u7a0b\u5e08\u90fd\u4f1a\u8fd9\u4e9b\u54c8\u54c8\uff0c\u8fd9\u4e5f\u8bf4\u660e\u4e86\u722c\u866b\u7684\u5165\u95e8\u96be\u5ea6\u6781\u4f4e\uff0c\u4ece\u8fd9\u4e24\u53e5\u8bdd\uff0c\u4f60\u80fd\u601d\u8003\u51fa\u54ea\u4e9b\u722c\u866b\u5fc5\u5907\u7684\u77e5\u8bc6\u70b9\u5462\uff1f \u57fa\u672c\u7684HTML\u77e5\u8bc6\uff0c\u4e86\u89e3HTML\u624d\u65b9\u4fbf\u76ee\u6807\u4fe1\u606f\u63d0\u53d6 \u57fa\u672c\u7684JS\u77e5\u8bc6 \uff0cJS\u53ef\u4ee5\u5f02\u6b65\u52a0\u8f7dHTML \u4e86\u89e3CSS Selector\u3001XPath\u4ee5\u53ca\u6b63\u5219\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u63d0\u53d6\u6570\u636e \u4e86\u89e3HTTP\u534f\u8bae\uff0c\u4e3a\u540e\u9762\u7684\u53cd\u722c\u866b\u6597\u4e89\u6253\u4e0b\u57fa\u7840 \u4e86\u89e3\u57fa\u672c\u7684\u6570\u636e\u5e93\u64cd\u4f5c\uff0c\u4e3a\u4e86\u6570\u636e\u6301\u4e45\u5316 \u6709\u4e86\u8fd9\u4e9b\u77e5\u8bc6\u50a8\u5907\uff0c\u63a5\u4e0b\u6765\u5c31\u53ef\u4ee5\u9009\u62e9\u4e00\u95e8\u8bed\u8a00\uff0c\u5f00\u59cb\u7f16\u5199\u81ea\u5df1\u7684\u722c\u866b\u7a0b\u5e8f\u4e86\uff0c\u8fd8\u662f\u6309\u7167\u4e0a\u4e00\u8282\u8bf4\u7684\u4e09\u4e2a\u6b65\u9aa4\uff0c\u7136\u540e\u4ee5Python\u4e3a\u4f8b\uff0c\u8bf4\u4e00\u8bf4\u8981\u5728\u7f16\u7a0b\u8bed\u8a00\u65b9\u9762\u505a\u90a3\u4e9b\u51c6\u5907\uff1a \u7f51\u9875\u8bf7\u6c42\uff1a\u5185\u7f6e\u6709urllib\u5e93\uff0c\u7b2c\u4e09\u65b9\u5e93\u7684\u8bdd\uff0c\u540c\u6b65\u8bf7\u6c42\u53ef\u4ee5\u4f7f\u7528requests\uff0c\u5f02\u6b65\u8bf7\u6c42\u4f7f\u7528aiohttp \u5206\u6790HTML\u7ed3\u6784\u5e76\u63d0\u53d6\u76ee\u6807\u5143\u7d20\uff1aCSS Selector\u548cXPath\u662f\u76ee\u524d\u4e3b\u6d41\u7684\u63d0\u53d6\u65b9\u5f0f\uff0c\u7b2c\u4e09\u65b9\u5e93\u53ef\u4ee5\u4f7f\u7528Beautiful Soup\u6216\u8005PyQuery \u6570\u636e\u6301\u4e45\u5316\uff1a\u76ee\u6807\u6570\u636e\u63d0\u53d6\u4e4b\u540e\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u4fdd\u5b58\u5230\u6570\u636e\u5e93\u4e2d\u8fdb\u884c\u6301\u4e45\u5316\uff0cMySQL\u3001MongoDB\u7b49\uff0c\u8fd9\u4e9b\u90fd\u6709\u5bf9\u5e94\u7684\u5e93\u652f\u6301\uff0c\u5f53\u7136\u4f60\u4e5f\u53ef\u4ee5\u4fdd\u5b58\u5728\u786c\u76d8\uff0c\u8c01\u786c\u76d8\u6ca1\u70b9\u4e1c\u897f\u5bf9\u5427\uff08\u6ed1\u7a3d\u8138\uff09 \u638c\u63e1\u4e86\u4e0a\u9762\u8fd9\u4e9b\uff0c\u4f60\u5927\u53ef\u653e\u5f00\u624b\u811a\u5927\u5e72\u4e00\u573a\uff0c\u4e07\u7ef4\u7f51\u5c31\u662f\u4f60\u7684\u540d\u5229\u573a\uff0c\u53bb\u5427\uff5e \u6211\u89c9\u5f97\u5bf9\u4e8e\u4e00\u4e2a\u76ee\u6807\u7f51\u7ad9\u7684\u7f51\u9875\uff0c\u53ef\u4ee5\u5206\u4e0b\u9762\u56db\u4e2a\u7c7b\u578b\uff1a \u5355\u9875\u9762\u5355\u76ee\u6807 \u5355\u9875\u9762\u591a\u76ee\u6807 \u591a\u9875\u9762\u5355\u76ee\u6807 \u591a\u9875\u9762\u591a\u76ee\u6807 \u5177\u4f53\u662f\u4ec0\u4e48\u610f\u601d\u5462\uff0c\u53ef\u80fd\u770b\u8d77\u6765\u6709\u70b9\u7ed5\uff0c\u4f46\u660e\u767d\u8fd9\u4e9b\uff0c\u4f60\u4e4b\u540e\u5199\u722c\u866b\uff0c\u53ea\u8981\u5728\u8111\u5b50\u91cc\u9762\u8fc7\u4e00\u904d\u7740\u7f51\u9875\u5bf9\u5e94\u4ec0\u4e48\u7c7b\u578b\uff0c\u7136\u540e\u5957\u4e0a\u5bf9\u5e94\u7c7b\u578b\u7684\u7a0b\u5e8f\uff08\u5199\u591a\u4e86\u90fd\u5e94\u8be5\u6709\u4e00\u5957\u81ea\u5df1\u7684\u5e38\u7528\u4ee3\u7801\u5e93\uff09\uff0c\u90a3\u5199\u722c\u866b\u7684\u901f\u5ea6\uff0c\u81ea\u7136\u4e0d\u4f1a\u6162 \u5355\u9875\u9762\u5355\u76ee\u6807 \u901a\u4fd7\u6765\u8bf4\uff0c\u5c31\u662f\u5728\u8fd9\u4e2a\u7f51\u9875\u91cc\u9762\uff0c\u6211\u4eec\u7684\u76ee\u6807\u5c31\u53ea\u6709\u4e00\u4e2a\uff0c\u5047\u8bbe\u6211\u4eec\u7684\u9700\u6c42\u662f\u6293\u53d6\u8fd9\u90e8 \u7535\u5f71-\u8096\u7533\u514b\u7684\u6551\u8d4e \u7684\u540d\u79f0\uff0c\u9996\u5148\u6253\u5f00\u7f51\u9875\u53f3\u952e\u5ba1\u67e5\u5143\u7d20\uff0c\u627e\u5230\u7535\u5f71\u540d\u79f0\u5bf9\u5e94\u7684\u5143\u7d20\u4f4d\u7f6e\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u5728\u67d0\u4e2a\u5355\u4e00\u9875\u9762\u5185\uff0c\u770b\u76ee\u6807\u662f\u4e0d\u662f\u53ea\u6709\u4e00\u4e2a\uff0c\u4e00\u773c\u5c31\u80fd\u770b\u51fa\u6807\u9898\u7684CSS Selector\u89c4\u5219\u4e3a\uff1a #content > h1 > span:nth-child(1) \uff0c\u7136\u540e\u7528\u6211\u81ea\u5df1\u5199\u7684\u5e38\u7528\u5e93\uff0c\u6211\u7528\u4e0d\u5230\u5341\u884c\u4ee3\u7801\u5c31\u80fd\u5199\u5b8c\u6293\u53d6\u8fd9\u4e2a\u9875\u9762\u7535\u5f71\u540d\u79f0\u7684\u722c\u866b\uff1a import asyncio from ruia import Item , TextField class DoubanItem ( Item ): \"\"\" \u5b9a\u4e49\u722c\u866b\u7684\u76ee\u6807\u5b57\u6bb5 \"\"\" title = TextField ( css_select = '#content > h1 > span:nth-child(1)' ) async_func = DoubanItem . get_item ( url = \"https://movie.douban.com/subject/1292052/\" ) item = asyncio . get_event_loop () . run_until_complete ( async_func ) print ( item ) \u591a\u9875\u9762\u591a\u76ee\u6807\u5c31\u662f\u6b64\u60c5\u51b5\u4e0b\u591a\u4e2aurl\u7684\u884d\u751f\u60c5\u51b5 \u5355\u9875\u9762\u591a\u76ee\u6807 \u5047\u8bbe\u73b0\u5728\u7684\u9700\u6c42\u662f\u6293\u53d6 \u8c46\u74e3\u7535\u5f71250 \u7b2c\u4e00\u9875\u4e2d\u7684\u6240\u6709\u7535\u5f71\u540d\u79f0\uff0c\u4f60\u9700\u8981\u63d0\u53d625\u4e2a\u7535\u5f71\u540d\u79f0\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u76ee\u6807\u9875\u7684\u76ee\u6807\u6570\u636e\u662f\u591a\u4e2aitem\u7684\uff0c\u6240\u4ee5\u76ee\u6807\u9700\u8981\u5faa\u73af\u83b7\u53d6\u3002 \u5bf9\u4e8e\u8fd9\u4e2a\u60c5\u51b5\uff0c\u6211\u5728 Item \u4e2d\u9650\u5236\u4e86\u4e00\u70b9\uff0c\u5f53\u4f60\u5b9a\u4e49\u7684\u722c\u866b\u9700\u8981\u5728\u67d0\u4e00\u9875\u9762\u5faa\u73af\u83b7\u53d6\u4f60\u7684\u76ee\u6807\u65f6\uff0c\u5219\u9700\u8981\u5b9a\u4e49 target_item \u5c5e\u6027\uff08\u5c31\u662f\u622a\u56fe\u4e2d\u7684\u7ea2\u6846\uff09 \u5bf9\u4e8e\u8c46\u74e3250\u8fd9\u4e2a\u9875\u9762\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f25\u90e8\u7535\u5f71\u4fe1\u606f\uff0c\u6240\u4ee5\u8be5\u8fd9\u6837\u5b9a\u4e49\uff1a field css_select target_item\uff08\u5fc5\u987b\uff09 div.item title span.title \u4ee3\u7801\u5b9e\u73b0\u5982\u4e0b\uff1a import asyncio from ruia import Item , TextField class DoubanItem ( Item ): \"\"\" \u5b9a\u4e49\u722c\u866b\u7684\u76ee\u6807\u5b57\u6bb5 \"\"\" target_item = TextField ( css_select = \"div.item\" ) title = TextField ( css_select = \"span.title\" ) async def clean_title ( self , title ): \"\"\" \u5bf9\u63d0\u53d6\u7684\u76ee\u6807\u6570\u636e\u8fdb\u884c\u6e05\u6d17 \u53ef\u9009 :param title: \u521d\u6b65\u63d0\u53d6\u7684\u76ee\u6807\u6570\u636e :return: \"\"\" if isinstance ( title , str ): return title else : return \"\" . join ([ i . text . strip () . replace ( \" \\xa0 \" , \"\" ) for i in title ]) async def run_item ( url : str ): async for item in DoubanItem . get_items ( url = url ): print ( item ) items = asyncio . get_event_loop () . run_until_complete ( run_item ( \"https://movie.douban.com/top250\" ) ) \u591a\u9875\u9762\u591a\u76ee\u6807 \u591a\u9875\u9762\u591a\u76ee\u6807\u662f\u4e0a\u8ff0\u5355\u9875\u9762\u591a\u76ee\u6807\u60c5\u51b5\u7684\u884d\u751f\uff0c\u5728\u8fd9\u4e2a\u95ee\u9898\u4e0a\u6765\u770b\uff0c\u6b64\u65f6\u5c31\u662f\u83b7\u53d6\u6240\u6709\u5206\u9875\u7684\u7535\u5f71\u540d\u79f0 from ruia import Item , Request , Spider , TextField class DoubanItem ( Item ): \"\"\" \u5b9a\u4e49\u722c\u866b\u7684\u76ee\u6807\u5b57\u6bb5 \"\"\" target_item = TextField ( css_select = \"div.item\" ) title = TextField ( css_select = \"span.title\" ) async def clean_title ( self , title ): if isinstance ( title , str ): return title else : return \"\" . join ([ i . text . strip () . replace ( \" \\xa0 \" , \"\" ) for i in title ]) class DoubanSpider ( Spider ): start_urls = [ \"https://movie.douban.com/top250\" ] concurrency = 10 async def parse ( self , res ): etree = res . html_etree ( html = await res . text ()) pages = [ \"?start=0&filter=\" ] + [ i . get ( \"href\" ) for i in etree . cssselect ( \".paginator>a\" ) ] for index , page in enumerate ( pages ): url = self . start_urls [ 0 ] + page yield Request ( url , callback = self . parse_item , metadata = { \"index\" : index }, request_config = self . request_config , ) async def parse_item ( self , res ): res_list = [] async for item in DoubanItem . get_items ( html = await res . text ()): res_list . append ( item . title ) return res_list if __name__ == \"__main__\" : DoubanSpider . start () \u5982\u679c\u7f51\u7edc\u6ca1\u95ee\u9898\u7684\u8bdd\uff0c\u4f1a\u5f97\u5230\u5982\u4e0b\u8f93\u51fa\uff1a \u6ce8\u610f\u722c\u866b\u8fd0\u884c\u65f6\u95f4\uff0c1s\u4e0d\u5230\uff0c\u8fd9\u5c31\u662f\u5f02\u6b65\u7684\u9b45\u529b \u7528Python\u5199\u722c\u866b\uff0c\u5c31\u662f\u8fd9\u4e48\u7b80\u5355\u4f18\u96c5\uff0c\u8bf8\u4f4d\uff0c\u770b\u7740\u7f51\u9875\u5c31\u601d\u8003\u4e0b\uff1a \u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u76ee\u6807\u7c7b\u578b \u7528\u4ec0\u4e48\u5e93\u6a21\u62df\u8bf7\u6c42 \u600e\u4e48\u89e3\u6790\u76ee\u6807\u5b57\u6bb5 \u600e\u4e48\u5b58\u50a8 \u4e00\u4e2a\u722c\u866b\u7a0b\u5e8f\u5c31\u6210\u578b\u4e86\uff0c\u987a\u4fbf\u4e00\u63d0\uff0c\u722c\u866b\u8fd9\u4e1c\u897f\uff0c\u53ef\u4ee5\u8bf4\u662f\u9632\u541b\u5b50\u4e0d\u9632\u5c0f\u4eba\uff0c robots.txt \u5927\u90e8\u5206\u7f51\u7ad9\u90fd\u6709\uff08\u5b83\u7684\u76ee\u7684\u662f\u544a\u8bc9\u722c\u866b\u4ec0\u4e48\u53ef\u4ee5\u722c\u53d6\u4ec0\u4e48\u4e0d\u53ef\u4ee5\u722c\u53d6\uff0c\u6bd4\u5982\uff1a https://www.baidu.com/robots.txt \uff09\uff0c\u5404\u4f4d\u60f3\u600e\u4e48\u722c\u53d6\uff0c\u81ea\u5df1\u8861\u91cf","title":"\u600e\u4e48\u5199\u722c\u866b"},{"location":"cn/04_%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/1.%E8%B0%88%E8%B0%88%E5%AF%B9%20Python%20%E7%88%AC%E8%99%AB%E7%9A%84%E7%90%86%E8%A7%A3.html#_3","text":"\u4e0d\u8981\u4ee5\u4e3a\u5199\u597d\u4e00\u4e2a\u722c\u866b\u7a0b\u5e8f\u5c31\u53ef\u4ee5\u51fa\u5e08\u4e86\uff0c\u6b64\u65f6\u8fd8\u6709\u66f4\u591a\u7684\u95ee\u9898\u5728\u524d\u9762\u7b49\u7740\u4f60\uff0c\u4f60\u8981\u542b\u60c5\u8109\u8109\u5730\u770b\u7740\u4f60\u7684\u722c\u866b\u7a0b\u5e8f\uff0c\u95ee\u81ea\u5df1\u4e09\u4e2a\u95ee\u9898\uff1a \u722c\u866b\u6293\u53d6\u6570\u636e\u540e\u662f\u6b63\u5f53\u7528\u9014\u4e48\uff1f \u722c\u866b\u4f1a\u628a\u76ee\u6807\u7f51\u7ad9\u5e72\u6389\u4e48\uff1f \u722c\u866b\u4f1a\u88ab\u53cd\u722c\u866b\u5e72\u6389\u4e48\uff1f \u524d\u4e24\u4e2a\u5173\u4e8e\u4eba\u6027\u7684\u95ee\u9898\u5728\u6b64\u4e0d\u505a\u8fc7\u591a\u53d9\u8ff0\uff0c\u56e0\u6b64\u8df3\u8fc7\uff0c\u4f46\u4f60\u4eec\u5982\u679c\u4f5c\u4e3a\u722c\u866b\u5de5\u7a0b\u5e08\u7684\u8bdd\uff0c\u5207\u4e0d\u53ef\u8df3\u8fc7 \u4f1a\u88ab\u53cd\u722c\u866b\u5e72\u6389\u4e48\uff1f \u6700\u540e\u5173\u4e8e\u53cd\u722c\u866b\u7684\u95ee\u9898\u624d\u662f\u4f60\u722c\u866b\u7a0b\u5e8f\u5f3a\u58ee\u4e0e\u5426\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4ec0\u4e48\u662f\u53cd\u722c\u866b\uff1f \u5f53\u8d8a\u6765\u8d8a\u591a\u7684\u722c\u866b\u5728\u4e92\u8054\u7f51\u4e0a\u6a2a\u51b2\u76f4\u649e\u540e\uff0c\u7f51\u9875\u8d44\u6e90\u7ef4\u62a4\u8005\u4e3a\u4e86\u9632\u6b62\u81ea\u8eab\u6570\u636e\u88ab\u6293\u53d6\uff0c\u5f00\u59cb\u8fdb\u884c\u4e00\u7cfb\u5217\u7684\u63aa\u65bd\u6765\u4f7f\u5f97\u81ea\u8eab\u6570\u636e\u4e0d\u6613\u88ab\u522b\u7684\u7a0b\u5e8f\u722c\u53d6\uff0c\u8fd9\u4e9b\u63aa\u65bd\u5c31\u662f\u53cd\u722c\u866b \u6bd4\u5982\u68c0\u6d4bIP\u8bbf\u95ee\u9891\u7387\u3001\u8d44\u6e90\u8bbf\u95ee\u901f\u5ea6\u3001\u94fe\u63a5\u662f\u5426\u5e26\u6709\u5173\u952e\u53c2\u6570\u3001\u9a8c\u8bc1\u7801\u68c0\u6d4b\u673a\u5668\u4eba\u3001ajax\u6df7\u6dc6\u3001js\u52a0\u5bc6\u7b49\u7b49 \u5bf9\u4e8e\u76ee\u524d\u5e02\u573a\u4e0a\u7684\u53cd\u722c\u866b\uff0c\u722c\u866b\u5de5\u7a0b\u5e08\u5e38\u6709\u7684\u53cd\u53cd\u722c\u866b\u65b9\u6848\u662f\u4e0b\u9762\u8fd9\u6837\u7684\uff1a \u4e0d\u65ad\u8bd5\u63a2\u76ee\u6807\u5e95\u7ebf\uff0c\u8bd5\u51fa\u5355IP\u4e0b\u6700\u4f18\u7684\u8bbf\u95ee\u9891\u7387 \u6784\u5efa\u81ea\u5df1\u7684IP\u4ee3\u7406\u6c60 \u7ef4\u62a4\u4e00\u4efd\u81ea\u5df1\u5e38\u7528\u7684UA\u5e93 \u9488\u5bf9\u76ee\u6807\u7f51\u9875\u7684Cookie\u6c60 \u9700\u8981JS\u6e32\u67d3\u7684\u7f51\u9875\u4f7f\u7528\u65e0\u5934\u6d4f\u89c8\u5668\u8fdb\u884c\u4ee3\u7801\u6e32\u67d3\u518d\u6293\u53d6 \u4e00\u5957\u7834\u89e3\u9a8c\u8bc1\u7801\u7a0b\u5e8f \u624e\u5b9e\u7684JS\u77e5\u8bc6\u6765\u7834\u89e3\u6df7\u6dc6\u51fd\u6570 \u722c\u866b\u5de5\u7a0b\u5e08\u7684\u8fdb\u9636\u4e4b\u8def\u5176\u5b9e\u5c31\u662f\u4e0d\u65ad\u53cd\u53cd\u722c\u866b\uff0c\u53ef\u8c13\u8270\u8f9b\uff0c\u4f46\u6362\u4e2a\u89d2\u5ea6\u60f3\u4e5f\u662f\u4e50\u8da3\u6240\u5728 \u5173\u4e8e\u6846\u67b6 \u722c\u866b\u6709\u81ea\u5df1\u7684\u7f16\u5199\u6d41\u7a0b\u548c\u6807\u51c6\uff0c\u6709\u4e86\u6807\u51c6\uff0c\u81ea\u7136\u5c31\u6709\u4e86\u6846\u67b6\uff0c\u50cfPython\u8fd9\u79cd\u751f\u6001\u5f3a\u5927\u7684\u8bed\u8a00\uff0c\u6846\u67b6\u81ea\u7136\u662f\u591a\u4e0d\u80dc\u6570\uff0c\u76ee\u524d\u4e16\u9762\u4e0a\u7528\u7684\u6bd4\u8f83\u591a\u7684\u6709\uff1a Scrapy PySpider Portia Ruia \uff1a\u5bb9\u6211\u81ea\u8350\u4e00\u6ce2 \u8fd9\u91cc\u4e0d\u8fc7\u591a\u4ecb\u7ecd\uff0c\u6846\u67b6\u53ea\u662f\u5de5\u5177\uff0c\u662f\u4e00\u79cd\u63d0\u5347\u6548\u7387\u7684\u65b9\u5f0f\uff0c\u770b\u4f60\u9009\u62e9","title":"\u5982\u4f55\u8fdb\u9636"},{"location":"cn/04_%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/1.%E8%B0%88%E8%B0%88%E5%AF%B9%20Python%20%E7%88%AC%E8%99%AB%E7%9A%84%E7%90%86%E8%A7%A3.html#_4","text":"\u4efb\u4f55\u4e8b\u7269\u90fd\u6709\u4e24\u9762\u6027\uff0c\u722c\u866b\u81ea\u7136\u4e5f\u4e0d\u4f8b\u5916\uff0c\u56e0\u6b64\u6211\u9001\u8bf8\u4f4d\u4e00\u5f20\u56fe\uff0c\u5173\u952e\u65f6\u523b\u597d\u597d\u60f3\u60f3 \u6700\u540e\uff0c\u6b22\u8fce\u4e00\u8d77\u4ea4\u6d41\uff1a","title":"\u8bf4\u660e"},{"location":"cn/04_%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/_index.html","text":"weight: 5 bookFlatSection: false title: \"\u5b9e\u8df5\u6307\u5357\"","title":" index"},{"location":"en/quickstart.html","text":"Create a Typical Ruia Spider Let's fetch some news from Hacker News in four steps: Define item Test item Write spider Run Step 1: Define Item After analyzing HTML structure, we define the following data item. The skill of analyzing HTML structure is important for a spider engineer, Ruia believe you have already had this skill, and won't talk about it here. from ruia import Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) It's easy to understand: we want to get an item from HTML structure, the item contains two fields: title and url . Wait! What is target_item ? target_item is a built-in Ruia field , indicates that the HTML element matched by its selectors contains one item. In this example, we are crawling a catalogue of Hacker News, and there are many news items in one page. target_item tells Ruia to focus on these HTML elements when extracting field. Step 2: Test Item Ruia is a low-coupling web crawling framework. Each class can be used separately in your project. You can even write a simple spider with only ruia.Item , ruia.TextField and ruia.AttrField . This feature provides a convenient way to test HackerNewsItem . import asyncio from ruia import Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def test_item (): url = 'https://news.ycombinator.com/news?p=1' async for item in HackerNewsItem . get_items ( url = url ): print ( ' {} : {} ' . format ( item . title , item . url )) if __name__ == '__main__' : # Python 3.7 Required. asyncio . run ( test_item ()) # For Python 3.6 # loop = asyncio.get_event_loop() # loop.run_until_complete(test_item()) Waiting for the output in your console. Step 3: Write Spider Ruia.spider is used to control requests and responses, such as concurrency control. It's important for a spider, or you will be banned by the server in one minute. By default, the concurrency is 3. \"\"\" Target: https://news.ycombinator.com/ pip install aiofiles \"\"\" import aiofiles from ruia import Item , TextField , AttrField , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): concurrency = 2 start_urls = [ f 'https://news.ycombinator.com/news?p= { index } ' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) Just define a property of the subclass of Spider . In this example, we crawl in two coroutines. If you do not know coroutine, as a crawler engineer, ruia believe you know the threading pool of spider. Coroutine is a more efficient way to behave like threading pool. parse(self, response) is the entry point of a spider. After starting a spider, it send requests to web server. Once received a response, ruia.Spider will call its parse function to extract data from HTML source code. Step 4: Run Now everything is ready. Run! import aiofiles from ruia import Item , TextField , AttrField , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): concurrency = 2 start_urls = [ f 'https://news.ycombinator.com/news?p= { index } ' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Hey, notice that, do not run Spider.start() in a await statement! It's just a normal function ! You just create a spider in one python file! Amazing!","title":"Quick Start"},{"location":"en/quickstart.html#create-a-typical-ruia-spider","text":"Let's fetch some news from Hacker News in four steps: Define item Test item Write spider Run","title":"Create a Typical Ruia Spider"},{"location":"en/quickstart.html#step-1-define-item","text":"After analyzing HTML structure, we define the following data item. The skill of analyzing HTML structure is important for a spider engineer, Ruia believe you have already had this skill, and won't talk about it here. from ruia import Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) It's easy to understand: we want to get an item from HTML structure, the item contains two fields: title and url . Wait! What is target_item ? target_item is a built-in Ruia field , indicates that the HTML element matched by its selectors contains one item. In this example, we are crawling a catalogue of Hacker News, and there are many news items in one page. target_item tells Ruia to focus on these HTML elements when extracting field.","title":"Step 1: Define Item"},{"location":"en/quickstart.html#step-2-test-item","text":"Ruia is a low-coupling web crawling framework. Each class can be used separately in your project. You can even write a simple spider with only ruia.Item , ruia.TextField and ruia.AttrField . This feature provides a convenient way to test HackerNewsItem . import asyncio from ruia import Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def test_item (): url = 'https://news.ycombinator.com/news?p=1' async for item in HackerNewsItem . get_items ( url = url ): print ( ' {} : {} ' . format ( item . title , item . url )) if __name__ == '__main__' : # Python 3.7 Required. asyncio . run ( test_item ()) # For Python 3.6 # loop = asyncio.get_event_loop() # loop.run_until_complete(test_item()) Waiting for the output in your console.","title":"Step 2: Test Item"},{"location":"en/quickstart.html#step-3-write-spider","text":"Ruia.spider is used to control requests and responses, such as concurrency control. It's important for a spider, or you will be banned by the server in one minute. By default, the concurrency is 3. \"\"\" Target: https://news.ycombinator.com/ pip install aiofiles \"\"\" import aiofiles from ruia import Item , TextField , AttrField , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): concurrency = 2 start_urls = [ f 'https://news.ycombinator.com/news?p= { index } ' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) Just define a property of the subclass of Spider . In this example, we crawl in two coroutines. If you do not know coroutine, as a crawler engineer, ruia believe you know the threading pool of spider. Coroutine is a more efficient way to behave like threading pool. parse(self, response) is the entry point of a spider. After starting a spider, it send requests to web server. Once received a response, ruia.Spider will call its parse function to extract data from HTML source code.","title":"Step 3: Write Spider"},{"location":"en/quickstart.html#step-4-run","text":"Now everything is ready. Run! import aiofiles from ruia import Item , TextField , AttrField , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): concurrency = 2 start_urls = [ f 'https://news.ycombinator.com/news?p= { index } ' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Hey, notice that, do not run Spider.start() in a await statement! It's just a normal function ! You just create a spider in one python file! Amazing!","title":"Step 4: Run"},{"location":"en/apis/field.html","text":"Define Data with Fields Overview Fields are used to extract value from HTML code. Ruia supports the following fields: ElementField : extract LXML element(s) from the selected HTML element TextField : extract text string of the selected HTML element AttrField : extract an attribute of the selected HTML element HtmlField : extract raw HTML code of the selected HTML element RegexField : use standard library re for better performance Note All the parameters of fields are keyword arguments . ElementField ElementField first select an HTML element by CSS Selector or XPath Selector, then get the LXML element(s) from the selected element. Parameters attr : str , required, the name of the attribute you want to extract css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True Example import ruia from lxml import etree HTML = ''' <body> <div class=\"title\" href=\"/\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_element_field (): ul = ruia . ElementField ( css_select = \"ul\" ) assert len ( ul . extract ( html_etree = html ) . xpath ( '//li' )) == 3 TextField TextField first select an HTML element by CSS Selector or XPath Selector, then get the text value of the selected element. Parameters css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True Example import ruia from lxml import etree HTML = ''' <body> <div class=\"title\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_text_field (): title_field = ruia . TextField ( css_select = '.title' , default = 'Untitled' ) assert title_field . extract ( html_etree = html ) == 'Ruia Documentation' tag_field = ruia . TextField ( css_select = '.tag' , default = 'No tag' , many = True ) assert tag_field . extract ( html_etree = html ) == [ 'easy' , 'fast' , 'powerful' ] AttrField TextField first select an HTML element by CSS Selector or XPath Selector, then get the attribute value of the selected element. Parameters attr : str , required, the name of the attribute you want to extract css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True Example import ruia from lxml import etree HTML = ''' <body> <div class=\"title\" href=\"/\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_attr_field (): title = ruia . AttrField ( css_select = '.title' , attr = 'href' , default = 'Untitled' ) assert title . extract ( html_etree = html ) == '/' tags = ruia . AttrField ( css_select = '.tag' , attr = 'href' , default = 'No tag' , many = True ) assert tags . extract ( html_etree = html )[ 0 ] == './easy.html' HtmlField TextField first select an HTML element by CSS Selector or XPath Selector, then get the raw HTML code of the selected element. If there's some spaces or some text outside any HTML elements between this element and next element, then this part of text will also inside the return value. It's an unstable feature, perhaps in later versions the outside text will be remove by default. Parameters css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True Example import ruia from lxml import etree HTML = ''' <body> <div class=\"title\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_html_field (): title = ruia . HtmlField ( css_select = '.title' , default = 'Untitled' ) assert title . extract ( html_etree = html ) == '<div class=\"title\" href=\"/\">Ruia Documentation</div> \\n ' tags = ruia . HtmlField ( css_select = '.tag' , default = 'No tag' , many = True ) assert tags . extract ( html_etree = html )[ 1 ] == '<li class=\"tag\" href=\"./fast.html\">fast</li> \\n ' RegexField TextField do not parse html structure, it directly use python standard library re . If your spider meets performance limitation, try RegexField . However, ruia is based on asyncio , you will seldom meet performance limitation! RegexField has a complex behaviour: if no group: return the whole matched string if regex has a group: return the group value if regex has multiple groups: return a list a string if regex has named groups, no matter one or more: return a dict, whose key and value are both string if many=True , return a list of above values Parameters re_select : str , required, match HTML element(s) with regular expression default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True Example import ruia from lxml import etree HTML = ''' <body> <div class=\"title\" href=\"/\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_regex_field (): title = ruia . RegexField ( re_select = '<div class=\"title\" href=\"(.*?)\">(.*?)</div>' ) assert title . extract ( html = HTML )[ 0 ] == '/' assert title . extract ( html = HTML )[ 1 ] == 'Ruia Documentation' tags = ruia . RegexField ( re_select = '<li class=\"tag\" href=\"(?P<href>.*?)\">(?P<text>.*?)</li>' , many = True ) result = tags . extract ( html = HTML ) assert isinstance ( result , list ) assert len ( result ) == 3 assert isinstance ( result [ 0 ], dict ) assert result [ 0 ][ 'href' ] == './easy.html' About Parameter many Parameter many=False indicates if the field will extract one value or multiple values from HTML source code. For example, one Github Issue has many tags, We can use Item.get_items to get multiple values of tags, but that means an extra class definition. Parameter many aims to solve this problem. A field is default by many=False , that means, for TextField , AttrField and HtmlField , Field.extract(*, **) will always return a string, and RegexField will return a string or a list or dict, depending on whether there are groups in the regular expression. We can consider it with a 'singular number'. With many=True , each field will return a 'plural', that is, return a list.","title":"Field"},{"location":"en/apis/field.html#define-data-with-fields","text":"","title":"Define Data with Fields"},{"location":"en/apis/field.html#overview","text":"Fields are used to extract value from HTML code. Ruia supports the following fields: ElementField : extract LXML element(s) from the selected HTML element TextField : extract text string of the selected HTML element AttrField : extract an attribute of the selected HTML element HtmlField : extract raw HTML code of the selected HTML element RegexField : use standard library re for better performance Note All the parameters of fields are keyword arguments .","title":"Overview"},{"location":"en/apis/field.html#elementfield","text":"ElementField first select an HTML element by CSS Selector or XPath Selector, then get the LXML element(s) from the selected element.","title":"ElementField"},{"location":"en/apis/field.html#parameters","text":"attr : str , required, the name of the attribute you want to extract css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True","title":"Parameters"},{"location":"en/apis/field.html#example","text":"import ruia from lxml import etree HTML = ''' <body> <div class=\"title\" href=\"/\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_element_field (): ul = ruia . ElementField ( css_select = \"ul\" ) assert len ( ul . extract ( html_etree = html ) . xpath ( '//li' )) == 3","title":"Example"},{"location":"en/apis/field.html#textfield","text":"TextField first select an HTML element by CSS Selector or XPath Selector, then get the text value of the selected element.","title":"TextField"},{"location":"en/apis/field.html#parameters_1","text":"css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True","title":"Parameters"},{"location":"en/apis/field.html#example_1","text":"import ruia from lxml import etree HTML = ''' <body> <div class=\"title\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_text_field (): title_field = ruia . TextField ( css_select = '.title' , default = 'Untitled' ) assert title_field . extract ( html_etree = html ) == 'Ruia Documentation' tag_field = ruia . TextField ( css_select = '.tag' , default = 'No tag' , many = True ) assert tag_field . extract ( html_etree = html ) == [ 'easy' , 'fast' , 'powerful' ]","title":"Example"},{"location":"en/apis/field.html#attrfield","text":"TextField first select an HTML element by CSS Selector or XPath Selector, then get the attribute value of the selected element.","title":"AttrField"},{"location":"en/apis/field.html#parameters_2","text":"attr : str , required, the name of the attribute you want to extract css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True","title":"Parameters"},{"location":"en/apis/field.html#example_2","text":"import ruia from lxml import etree HTML = ''' <body> <div class=\"title\" href=\"/\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_attr_field (): title = ruia . AttrField ( css_select = '.title' , attr = 'href' , default = 'Untitled' ) assert title . extract ( html_etree = html ) == '/' tags = ruia . AttrField ( css_select = '.tag' , attr = 'href' , default = 'No tag' , many = True ) assert tags . extract ( html_etree = html )[ 0 ] == './easy.html'","title":"Example"},{"location":"en/apis/field.html#htmlfield","text":"TextField first select an HTML element by CSS Selector or XPath Selector, then get the raw HTML code of the selected element. If there's some spaces or some text outside any HTML elements between this element and next element, then this part of text will also inside the return value. It's an unstable feature, perhaps in later versions the outside text will be remove by default.","title":"HtmlField"},{"location":"en/apis/field.html#parameters_3","text":"css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True","title":"Parameters"},{"location":"en/apis/field.html#example_3","text":"import ruia from lxml import etree HTML = ''' <body> <div class=\"title\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_html_field (): title = ruia . HtmlField ( css_select = '.title' , default = 'Untitled' ) assert title . extract ( html_etree = html ) == '<div class=\"title\" href=\"/\">Ruia Documentation</div> \\n ' tags = ruia . HtmlField ( css_select = '.tag' , default = 'No tag' , many = True ) assert tags . extract ( html_etree = html )[ 1 ] == '<li class=\"tag\" href=\"./fast.html\">fast</li> \\n '","title":"Example"},{"location":"en/apis/field.html#regexfield","text":"TextField do not parse html structure, it directly use python standard library re . If your spider meets performance limitation, try RegexField . However, ruia is based on asyncio , you will seldom meet performance limitation! RegexField has a complex behaviour: if no group: return the whole matched string if regex has a group: return the group value if regex has multiple groups: return a list a string if regex has named groups, no matter one or more: return a dict, whose key and value are both string if many=True , return a list of above values","title":"RegexField"},{"location":"en/apis/field.html#parameters_4","text":"re_select : str , required, match HTML element(s) with regular expression default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True","title":"Parameters"},{"location":"en/apis/field.html#example_4","text":"import ruia from lxml import etree HTML = ''' <body> <div class=\"title\" href=\"/\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_regex_field (): title = ruia . RegexField ( re_select = '<div class=\"title\" href=\"(.*?)\">(.*?)</div>' ) assert title . extract ( html = HTML )[ 0 ] == '/' assert title . extract ( html = HTML )[ 1 ] == 'Ruia Documentation' tags = ruia . RegexField ( re_select = '<li class=\"tag\" href=\"(?P<href>.*?)\">(?P<text>.*?)</li>' , many = True ) result = tags . extract ( html = HTML ) assert isinstance ( result , list ) assert len ( result ) == 3 assert isinstance ( result [ 0 ], dict ) assert result [ 0 ][ 'href' ] == './easy.html'","title":"Example"},{"location":"en/apis/field.html#about-parameter-many","text":"Parameter many=False indicates if the field will extract one value or multiple values from HTML source code. For example, one Github Issue has many tags, We can use Item.get_items to get multiple values of tags, but that means an extra class definition. Parameter many aims to solve this problem. A field is default by many=False , that means, for TextField , AttrField and HtmlField , Field.extract(*, **) will always return a string, and RegexField will return a string or a list or dict, depending on whether there are groups in the regular expression. We can consider it with a 'singular number'. With many=True , each field will return a 'plural', that is, return a list.","title":"About Parameter many"},{"location":"en/apis/item.html","text":"Item item is mainly used to define data model and extract data from HTML source code. It has the following two methods: get_item : extract one data from HTML source code get_items : extract many data from HTML source code Core arguments get_item and get_items receives same arguments: html: optional, HTML source code url: optional, HTML href link html_etree: optional, etree._Element object Usage From the arguments above, we can see that, there are three ways to feed Item object: from a web link, from HTML source code, or even from etree._Element object. If you are using get_items , the attr named target_item is expected. import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async def main (): async for item in HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ): print ( item . title , item . url ) if __name__ == '__main__' : items = asyncio . run ( main ()) Sometimes we may come across such a condition. When crawling github issues, we will find that there are several tags to a issue. Define TagItem as a standalone item is not that beautiful. It's time to focus on the many=True argument. Fields with many=True will return a list. from ruia import Item , TextField class GithiubIssueItem ( Item ): issue_id = TextField ( css_select = 'issue_id_class' ) title = TextField ( css_select = 'issue_title_class' ) tags = TextField ( css_select = 'tag_class' , many = True ) item = GithiubIssueItem . get_item ( html ) assert isinstance ( item . tags , list ) AttrField also has the argument many . How It Works? Inner, item class will change different kinds of inputs into etree._Element obejct, and then extract data. Meta class will help to get every property defined by Filed .","title":"Item"},{"location":"en/apis/item.html#item","text":"item is mainly used to define data model and extract data from HTML source code. It has the following two methods: get_item : extract one data from HTML source code get_items : extract many data from HTML source code","title":"Item"},{"location":"en/apis/item.html#core-arguments","text":"get_item and get_items receives same arguments: html: optional, HTML source code url: optional, HTML href link html_etree: optional, etree._Element object","title":"Core arguments"},{"location":"en/apis/item.html#usage","text":"From the arguments above, we can see that, there are three ways to feed Item object: from a web link, from HTML source code, or even from etree._Element object. If you are using get_items , the attr named target_item is expected. import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async def main (): async for item in HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ): print ( item . title , item . url ) if __name__ == '__main__' : items = asyncio . run ( main ()) Sometimes we may come across such a condition. When crawling github issues, we will find that there are several tags to a issue. Define TagItem as a standalone item is not that beautiful. It's time to focus on the many=True argument. Fields with many=True will return a list. from ruia import Item , TextField class GithiubIssueItem ( Item ): issue_id = TextField ( css_select = 'issue_id_class' ) title = TextField ( css_select = 'issue_title_class' ) tags = TextField ( css_select = 'tag_class' , many = True ) item = GithiubIssueItem . get_item ( html ) assert isinstance ( item . tags , list ) AttrField also has the argument many .","title":"Usage"},{"location":"en/apis/item.html#how-it-works","text":"Inner, item class will change different kinds of inputs into etree._Element obejct, and then extract data. Meta class will help to get every property defined by Filed .","title":"How It Works?"},{"location":"en/apis/middleware.html","text":"Middleware Middleware is mainly used to process before request and process after response, such as listening request and response. Middleware().request : do some operations before request; Middleware().response : do some operations after response. Usage Note: The function should receive a special argument; The function return nothing. The arguments are listed in the following example: from ruia import Middleware middleware = Middleware () @middleware . request async def print_on_request ( spider_ins , request ): \"\"\" This function will be called before every request. request: an object of Request \"\"\" print ( \"request: print when a request is received\" ) @middleware . response async def print_on_response ( spider_ins , request , response ): \"\"\" This function will be called after every request. request: an object of Request response: an object of Response \"\"\" print ( \"response: print when a response is received\" ) How It Works? Middleware used decorators to implement the callback function, aims at writting middlewares easier for developers. Middleware().request_middleware and Middleware().response_middleware are two queues, stands for the user-defined functions.","title":"Middleware"},{"location":"en/apis/middleware.html#middleware","text":"Middleware is mainly used to process before request and process after response, such as listening request and response. Middleware().request : do some operations before request; Middleware().response : do some operations after response.","title":"Middleware"},{"location":"en/apis/middleware.html#usage","text":"Note: The function should receive a special argument; The function return nothing. The arguments are listed in the following example: from ruia import Middleware middleware = Middleware () @middleware . request async def print_on_request ( spider_ins , request ): \"\"\" This function will be called before every request. request: an object of Request \"\"\" print ( \"request: print when a request is received\" ) @middleware . response async def print_on_response ( spider_ins , request , response ): \"\"\" This function will be called after every request. request: an object of Request response: an object of Response \"\"\" print ( \"response: print when a response is received\" )","title":"Usage"},{"location":"en/apis/middleware.html#how-it-works","text":"Middleware used decorators to implement the callback function, aims at writting middlewares easier for developers. Middleware().request_middleware and Middleware().response_middleware are two queues, stands for the user-defined functions.","title":"How It Works?"},{"location":"en/apis/request.html","text":"Request Request is used for operating web requests. It returns a Response object. Methods: Request().fetch : request a web resource, it can be used standalone Request().fetch_callback : it is a core method for Spider class Core arguments url: the resource link method: request method, shoud be GET or POST callback: callback function encoding: html encode headers: request headers metadata: some data that need pass to next request request_config: the configure of the request RETRIES : number of retries before failing (default: 3 ) DELAY : delay (seconds) between each request (default: 0 ) RETRY_DELAY : delay (seconds) between each retry (default: 0 ) TIMEOUT : time (seconds) to presist with request before failing/retrying (default: 10 ) RETRY_FUNC : function to call on retry VALID : function to call after retrieving data request_session: aiohttp.ClientSession aiohttp_kwargs: aiohttp arguments for request Usage From the arguments above, we can see that Request can be used both associated with Spider and standalone. import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}> How It Works? Request class will send asynchronous http request by packaging aiohttp and pyppeteer .","title":"Request"},{"location":"en/apis/request.html#request","text":"Request is used for operating web requests. It returns a Response object. Methods: Request().fetch : request a web resource, it can be used standalone Request().fetch_callback : it is a core method for Spider class","title":"Request"},{"location":"en/apis/request.html#core-arguments","text":"url: the resource link method: request method, shoud be GET or POST callback: callback function encoding: html encode headers: request headers metadata: some data that need pass to next request request_config: the configure of the request RETRIES : number of retries before failing (default: 3 ) DELAY : delay (seconds) between each request (default: 0 ) RETRY_DELAY : delay (seconds) between each retry (default: 0 ) TIMEOUT : time (seconds) to presist with request before failing/retrying (default: 10 ) RETRY_FUNC : function to call on retry VALID : function to call after retrieving data request_session: aiohttp.ClientSession aiohttp_kwargs: aiohttp arguments for request","title":"Core arguments"},{"location":"en/apis/request.html#usage","text":"From the arguments above, we can see that Request can be used both associated with Spider and standalone. import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}>","title":"Usage"},{"location":"en/apis/request.html#how-it-works","text":"Request class will send asynchronous http request by packaging aiohttp and pyppeteer .","title":"How It Works?"},{"location":"en/apis/response.html","text":"Response Response is used to return a uniformed and friendly response object. Main properties: url: the href of resource method: request method, shoud be GET or `POST encoding: html encode html: HTML source code from website metadata: some data that need pass to next request cookies: cookies of website history: the request history headers: response headers status: response status code aws_json: get json data from target url aws_read: get bytes data from target url aws_text: get text data from target url","title":"Response"},{"location":"en/apis/response.html#response","text":"Response is used to return a uniformed and friendly response object. Main properties: url: the href of resource method: request method, shoud be GET or `POST encoding: html encode html: HTML source code from website metadata: some data that need pass to next request cookies: cookies of website history: the request history headers: response headers status: response status code aws_json: get json data from target url aws_read: get bytes data from target url aws_text: get text data from target url","title":"Response"},{"location":"en/apis/spider.html","text":"Spider Spider is the entrypoint of the crawler program. It combines Item , Middleware , Request and other models, to build a strong crawler for you. You should focus on the following two functions: Spider.start : the entrypoint parse : The first parse function, required for subclass of Spider Core arguments Spider.start arguments: after_start: a hook after starting the crawler before_stop: a hook before starting the crawler middleware: Middleware class, can be an object of Middleware() , or a list of Middleware() loop: event loop Usage import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Controlling the max number of concurrency Define an attribute concurrency at the subclass of Spider . Here is an example: import ruia class MySpider ( ruia . Spider ): start_urls = [ 'https://news.ycombinator.com' ] concurrency = 3 async def parse ( self , res ): pass How It Works? Spider will read links in start_urls , and maintains a asynchronous queue. The queue is a producer consumer model, and the loop will run until no more request functions.","title":"Spider"},{"location":"en/apis/spider.html#spider","text":"Spider is the entrypoint of the crawler program. It combines Item , Middleware , Request and other models, to build a strong crawler for you. You should focus on the following two functions: Spider.start : the entrypoint parse : The first parse function, required for subclass of Spider","title":"Spider"},{"location":"en/apis/spider.html#core-arguments","text":"Spider.start arguments: after_start: a hook after starting the crawler before_stop: a hook before starting the crawler middleware: Middleware class, can be an object of Middleware() , or a list of Middleware() loop: event loop","title":"Core arguments"},{"location":"en/apis/spider.html#usage","text":"import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start ()","title":"Usage"},{"location":"en/apis/spider.html#controlling-the-max-number-of-concurrency","text":"Define an attribute concurrency at the subclass of Spider . Here is an example: import ruia class MySpider ( ruia . Spider ): start_urls = [ 'https://news.ycombinator.com' ] concurrency = 3 async def parse ( self , res ): pass","title":"Controlling the max number of concurrency"},{"location":"en/apis/spider.html#how-it-works","text":"Spider will read links in start_urls , and maintains a asynchronous queue. The queue is a producer consumer model, and the loop will run until no more request functions.","title":"How It Works?"},{"location":"en/examples/project.html","text":"Start a Full Featured Ruia Project This essay will talk about how to create a full featured Ruia Spider Project. Comming soon...","title":"Full featured project"},{"location":"en/examples/project.html#start-a-full-featured-ruia-project","text":"This essay will talk about how to create a full featured Ruia Spider Project.","title":"Start a Full Featured Ruia Project"},{"location":"en/examples/project.html#comming-soon","text":"","title":"Comming soon..."},{"location":"en/examples/spider_parent_class.html","text":"Create A Parent Class For Your Spiders A Simple Example Take following basic spiders: # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = \".sidebar-menu a\" ) title = TextField ( css_select = \"a\" ) link = AttrField ( css_select = \"a\" , attr = \"href\" ) async def clean_link ( self , value ): return f \"https://developer.github.com { value } \" class PageItem ( Item ): content = HtmlField ( css_select = \".content\" ) class GithubDeveloperSpider ( Spider ): start_urls = [ \"https://developer.github.com/v3/\" ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) urls = [ page . link for page in catalogue ][: 10 ] async for response in self . multiple_request ( urls , is_gather = True ): title = catalogue [ response . index ] . title yield self . parse_page ( response , title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) class GithubDeveloperSpiderSingleRequest ( Spider ): start_urls = [ \"https://developer.github.com/v3/\" ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) for page in catalogue : response = await self . request ( url = page . link ) yield self . parse_page ( response , page . title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) if __name__ == \"__main__\" : GithubDeveloperSpider . start () As you can see, there is quite a bit of common code in all the spiders - for example the start_urls and concurrency variables as they are standard in this project, and, of particular interest, the parse_page function. We'd like to refactor the commonalities into a parent class. To do this is very easy - we define the parent class with the common variables/functions contained: class GithubSpiderParent ( Spider ): start_urls = [ \"https://developer.github.com/v3/\" ] concurrency = 5 async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) And then we can refactor the two spiders: class GithubDeveloperSpider ( Spider ): async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) urls = [ page . link for page in catalogue ][: 10 ] async for response in self . multiple_request ( urls , is_gather = True ): title = catalogue [ response . index ] . title yield self . parse_page ( response , title ) class GithubDeveloperSpiderSingleRequest ( Spider ): async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) for page in catalogue : response = await self . request ( url = page . link ) yield self . parse_page ( response , page . title ) A More Complex Example Say you wanted to dynamically initialise objects for the Spider sub-classes, e.g. a unique database connection for each class in which the data the spiders retrieve is inserted into. There are a few ways to go about doing this, but the most concise is to use a parent class, as above, with a few modifications. First let us define the database connection strings in the spiders. This tutorial assumes that the dataset (https://dataset.readthedocs.io/en/latest/) library is pre-installed: class GithubDeveloperSpider ( GithubSpiderParent ): start_urls = [ \"https://developer.github.com/v3/\" ] concurrency = 5 database_url = \"sqlite:///developer.db\" async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) urls = [ page . link for page in catalogue ][: 10 ] async for response in self . multiple_request ( urls , is_gather = True ): title = catalogue [ response . index ] . title yield self . parse_page ( response , title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) # Insert the title as a row into the database self . db [ 'developers' ] . insert ( dict ( title = title )) class GithubDeveloperSpiderSingleRequest ( GithubSpiderParent ): start_urls = [ \"https://developer.github.com/v3/\" ] concurrency = 5 database_url = \"sqlite:///developer_single.db\" async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) for page in catalogue : response = await self . request ( url = page . link ) yield self . parse_page ( response , page . title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) # Insert the title as a row into the database self . db [ 'developer' ] . insert ( dict ( title = title )) Now you may assume that the parent class for creating a database connection at initialisation would be something like this: import dataset class GithubSpiderParent ( Spider ): def __init__ ( self , * args , ** kwargs ): super ( Spider , self ) . __init__ ( * args , ** kwargs ) self . db = dataset . connect ( self . database_url ) But this is incorrect. Why? Because of classmethods . It can be a slightly difficult concept to grasp - classmethods give you control of the class at a class level, not at an instance level (as self does) and therefore you must attach the database connection to the class instance. Thus to acheive the aforementioned goal the parent class needs to be so: import dataset class GithubSpiderParent ( Spider ): @classmethod def start ( cls ): cls . db = dataset . connect ( cls . database_url ) super () . start () And developer.db should have been created.","title":"Parent Class For Your Spider"},{"location":"en/examples/spider_parent_class.html#create-a-parent-class-for-your-spiders","text":"","title":"Create A Parent Class For Your Spiders"},{"location":"en/examples/spider_parent_class.html#a-simple-example","text":"Take following basic spiders: # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = \".sidebar-menu a\" ) title = TextField ( css_select = \"a\" ) link = AttrField ( css_select = \"a\" , attr = \"href\" ) async def clean_link ( self , value ): return f \"https://developer.github.com { value } \" class PageItem ( Item ): content = HtmlField ( css_select = \".content\" ) class GithubDeveloperSpider ( Spider ): start_urls = [ \"https://developer.github.com/v3/\" ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) urls = [ page . link for page in catalogue ][: 10 ] async for response in self . multiple_request ( urls , is_gather = True ): title = catalogue [ response . index ] . title yield self . parse_page ( response , title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) class GithubDeveloperSpiderSingleRequest ( Spider ): start_urls = [ \"https://developer.github.com/v3/\" ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) for page in catalogue : response = await self . request ( url = page . link ) yield self . parse_page ( response , page . title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) if __name__ == \"__main__\" : GithubDeveloperSpider . start () As you can see, there is quite a bit of common code in all the spiders - for example the start_urls and concurrency variables as they are standard in this project, and, of particular interest, the parse_page function. We'd like to refactor the commonalities into a parent class. To do this is very easy - we define the parent class with the common variables/functions contained: class GithubSpiderParent ( Spider ): start_urls = [ \"https://developer.github.com/v3/\" ] concurrency = 5 async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) And then we can refactor the two spiders: class GithubDeveloperSpider ( Spider ): async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) urls = [ page . link for page in catalogue ][: 10 ] async for response in self . multiple_request ( urls , is_gather = True ): title = catalogue [ response . index ] . title yield self . parse_page ( response , title ) class GithubDeveloperSpiderSingleRequest ( Spider ): async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) for page in catalogue : response = await self . request ( url = page . link ) yield self . parse_page ( response , page . title )","title":"A Simple Example"},{"location":"en/examples/spider_parent_class.html#a-more-complex-example","text":"Say you wanted to dynamically initialise objects for the Spider sub-classes, e.g. a unique database connection for each class in which the data the spiders retrieve is inserted into. There are a few ways to go about doing this, but the most concise is to use a parent class, as above, with a few modifications. First let us define the database connection strings in the spiders. This tutorial assumes that the dataset (https://dataset.readthedocs.io/en/latest/) library is pre-installed: class GithubDeveloperSpider ( GithubSpiderParent ): start_urls = [ \"https://developer.github.com/v3/\" ] concurrency = 5 database_url = \"sqlite:///developer.db\" async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) urls = [ page . link for page in catalogue ][: 10 ] async for response in self . multiple_request ( urls , is_gather = True ): title = catalogue [ response . index ] . title yield self . parse_page ( response , title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) # Insert the title as a row into the database self . db [ 'developers' ] . insert ( dict ( title = title )) class GithubDeveloperSpiderSingleRequest ( GithubSpiderParent ): start_urls = [ \"https://developer.github.com/v3/\" ] concurrency = 5 database_url = \"sqlite:///developer_single.db\" async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if \"#\" in cat . link : continue catalogue . append ( cat ) for page in catalogue : response = await self . request ( url = page . link ) yield self . parse_page ( response , page . title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) # Insert the title as a row into the database self . db [ 'developer' ] . insert ( dict ( title = title )) Now you may assume that the parent class for creating a database connection at initialisation would be something like this: import dataset class GithubSpiderParent ( Spider ): def __init__ ( self , * args , ** kwargs ): super ( Spider , self ) . __init__ ( * args , ** kwargs ) self . db = dataset . connect ( self . database_url ) But this is incorrect. Why? Because of classmethods . It can be a slightly difficult concept to grasp - classmethods give you control of the class at a class level, not at an instance level (as self does) and therefore you must attach the database connection to the class instance. Thus to acheive the aforementioned goal the parent class needs to be so: import dataset class GithubSpiderParent ( Spider ): @classmethod def start ( cls ): cls . db = dataset . connect ( cls . database_url ) super () . start () And developer.db should have been created.","title":"A More Complex Example"},{"location":"en/topics/item_data_cleaning.html","text":"Item Data Cleaning If you visit other's blog, you may find that the titles of articles are often such a format: Ruia is a great framework | Ruia's blog . Open the inspector of browser, you will find such an element: < title > Ruia is a great framework | Ruia's blog </ title > The title element contains two parts: the actual title of this article and the site name of the blog. Now we just want to get the actual title Ruia is a great framework . We can write a statement in parse method, like: from ruia import Item , TextField class MyItem ( Item ): title = TextField ( css_select = 'title' ) async def parse ( self , response ): title = MyItem . get_item ( await response . text ()) . title title = title . split ( ' | ' )[ 0 ] with open ( 'data.txt' , mode = 'a' ) as file : file . writelines ([ title ]) It works well. However, in ruia, we want to separate the two processes: Data acquisition, for parsing HTML and create structured data; Data processing, for data persistence or some other operations. By separating data acquisition and data processing, our code can be more readable. We provide a better way for data cleaning. from ruia import Item , TextField class MyItem ( Item ): title = TextField ( css_select = 'title' ) def clean_title ( self , value ): value = value . split ( ' | ' )[ 0 ] return value async def parse ( self , response ): title = MyItem . get_item ( await response . text ()) . title with open ( 'data.txt' , mode = 'a' ) as file : file . writelines ([ title ]) Now we get a better item. We just get the property title of item , like item.title , and we can get a pure title we want. ruia will automatically recognize methods starts with clean_ . If there's a field named the_field , then its corresponding data cleaning method is clean_the_field . Just add a prefix clean_ is okay. The default clean method of each field is just return the string itself. Before data cleaning, fields are all pure python strings (sometimes a list or a dict of pure python strings). If you want item.index to return a python integer, please define clean_index method to return int(value) . Now let's focus on such a HTML code. For some reason, perhaps for css layout, there are some empty items. We want 5 movies, while ruia get 7. Of course you can delete useless items in parse function, however, it violated our principle that we should separate get items and save items. <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > </ head > < body > < div class = \"container\" > < div class = \"movie\" >< a class = \"title\" > Movie 1 </ a >< span class = \"star\" > 3 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 2 </ a >< span class = \"star\" > 5 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 3 </ a >< span class = \"star\" > 2 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 4 </ a >< span class = \"star\" > 1 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 5 </ a >< span class = \"star\" > 5 </ span ></ div > < div class = \"movie\" >< a class = \"title\" ></ a >< span class = \"star\" ></ span ></ div > < div class = \"movie\" >< a class = \"title\" ></ a >< span class = \"star\" ></ span ></ div > </ div > </ body > </ html > Ruia use an Exception to solve this problem. In clean_* functions, we can raise a ruia.IgnoreThisItem to skip useless items. Here's a snippet as a demo. from ruia import Item , TextField , IgnoreThisItem class MyItem ( Item ): target_item = TextField ( css_select = '.movie' ) title = TextField ( css_select = \".title\" ) star = TextField ( css_select = \".star\" ) @staticmethod async def clean_title ( value ): if not value : raise IgnoreThisItem return value async def main (): items = list () async for item in MyItem . get_items ( html = HTML ): items . append ( item ) assert len ( items ) == 5 Now, the length of items is 5, instead of 7.","title":"Item Data Cleaning"},{"location":"en/topics/item_data_cleaning.html#item-data-cleaning","text":"If you visit other's blog, you may find that the titles of articles are often such a format: Ruia is a great framework | Ruia's blog . Open the inspector of browser, you will find such an element: < title > Ruia is a great framework | Ruia's blog </ title > The title element contains two parts: the actual title of this article and the site name of the blog. Now we just want to get the actual title Ruia is a great framework . We can write a statement in parse method, like: from ruia import Item , TextField class MyItem ( Item ): title = TextField ( css_select = 'title' ) async def parse ( self , response ): title = MyItem . get_item ( await response . text ()) . title title = title . split ( ' | ' )[ 0 ] with open ( 'data.txt' , mode = 'a' ) as file : file . writelines ([ title ]) It works well. However, in ruia, we want to separate the two processes: Data acquisition, for parsing HTML and create structured data; Data processing, for data persistence or some other operations. By separating data acquisition and data processing, our code can be more readable. We provide a better way for data cleaning. from ruia import Item , TextField class MyItem ( Item ): title = TextField ( css_select = 'title' ) def clean_title ( self , value ): value = value . split ( ' | ' )[ 0 ] return value async def parse ( self , response ): title = MyItem . get_item ( await response . text ()) . title with open ( 'data.txt' , mode = 'a' ) as file : file . writelines ([ title ]) Now we get a better item. We just get the property title of item , like item.title , and we can get a pure title we want. ruia will automatically recognize methods starts with clean_ . If there's a field named the_field , then its corresponding data cleaning method is clean_the_field . Just add a prefix clean_ is okay. The default clean method of each field is just return the string itself. Before data cleaning, fields are all pure python strings (sometimes a list or a dict of pure python strings). If you want item.index to return a python integer, please define clean_index method to return int(value) . Now let's focus on such a HTML code. For some reason, perhaps for css layout, there are some empty items. We want 5 movies, while ruia get 7. Of course you can delete useless items in parse function, however, it violated our principle that we should separate get items and save items. <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > </ head > < body > < div class = \"container\" > < div class = \"movie\" >< a class = \"title\" > Movie 1 </ a >< span class = \"star\" > 3 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 2 </ a >< span class = \"star\" > 5 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 3 </ a >< span class = \"star\" > 2 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 4 </ a >< span class = \"star\" > 1 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 5 </ a >< span class = \"star\" > 5 </ span ></ div > < div class = \"movie\" >< a class = \"title\" ></ a >< span class = \"star\" ></ span ></ div > < div class = \"movie\" >< a class = \"title\" ></ a >< span class = \"star\" ></ span ></ div > </ div > </ body > </ html > Ruia use an Exception to solve this problem. In clean_* functions, we can raise a ruia.IgnoreThisItem to skip useless items. Here's a snippet as a demo. from ruia import Item , TextField , IgnoreThisItem class MyItem ( Item ): target_item = TextField ( css_select = '.movie' ) title = TextField ( css_select = \".title\" ) star = TextField ( css_select = \".star\" ) @staticmethod async def clean_title ( value ): if not value : raise IgnoreThisItem return value async def main (): items = list () async for item in MyItem . get_items ( html = HTML ): items . append ( item ) assert len ( items ) == 5 Now, the length of items is 5, instead of 7.","title":"Item Data Cleaning"},{"location":"en/topics/write_spiders_like_scrapy.html","text":"Write Spiders like Scrapy I am a user of scrapy myself. Scrapy is a great framework, which is a de facto standard of python crawlers for years. Ruia provides APIs like scrapy, for users to migrate crawlers from scrapy to ruia. If you like the Declarative Programming feature, but you know little about python async/await syntax, this essay is for you. For this example, we'll crawl Github Developer Documentation . # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com { value } ' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): catalogue . append ( cat ) for page in catalogue [: 6 ]: if '#' in page . link : continue yield Request ( url = page . link , metadata = { 'title' : page . title }, callback = self . parse_page ) async def parse_page ( self , response : Response ): item = await PageItem . get_item ( html = await response . text ()) title = response . metadata [ 'title' ] print ( title , len ( item . content )) See the GithubDeveloperSpider.parse method. After extracting titles and urls, it yield a request. About yield , you should learn from python documentation. Here we can regard it as sending a task to background process. Okay, now that we have already send the request to background process, we have loss the control of the request. Nothing serious, after the request finished, the response will send to its callback parameter. callback parameter should be a function, or something callable. In parse_page method, we accept the response. Then it comes with another problem: we have already get the page title from catalogue in method parse , but they are not in the context of parse_page . That's why we need a metadata argument. we put data into metadata in the previous method, and get data from it in the following method. Now, run the spider. output: Media Types 8652 Overview 38490 OAuth Authorizations API 66565 Other Authentication Methods 6651 Troubleshooting 2551 BTW, we sincerely recommend you to migrate your code to new APIs of ruia . ruia provides a better way to replace callback functions with coroutines. It provides more readability and is more flexible. We do not need callback and metadata now. Crawlers are more concise.","title":"Write Spiders like Scrapy"},{"location":"en/topics/write_spiders_like_scrapy.html#write-spiders-like-scrapy","text":"I am a user of scrapy myself. Scrapy is a great framework, which is a de facto standard of python crawlers for years. Ruia provides APIs like scrapy, for users to migrate crawlers from scrapy to ruia. If you like the Declarative Programming feature, but you know little about python async/await syntax, this essay is for you. For this example, we'll crawl Github Developer Documentation . # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com { value } ' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): catalogue . append ( cat ) for page in catalogue [: 6 ]: if '#' in page . link : continue yield Request ( url = page . link , metadata = { 'title' : page . title }, callback = self . parse_page ) async def parse_page ( self , response : Response ): item = await PageItem . get_item ( html = await response . text ()) title = response . metadata [ 'title' ] print ( title , len ( item . content )) See the GithubDeveloperSpider.parse method. After extracting titles and urls, it yield a request. About yield , you should learn from python documentation. Here we can regard it as sending a task to background process. Okay, now that we have already send the request to background process, we have loss the control of the request. Nothing serious, after the request finished, the response will send to its callback parameter. callback parameter should be a function, or something callable. In parse_page method, we accept the response. Then it comes with another problem: we have already get the page title from catalogue in method parse , but they are not in the context of parse_page . That's why we need a metadata argument. we put data into metadata in the previous method, and get data from it in the following method. Now, run the spider. output: Media Types 8652 Overview 38490 OAuth Authorizations API 66565 Other Authentication Methods 6651 Troubleshooting 2551 BTW, we sincerely recommend you to migrate your code to new APIs of ruia . ruia provides a better way to replace callback functions with coroutines. It provides more readability and is more flexible. We do not need callback and metadata now. Crawlers are more concise.","title":"Write Spiders like Scrapy"},{"location":"en/tutorials/installation.html","text":"Installation Ruia is based on Python 3.6+. Check your python version before continue. python --version Installation # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # Install from github for new features pip install git+https://github.com/howie6879/ruia","title":"2. Installation"},{"location":"en/tutorials/installation.html#installation","text":"Ruia is based on Python 3.6+. Check your python version before continue. python --version","title":"Installation"},{"location":"en/tutorials/installation.html#installation_1","text":"# For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # Install from github for new features pip install git+https://github.com/howie6879/ruia","title":"Installation"},{"location":"en/tutorials/item.html","text":"Define Data Item Item and Fields First let's talk about what is Item . Item is an important concept in ruia . It defines what you want to get from HTML document. ruia.Item is used to get data from HTML document and save structured data. Define Item Here's an example of a simple Item. from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) Now let's reconstruct this Item. Analyze HTML document Supposing that we want to get the current python documentation version and its tutorial page link. We read the HTML source at https://docs.python.org/3/ . We find such two elements: < title > 3.7.2 Documentation </ title > < a class = \"biglink\" href = \"tutorial/index.html\" > Tutorial </ a > What we want to do: * navigate to the element; * extract data from element. Navigate to an element Ruia use selectors to navigate to the HTML element. As a crawler engineer, ruia believes that you have a full knowledge of at least one of CSS Selector and XPath Selector. For title element, because of his uniqueness, a simple CSS Selector is enough: css_select = 'title' For the Tutorial element, we have to use a XPath Selector to address it by it's text. xpath_select = \"//a[text()='Tutorial']\" Extract string from HTML element Now we have navigated to HTML elements. Time to extract string from it. Ruia use Field to extract data from HTML elements. For the title element, we want its text property. TextField is quite suitable for this purpose. from ruia import TextField title = TextField ( css_select = 'title' ) For the Tutorial element, we want its href property. AttrField is useful now: from ruia import AttrField tutorial_href = AttrField ( xpath_select = \"//a[text()='Tutorial'\" , attr = 'href' ) Combine fields to a item We have already told ruia how to find and extract data from HTML document. It's high time to combine them together as a structured data. from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) We inherit a new Item named HackerNewsItem from ruia.Item . Now, feed a HTML document to PythonDocumentationItem , it will extract the title and tutorial_link for us. Test this item We just defined an item. But will it perform just as what we want? Let's have a simple test. Ruia.Item has a convenient API. It's normal that is it can extract data from HTML document as a string. The magic is that it is also able to extract data from the given URL. import asyncio from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) async def main (): url = 'https://docs.python.org/3/' item = await PythonDocumentationItem . get_item ( url = url ) print ( item . title ) print ( item . tutorial_link ) if __name__ == '__main__' : # Python 3.7 required asyncio . run ( main ()) # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) # Output: # [2019:01:21 18:19:02]-Request-INFO request: <GET: https://docs.python.org/3/> # 3.7.2 Documentation # tutorial/index.html We hope you have already know python asyncio library, and know its basic usage. If not, remember the following tips: Functions defined with async keyword are now named coroutine; Define coroutine with async keyword; Call coroutine with await keyword; Start coroutine use asyncio.run function like the example. Now focus on the screen output. The first line is the log of Ruia , and the following two lines are the data we want. Okay, we have already finished the construction of our first Item. Get Many Items from One Page Here's a HTML document. It's simple and readable. <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > </ head > < body > < div class = \"container\" > < div class = \"movie\" >< a class = \"title\" > Movie 1 </ a >< span class = \"star\" > 3 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 2 </ a >< span class = \"star\" > 5 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 3 </ a >< span class = \"star\" > 2 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 4 </ a >< span class = \"star\" > 1 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 5 </ a >< span class = \"star\" > 5 </ span ></ div > </ div > </ body > </ html > It's a catalogue of movies. We want to get the name of movies and their stars. After analyzing document structure, we find that each movie is in a class div.movie . Then we can navigate to the container element by a CSS Selector: css_select='div.movie' . Then we can get our fields as before. ruia.Item has a convenient way to finish this task. If you define a target_item field to an Item, then it stands for the container. Here's an example. import asyncio from ruia import Item , AttrField , TextField HTML = \"\"\" <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Title</title> </head> <body> <div class=\"container\"> <div class=\"movie\"><a class=\"title\">Movie 1</a><span class=\"star\">3</span></div> <div class=\"movie\"><a class=\"title\">Movie 2</a><span class=\"star\">5</span></div> <div class=\"movie\"><a class=\"title\">Movie 3</a><span class=\"star\">2</span></div> <div class=\"movie\"><a class=\"title\">Movie 4</a><span class=\"star\">1</span></div> <div class=\"movie\"><a class=\"title\">Movie 5</a><span class=\"star\">5</span></div> </div> </body> </html> \"\"\" class MyItem ( Item ): target_item = TextField ( css_select = '.movie' ) title = TextField ( css_select = '.title' ) star = TextField ( css_select = '.star' ) async def clean_star ( self , value ): return int ( value ) async def main (): async for item in MyItem . get_items ( html = HTML ): print ( f 'Title= { item . title } , Star= { item . star } ' ) if __name__ == '__main__' : asyncio . run ( main ()) # Python 3.7 required # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) Previously, we call Item.get_item(html=HTML) to get an item. Here, we call Item.get_items(html=HTML) to get a list of items. The data cleaning methods still process a string, it has no difference. Output: Title=Movie 1, Star=3 Title=Movie 2, Star=5 Title=Movie 3, Star=2 Title=Movie 4, Star=1 Title=Movie 5, Star=5 Get Many Value by One Field Here is another example. Consider that we are now at a movie detail page. The HTML document shows like: <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > </ head > < body > < div class = \"movie\" > < div class = \"title\" > Movie Title </ div > < div class = \"star\" > 5 </ div > < div class = \"tags\" > < div class = \"tag\" > Comedy </ div > < div class = \"tag\" > 2019 </ div > < div class = \"tag\" > China </ div > </ div > </ div > </ body > </ html > We want to get the title, star, and tags of this movie. Previously, we only pure strings from Field . Here we want to get a list a pure strings from tag field. ruia.Item provides an easy way, that is the many=True parameter of Fields . Here is the implementation: import asyncio from ruia import Item , TextField HTML = \"\"\" <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Title</title> </head> <body> <div class=\"movie\"> <div class=\"title\">Movie Title</div> <div class=\"star\">5</div> <div class=\"tags\"> <div class=\"tag\">Comedy</div> <div class=\"tag\">2019</div> <div class=\"tag\">China</div> </div> </div> </body> </html> \"\"\" class MyItem ( Item ): title = TextField ( css_select = '.title' ) star = TextField ( css_select = '.star' ) tags = TextField ( css_select = '.tag' , many = True ) async def clean_star ( self , value ): return int ( value ) async def main (): item = await MyItem . get_item ( html = HTML ) print ( 'Title: ' , item . title ) print ( 'Star: ' , item . star ) for tag in item . tags : print ( 'Tag: ' , tag ) if __name__ == '__main__' : asyncio . run ( main ()) # Python 3.7 required # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) As we can see, tags field return a list of tags. All fields have this parameter. More Fields Ruia supports more fields than TextField and AttrField . However, they are the two fields that mostly used. Ruia also supports RegexField for extract data from HTML document directly by regular expression. It is only used for performance limitation, however, because of ruia's fast, we seldom meet performance limitation. There is a HtmlField to extract pure HTML source of a HTML element. Finally there is a ElementField to extract raw LXML element(s). Read Field API to get more information. Further Read the following essays for further learning. Data Cleaning","title":"3. Define Data Items"},{"location":"en/tutorials/item.html#define-data-item","text":"","title":"Define Data Item"},{"location":"en/tutorials/item.html#item-and-fields","text":"First let's talk about what is Item . Item is an important concept in ruia . It defines what you want to get from HTML document. ruia.Item is used to get data from HTML document and save structured data.","title":"Item and Fields"},{"location":"en/tutorials/item.html#define-item","text":"Here's an example of a simple Item. from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) Now let's reconstruct this Item.","title":"Define Item"},{"location":"en/tutorials/item.html#analyze-html-document","text":"Supposing that we want to get the current python documentation version and its tutorial page link. We read the HTML source at https://docs.python.org/3/ . We find such two elements: < title > 3.7.2 Documentation </ title > < a class = \"biglink\" href = \"tutorial/index.html\" > Tutorial </ a > What we want to do: * navigate to the element; * extract data from element.","title":"Analyze HTML document"},{"location":"en/tutorials/item.html#navigate-to-an-element","text":"Ruia use selectors to navigate to the HTML element. As a crawler engineer, ruia believes that you have a full knowledge of at least one of CSS Selector and XPath Selector. For title element, because of his uniqueness, a simple CSS Selector is enough: css_select = 'title' For the Tutorial element, we have to use a XPath Selector to address it by it's text. xpath_select = \"//a[text()='Tutorial']\"","title":"Navigate to an element"},{"location":"en/tutorials/item.html#extract-string-from-html-element","text":"Now we have navigated to HTML elements. Time to extract string from it. Ruia use Field to extract data from HTML elements. For the title element, we want its text property. TextField is quite suitable for this purpose. from ruia import TextField title = TextField ( css_select = 'title' ) For the Tutorial element, we want its href property. AttrField is useful now: from ruia import AttrField tutorial_href = AttrField ( xpath_select = \"//a[text()='Tutorial'\" , attr = 'href' )","title":"Extract string from HTML element"},{"location":"en/tutorials/item.html#combine-fields-to-a-item","text":"We have already told ruia how to find and extract data from HTML document. It's high time to combine them together as a structured data. from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) We inherit a new Item named HackerNewsItem from ruia.Item . Now, feed a HTML document to PythonDocumentationItem , it will extract the title and tutorial_link for us.","title":"Combine fields to a item"},{"location":"en/tutorials/item.html#test-this-item","text":"We just defined an item. But will it perform just as what we want? Let's have a simple test. Ruia.Item has a convenient API. It's normal that is it can extract data from HTML document as a string. The magic is that it is also able to extract data from the given URL. import asyncio from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) async def main (): url = 'https://docs.python.org/3/' item = await PythonDocumentationItem . get_item ( url = url ) print ( item . title ) print ( item . tutorial_link ) if __name__ == '__main__' : # Python 3.7 required asyncio . run ( main ()) # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) # Output: # [2019:01:21 18:19:02]-Request-INFO request: <GET: https://docs.python.org/3/> # 3.7.2 Documentation # tutorial/index.html We hope you have already know python asyncio library, and know its basic usage. If not, remember the following tips: Functions defined with async keyword are now named coroutine; Define coroutine with async keyword; Call coroutine with await keyword; Start coroutine use asyncio.run function like the example. Now focus on the screen output. The first line is the log of Ruia , and the following two lines are the data we want. Okay, we have already finished the construction of our first Item.","title":"Test this item"},{"location":"en/tutorials/item.html#get-many-items-from-one-page","text":"Here's a HTML document. It's simple and readable. <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > </ head > < body > < div class = \"container\" > < div class = \"movie\" >< a class = \"title\" > Movie 1 </ a >< span class = \"star\" > 3 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 2 </ a >< span class = \"star\" > 5 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 3 </ a >< span class = \"star\" > 2 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 4 </ a >< span class = \"star\" > 1 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 5 </ a >< span class = \"star\" > 5 </ span ></ div > </ div > </ body > </ html > It's a catalogue of movies. We want to get the name of movies and their stars. After analyzing document structure, we find that each movie is in a class div.movie . Then we can navigate to the container element by a CSS Selector: css_select='div.movie' . Then we can get our fields as before. ruia.Item has a convenient way to finish this task. If you define a target_item field to an Item, then it stands for the container. Here's an example. import asyncio from ruia import Item , AttrField , TextField HTML = \"\"\" <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Title</title> </head> <body> <div class=\"container\"> <div class=\"movie\"><a class=\"title\">Movie 1</a><span class=\"star\">3</span></div> <div class=\"movie\"><a class=\"title\">Movie 2</a><span class=\"star\">5</span></div> <div class=\"movie\"><a class=\"title\">Movie 3</a><span class=\"star\">2</span></div> <div class=\"movie\"><a class=\"title\">Movie 4</a><span class=\"star\">1</span></div> <div class=\"movie\"><a class=\"title\">Movie 5</a><span class=\"star\">5</span></div> </div> </body> </html> \"\"\" class MyItem ( Item ): target_item = TextField ( css_select = '.movie' ) title = TextField ( css_select = '.title' ) star = TextField ( css_select = '.star' ) async def clean_star ( self , value ): return int ( value ) async def main (): async for item in MyItem . get_items ( html = HTML ): print ( f 'Title= { item . title } , Star= { item . star } ' ) if __name__ == '__main__' : asyncio . run ( main ()) # Python 3.7 required # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) Previously, we call Item.get_item(html=HTML) to get an item. Here, we call Item.get_items(html=HTML) to get a list of items. The data cleaning methods still process a string, it has no difference. Output: Title=Movie 1, Star=3 Title=Movie 2, Star=5 Title=Movie 3, Star=2 Title=Movie 4, Star=1 Title=Movie 5, Star=5","title":"Get Many Items from One Page"},{"location":"en/tutorials/item.html#get-many-value-by-one-field","text":"Here is another example. Consider that we are now at a movie detail page. The HTML document shows like: <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > </ head > < body > < div class = \"movie\" > < div class = \"title\" > Movie Title </ div > < div class = \"star\" > 5 </ div > < div class = \"tags\" > < div class = \"tag\" > Comedy </ div > < div class = \"tag\" > 2019 </ div > < div class = \"tag\" > China </ div > </ div > </ div > </ body > </ html > We want to get the title, star, and tags of this movie. Previously, we only pure strings from Field . Here we want to get a list a pure strings from tag field. ruia.Item provides an easy way, that is the many=True parameter of Fields . Here is the implementation: import asyncio from ruia import Item , TextField HTML = \"\"\" <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Title</title> </head> <body> <div class=\"movie\"> <div class=\"title\">Movie Title</div> <div class=\"star\">5</div> <div class=\"tags\"> <div class=\"tag\">Comedy</div> <div class=\"tag\">2019</div> <div class=\"tag\">China</div> </div> </div> </body> </html> \"\"\" class MyItem ( Item ): title = TextField ( css_select = '.title' ) star = TextField ( css_select = '.star' ) tags = TextField ( css_select = '.tag' , many = True ) async def clean_star ( self , value ): return int ( value ) async def main (): item = await MyItem . get_item ( html = HTML ) print ( 'Title: ' , item . title ) print ( 'Star: ' , item . star ) for tag in item . tags : print ( 'Tag: ' , tag ) if __name__ == '__main__' : asyncio . run ( main ()) # Python 3.7 required # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) As we can see, tags field return a list of tags. All fields have this parameter.","title":"Get Many Value by One Field"},{"location":"en/tutorials/item.html#more-fields","text":"Ruia supports more fields than TextField and AttrField . However, they are the two fields that mostly used. Ruia also supports RegexField for extract data from HTML document directly by regular expression. It is only used for performance limitation, however, because of ruia's fast, we seldom meet performance limitation. There is a HtmlField to extract pure HTML source of a HTML element. Finally there is a ElementField to extract raw LXML element(s). Read Field API to get more information.","title":"More Fields"},{"location":"en/tutorials/item.html#further","text":"Read the following essays for further learning. Data Cleaning","title":"Further"},{"location":"en/tutorials/middleware.html","text":"Customize Middleware Middleware is mainly used to process before request and process after response, such as listening request and response. Here is an example: from ruia import Spider , Middleware middleware = Middleware () @middleware . request async def print_on_request ( spider_ins , request ): request . metadata = { 'url' : request . url } print ( f \"request: { request . metadata } \" ) # Just operate request object, and do not return anything. @middleware . response async def print_on_response ( spider_ins , request , response ): print ( f \"response: { response . metadata } \" ) class MiddlewareSpiderDemo ( Spider ): start_urls = [ 'https://httpbin.org/get' ] concurrency = 10 async def parse ( self , response ): pages = [ f 'https://httpbin.org/get?p= { i } ' for i in range ( 1 , 2 )] async for resp in self . multiple_request ( urls = pages ): print ( resp . url ) if __name__ == '__main__' : MiddlewareSpiderDemo . start ( middleware = middleware ) If successful, your terminal will have the following output: [ 2019 : 03 : 05 15 : 20 : 03 ] INFO Spider Spider started ! [ 2019 : 03 : 05 15 : 20 : 03 ] INFO Spider Worker started : 4396957904 [ 2019 : 03 : 05 15 : 20 : 03 ] INFO Spider Worker started : 4396958040 [ 2019 : 03 : 05 15 : 20 : 03 ] INFO Request < GET : https : // httpbin . org / get > request : { 'url' : 'https://httpbin.org/get' } request : { 'url' : 'https://httpbin.org/get?p=1' } [ 2019 : 03 : 05 15 : 20 : 05 ] INFO Request < GET : https : // httpbin . org / get ? p = 1 > [ 2019 : 03 : 05 15 : 20 : 06 ] INFO Spider Stopping spider : Ruia [ 2019 : 03 : 05 15 : 20 : 06 ] INFO Spider Total requests : 2 [ 2019 : 03 : 05 15 : 20 : 06 ] INFO Spider Time usage : 0 : 00 : 02.531665 [ 2019 : 03 : 05 15 : 20 : 06 ] INFO Spider Spider finished ! response : { 'url' : 'https://httpbin.org/get?p=1' } https : // httpbin . org / get ? p = 1 response : { 'url' : 'https://httpbin.org/get' } For full usage of Middleware, see Middleware API","title":"6. Customize Middleware"},{"location":"en/tutorials/middleware.html#customize-middleware","text":"Middleware is mainly used to process before request and process after response, such as listening request and response. Here is an example: from ruia import Spider , Middleware middleware = Middleware () @middleware . request async def print_on_request ( spider_ins , request ): request . metadata = { 'url' : request . url } print ( f \"request: { request . metadata } \" ) # Just operate request object, and do not return anything. @middleware . response async def print_on_response ( spider_ins , request , response ): print ( f \"response: { response . metadata } \" ) class MiddlewareSpiderDemo ( Spider ): start_urls = [ 'https://httpbin.org/get' ] concurrency = 10 async def parse ( self , response ): pages = [ f 'https://httpbin.org/get?p= { i } ' for i in range ( 1 , 2 )] async for resp in self . multiple_request ( urls = pages ): print ( resp . url ) if __name__ == '__main__' : MiddlewareSpiderDemo . start ( middleware = middleware ) If successful, your terminal will have the following output: [ 2019 : 03 : 05 15 : 20 : 03 ] INFO Spider Spider started ! [ 2019 : 03 : 05 15 : 20 : 03 ] INFO Spider Worker started : 4396957904 [ 2019 : 03 : 05 15 : 20 : 03 ] INFO Spider Worker started : 4396958040 [ 2019 : 03 : 05 15 : 20 : 03 ] INFO Request < GET : https : // httpbin . org / get > request : { 'url' : 'https://httpbin.org/get' } request : { 'url' : 'https://httpbin.org/get?p=1' } [ 2019 : 03 : 05 15 : 20 : 05 ] INFO Request < GET : https : // httpbin . org / get ? p = 1 > [ 2019 : 03 : 05 15 : 20 : 06 ] INFO Spider Stopping spider : Ruia [ 2019 : 03 : 05 15 : 20 : 06 ] INFO Spider Total requests : 2 [ 2019 : 03 : 05 15 : 20 : 06 ] INFO Spider Time usage : 0 : 00 : 02.531665 [ 2019 : 03 : 05 15 : 20 : 06 ] INFO Spider Spider finished ! response : { 'url' : 'https://httpbin.org/get?p=1' } https : // httpbin . org / get ? p = 1 response : { 'url' : 'https://httpbin.org/get' } For full usage of Middleware, see Middleware API","title":"Customize Middleware"},{"location":"en/tutorials/overview.html","text":"An Overview of Ruia Ruia is An asynchronous web scraping micro-framework, powered by asyncio and aiohttp , aims at making crawling url as convenient as possible. Write less, run faster is Ruia's philosophy. Ruia spider consists the following four parts: Ruia Part Is Required Description Data Items Required a collection of fields Spider Recommended a manager to make your spider stronger Middleware Optional used for processing request and response Plugin Optional used to enhance ruia functions","title":"1. Overview"},{"location":"en/tutorials/overview.html#an-overview-of-ruia","text":"Ruia is An asynchronous web scraping micro-framework, powered by asyncio and aiohttp , aims at making crawling url as convenient as possible. Write less, run faster is Ruia's philosophy. Ruia spider consists the following four parts: Ruia Part Is Required Description Data Items Required a collection of fields Spider Recommended a manager to make your spider stronger Middleware Optional used for processing request and response Plugin Optional used to enhance ruia functions","title":"An Overview of Ruia"},{"location":"en/tutorials/plugins.html","text":"How to Write a Plugins Plugins are used to package some common functions as a third-party model. Ruia allow developers to implement third-party extensions in the following ways: by using Middleware class by overwriting some core modules(just like Spider, Request etc...) In the previous section, we talked about Middleware . It is used to process before request and after response. Then, we implemeneted a function, that is to add User-Agent in request headers. Perhaps any crawler need such a function, to add User-Agent randomly, so, let's packaging this function as a third-party extension. Do it! Creating a project The project name is ruia-ua . Ruia is based on Python3.6+ , so is ruia-ua . Supposing that you're now in Python 3.6+ . # Install package management tool: pipenv pip install pipenv # Create project directory mkdir ruia-ua cd ruia-ua # Install virtual environment pipenv install # Install ruia pipenv install ruia # Install aiofiles pipenv install aiofiles # Create project directory in the project directory mkdir ruia_ua cd ruia_ua # Here's your implementation touch __init__.py Directory structure: ruia-ua \u251c\u2500\u2500 LICENSE # Open source license \u251c\u2500\u2500 Pipfile # pipenv management tools \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # Main code of your plugin \u2502 \u2514\u2500\u2500 user_agents.txt # some random user_agents \u2514\u2500\u2500 setup.py First plugin user_agents.txt contains all kinds of UA , then we only need to use Middleware of ruia to add a random User-Agent before every request. Here is one implementation: import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware . request async def add_random_ua ( spider_ins , request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } Now it's high time to upload ruia-ua to community, then all other ruia users are able to use your third-party extension. Sounds great! Usage All crawlers can use ruia-ua to add User-Agent automatically. pip install ruia - ua Here is an example: from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware as ua_middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = ua_middleware ) The implementations of third-party plugins will make developing crawlers easier! Ruia do want your developing and uploading your own third-party plugins!","title":"7. Write a Plugins"},{"location":"en/tutorials/plugins.html#how-to-write-a-plugins","text":"Plugins are used to package some common functions as a third-party model. Ruia allow developers to implement third-party extensions in the following ways: by using Middleware class by overwriting some core modules(just like Spider, Request etc...) In the previous section, we talked about Middleware . It is used to process before request and after response. Then, we implemeneted a function, that is to add User-Agent in request headers. Perhaps any crawler need such a function, to add User-Agent randomly, so, let's packaging this function as a third-party extension. Do it!","title":"How to Write a Plugins"},{"location":"en/tutorials/plugins.html#creating-a-project","text":"The project name is ruia-ua . Ruia is based on Python3.6+ , so is ruia-ua . Supposing that you're now in Python 3.6+ . # Install package management tool: pipenv pip install pipenv # Create project directory mkdir ruia-ua cd ruia-ua # Install virtual environment pipenv install # Install ruia pipenv install ruia # Install aiofiles pipenv install aiofiles # Create project directory in the project directory mkdir ruia_ua cd ruia_ua # Here's your implementation touch __init__.py Directory structure: ruia-ua \u251c\u2500\u2500 LICENSE # Open source license \u251c\u2500\u2500 Pipfile # pipenv management tools \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # Main code of your plugin \u2502 \u2514\u2500\u2500 user_agents.txt # some random user_agents \u2514\u2500\u2500 setup.py","title":"Creating a project"},{"location":"en/tutorials/plugins.html#first-plugin","text":"user_agents.txt contains all kinds of UA , then we only need to use Middleware of ruia to add a random User-Agent before every request. Here is one implementation: import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware . request async def add_random_ua ( spider_ins , request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } Now it's high time to upload ruia-ua to community, then all other ruia users are able to use your third-party extension. Sounds great!","title":"First plugin"},{"location":"en/tutorials/plugins.html#usage","text":"All crawlers can use ruia-ua to add User-Agent automatically. pip install ruia - ua Here is an example: from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware as ua_middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = ua_middleware ) The implementations of third-party plugins will make developing crawlers easier! Ruia do want your developing and uploading your own third-party plugins!","title":"Usage"},{"location":"en/tutorials/request.html","text":"Request & Response Ruia provides friendly and convenient Request and Response APis. Here is an example: import asyncio from ruia import Request async def request_example (): url = 'http://httpbin.org/get' params = { 'name' : 'ruia' , } headers = { 'User-Agent' : 'Python3.6' , } request = Request ( url = url , method = 'GET' , params = params , headers = headers ) response = await request . fetch () json_result = await response . json () assert json_result [ 'args' ][ 'name' ] == 'ruia' assert json_result [ 'headers' ][ 'User-Agent' ] == 'Python3.6' if __name__ == '__main__' : asyncio . get_event_loop () . run_until_complete ( request_example ()) We define a request by class Request . Then, we call await request.fetch() to get it's response. Note ruia.Request provides asynchronous methods, be sure to use it in an asynchronous function with async statement, and get it's respones with await statement. For full usage of response and request, see Request API and Response API","title":"5. Request & Response"},{"location":"en/tutorials/request.html#request-response","text":"Ruia provides friendly and convenient Request and Response APis. Here is an example: import asyncio from ruia import Request async def request_example (): url = 'http://httpbin.org/get' params = { 'name' : 'ruia' , } headers = { 'User-Agent' : 'Python3.6' , } request = Request ( url = url , method = 'GET' , params = params , headers = headers ) response = await request . fetch () json_result = await response . json () assert json_result [ 'args' ][ 'name' ] == 'ruia' assert json_result [ 'headers' ][ 'User-Agent' ] == 'Python3.6' if __name__ == '__main__' : asyncio . get_event_loop () . run_until_complete ( request_example ()) We define a request by class Request . Then, we call await request.fetch() to get it's response. Note ruia.Request provides asynchronous methods, be sure to use it in an asynchronous function with async statement, and get it's respones with await statement. For full usage of response and request, see Request API and Response API","title":"Request &amp; Response"},{"location":"en/tutorials/spider.html","text":"Spider Control ruia.Spider is used to control the whole spider, it provides the following functions: Normalize your code Maintain a event loop Manage requests and responses Concurrency control Manage middlewares and plugins Although it works well, to use only ruia.Item to create a spider, ruia recommend to use ruia.Spider to implement a stronger spider. Normalize your code ruia.Spider requires a class property start_urls as the entry point of a spider. Inner, ruia will iterate start_urls , and send a request to server for each request. After receiving server response, ruia will call spider.parse(response) , and this is the main part of your spider. Here's a simple parse example, to simply save response fields to a text file. We only have to define start_urls , and implement a parse method. import aiofiles from ruia import Spider , Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p= { index } ' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) aiofiles is a third-party library to operate files in asynchronous way. It provides APIs the same as python standard open function. Now, we have written a spider, and time to start crawling. import aiofiles from ruia import Spider , Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p= { index } ' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Done. Now your code is more readable and maintainable. Send Further Requests I don't think that just crawling the catalogue of news satisfied you. Next we will crawl news itself. Hacker news gathers news from many websites, it's not easy to parse each article of it. For this example, we'll crawl Github Developer Documentation . If you are a user of scrapy , perhaps you'd like this essay for migration: Write Spiders like Scrapy . Ruia provides a better way to send further requests by new asynchronous syntax async/await. It provides more readability and is more flexible. In any parse method, just yield a coroutine, and the coroutine will be processed by ruia . Here is a simple pseudo-code. from ruia import Spider class MySpider ( Spider ): async def parse ( self , response ): next_response = await self . request ( f ' { response . url } /next' ) yield self . parse_next_page ( next_response , metadata = 'nothing' ) async def parse_next_page ( self , response , metadata ): print ( await response . text ()) It works well, except you want to yield many coroutines in a for loop. Look at the following pseudo-code: from ruia import Spider class MySpider ( Spider ): async def parse ( self , response ): for i in range ( 10 ): response = await self . request ( f 'https://some.site/ { i } ' ) yield self . parse_next ( response ) async def parse_next ( self , response ): print ( await response . text ()) You will find the requests in the for loop runs in a synchronous way! Oh, awful. To solve this problem, ruia provides a multiple_request method. Here is an example for Github Developer Documentation. # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com { value } ' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if '#' in cat . link : continue catalogue . append ( cat ) urls = [ page . link for page in catalogue ][: 10 ] async for response in self . multiple_request ( urls , is_gather = True ): title = catalogue [ response . index ] . title yield self . parse_page ( response , title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) if __name__ == '__main__' : GithubDeveloperSpider . start () Our crawler starts with start_urls and parse method as usual. We get a list of urls. Then we call self.multiple_request method to send further requests. multiple_request(urls, **kwargs) method requires a positional argument urls . It is a list of string. It also accept any other arguments like ruia.Request . Pay attention to use async for statement. multiple_request method returns an asynchronous generator. It yields responses. You may want to use enumerate to get the index of responses like this: async def parse ( self , response ): urls = [ f 'https://site.com/ { page } ' for page in range ( 10 )] async for response in self . multiple_request ( urls ): pass Then you will get an Exception, telling you that enumerate cannot process asynchronous generator. ruia provides a property for every response object: response.index . It is useful when you want to pass some context to the next parsing method. The order of responses currently is just the same as urls, but it's an unstable feature. Use response.index to get its position. multiple_request has another argument is_gather , which indicates whether ruia should run the requests together or not. If is_gather=True , then the requests will run together. If not, the requests will run one by one. is_gather=True is usually better, except one condition: we have a catalogue contains 1000 pages. If we use gather=True , we will get the response after 1000 requests. It may take too long before parsing. Concurrency Control Let's repeat the Github Developer spider. # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com { value } ' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): catalogue . append ( cat ) for page in catalogue [: 20 ]: if '#' in page . link : continue yield Request ( url = page . link , metadata = { 'title' : page . title }, callback = self . parse_page ) async def parse_page ( self , response : Response ): item = await PageItem . get_item ( html = await response . text ()) title = response . metadata [ 'title' ] print ( title , len ( item . content )) if __name__ == '__main__' : GithubDeveloperSpider . start () This time, there's a line added: concurrency = 5 Here's a brief introduction about concurrency. Some websites are friendly to crawlers, while some are not. If you visit a website too frequently, you will be banned from the server. Besides, to be a good crawler, we should protect the server, rather than making it crash. Not every server can burden a huge spider. To protect both, we have to control our concurrency. Concurrency means the connection numbers in a time. In this case, we set it to 5. Let's have a short look on the log. Output: [2019:01:23 00:01:59]-ruia-INFO spider : Spider started! [2019:01:23 00:01:59]-ruia-WARNINGspider : ruia tried to use loop.add_signal_handler but it is not implemented on this platform. [2019:01:23 00:01:59]-ruia-WARNINGspider : ruia tried to use loop.add_signal_handler but it is not implemented on this platform. [2019:01:23 00:01:59]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/media/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/oauth_authorizations/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/auth/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/troubleshooting/> [2019:01:23 00:02:01]-Request-INFO request: <GET: https://developer.github.com/v3/previews/> Overview 38490 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/versions/> OAuth Authorizations API 66565 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/> Media Types 8652 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/events/> Troubleshooting 2551 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/events/types/> API Previews 19537 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/feeds/> Other Authentication Methods 6651 [2019:01:23 00:02:03]-Request-INFO request: <GET: https://developer.github.com/v3/activity/notifications/> Versions 1344 Feeds 14090 [2019:01:23 00:02:03]-Request-INFO request: <GET: https://developer.github.com/v3/activity/starring/> Activity 2178 [2019:01:23 00:02:04]-Request-INFO request: <GET: https://developer.github.com/v3/activity/watching/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/> Events 11844 Starring 55228 [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/runs/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/suites/> Event Types & Payloads 1225037 Notifications 65679 Watching 35775 Checks 7379 Check Runs 116607 [2019:01:23 00:02:06]-ruia-INFO spider : Stopping spider: ruia Check Suites 115330 [2019:01:23 00:02:06]-ruia-INFO spider : Total requests: 18 [2019:01:23 00:02:06]-ruia-INFO spider : Time usage: 0:00:07.342048 [2019:01:23 00:02:06]-ruia-INFO spider : Spider finished! Focus on the first several lines. [2019:01:23 00:01:54]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/media/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/oauth_authorizations/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/auth/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/troubleshooting/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/previews/> Overview 38490 [2019:01:23 00:02:07]-Request-INFO request: <GET: https://developer.github.com/v3/versions/> OAuth Authorizations API 66565 The first request is at our requesting the catalogue page. Then, our spider send 5 requests at almost same time, at [00:02:00] . 5 seconds later, at [00:02:05] , our spider receives a response, and then sent another request. The response was parsed immediately. 2 seconds later, at [00:02:07] , our spider receives another response, and sent another request. Then, it parsed the response immediately. That is to say, at any time, there are 5 connections between spider and server. That is concurrency control. Hey, notice that our spider sent 5 requests at same time! Thanks to python's asyncio library, we can write asynchronous crawler easier and faster. Coroutines runs faster than multi-threadings. Use Middleware Ruia provides mainly two ways to enhance itself. Firstly let's talk about middlewares. Middlewares are used to process a request before it's sending and to process a response after it's receiving In a word, it is something between your spider and server. Here is a simple middleware named ruia-ua , it is used to automatically add random User-Agent to your requests. Firstly, install ruia-ua . pip install ruia-ua Then, add it to your spider. from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , res ): async for item in HackerNewsItem . get_items ( html = res . html ): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) ruia.Spider has an argument middleware . It receives a list or a single middleware. Use Plugin If you want better control of your spider, try to use some plugins. ruia-pyppeteer is a ruia plugin used for loading JavaScript. Firstly, install ruia-pyppeteer . pip install ruia_pyppeteer # New features pip install git+https://github.com/ruia-plugins/ruia-pyppeteer Note When you use load_js, it will download a recent version of Chromium (~100MB). This only happens once. Here is a simple example to show how to load JavaScript. import asyncio from ruia_pyppeteer import PyppeteerRequest as Request request = Request ( \"https://www.jianshu.com/\" , load_js = True ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) print ( await response . text ()) Here is an example to use it in your spider: from ruia import AttrField , TextField , Item from ruia_pyppeteer import PyppeteerSpider as Spider from ruia_pyppeteer import PyppeteerRequest as Request class JianshuItem ( Item ): target_item = TextField ( css_select = 'ul.list>li' ) author_name = TextField ( css_select = 'a.name' ) author_url = AttrField ( attr = 'href' , css_select = 'a.name' ) async def clean_author_url ( self , author_url ): return f \"https://www.jianshu.com { author_url } \" class JianshuSpider ( Spider ): start_urls = [ 'https://www.jianshu.com/' ] concurrency = 10 # Load js on the first request load_js = True async def parse ( self , response ): async for item in JianshuItem . get_items ( html = await response . text ()): # Loading js by using PyppeteerRequest yield Request ( url = item . author_url , load_js = self . load_js , callback = self . parse_item ) async def parse_item ( self , response ): print ( response ) if __name__ == '__main__' : JianshuSpider . start ()","title":"4. Spider Control"},{"location":"en/tutorials/spider.html#spider-control","text":"ruia.Spider is used to control the whole spider, it provides the following functions: Normalize your code Maintain a event loop Manage requests and responses Concurrency control Manage middlewares and plugins Although it works well, to use only ruia.Item to create a spider, ruia recommend to use ruia.Spider to implement a stronger spider.","title":"Spider Control"},{"location":"en/tutorials/spider.html#normalize-your-code","text":"ruia.Spider requires a class property start_urls as the entry point of a spider. Inner, ruia will iterate start_urls , and send a request to server for each request. After receiving server response, ruia will call spider.parse(response) , and this is the main part of your spider. Here's a simple parse example, to simply save response fields to a text file. We only have to define start_urls , and implement a parse method. import aiofiles from ruia import Spider , Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p= { index } ' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) aiofiles is a third-party library to operate files in asynchronous way. It provides APIs the same as python standard open function. Now, we have written a spider, and time to start crawling. import aiofiles from ruia import Spider , Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p= { index } ' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = await response . text ()): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Done. Now your code is more readable and maintainable.","title":"Normalize your code"},{"location":"en/tutorials/spider.html#send-further-requests","text":"I don't think that just crawling the catalogue of news satisfied you. Next we will crawl news itself. Hacker news gathers news from many websites, it's not easy to parse each article of it. For this example, we'll crawl Github Developer Documentation . If you are a user of scrapy , perhaps you'd like this essay for migration: Write Spiders like Scrapy . Ruia provides a better way to send further requests by new asynchronous syntax async/await. It provides more readability and is more flexible. In any parse method, just yield a coroutine, and the coroutine will be processed by ruia . Here is a simple pseudo-code. from ruia import Spider class MySpider ( Spider ): async def parse ( self , response ): next_response = await self . request ( f ' { response . url } /next' ) yield self . parse_next_page ( next_response , metadata = 'nothing' ) async def parse_next_page ( self , response , metadata ): print ( await response . text ()) It works well, except you want to yield many coroutines in a for loop. Look at the following pseudo-code: from ruia import Spider class MySpider ( Spider ): async def parse ( self , response ): for i in range ( 10 ): response = await self . request ( f 'https://some.site/ { i } ' ) yield self . parse_next ( response ) async def parse_next ( self , response ): print ( await response . text ()) You will find the requests in the for loop runs in a synchronous way! Oh, awful. To solve this problem, ruia provides a multiple_request method. Here is an example for Github Developer Documentation. # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com { value } ' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): if '#' in cat . link : continue catalogue . append ( cat ) urls = [ page . link for page in catalogue ][: 10 ] async for response in self . multiple_request ( urls , is_gather = True ): title = catalogue [ response . index ] . title yield self . parse_page ( response , title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = await response . text ()) print ( title , len ( item . content )) if __name__ == '__main__' : GithubDeveloperSpider . start () Our crawler starts with start_urls and parse method as usual. We get a list of urls. Then we call self.multiple_request method to send further requests. multiple_request(urls, **kwargs) method requires a positional argument urls . It is a list of string. It also accept any other arguments like ruia.Request . Pay attention to use async for statement. multiple_request method returns an asynchronous generator. It yields responses. You may want to use enumerate to get the index of responses like this: async def parse ( self , response ): urls = [ f 'https://site.com/ { page } ' for page in range ( 10 )] async for response in self . multiple_request ( urls ): pass Then you will get an Exception, telling you that enumerate cannot process asynchronous generator. ruia provides a property for every response object: response.index . It is useful when you want to pass some context to the next parsing method. The order of responses currently is just the same as urls, but it's an unstable feature. Use response.index to get its position. multiple_request has another argument is_gather , which indicates whether ruia should run the requests together or not. If is_gather=True , then the requests will run together. If not, the requests will run one by one. is_gather=True is usually better, except one condition: we have a catalogue contains 1000 pages. If we use gather=True , we will get the response after 1000 requests. It may take too long before parsing.","title":"Send Further Requests"},{"location":"en/tutorials/spider.html#concurrency-control","text":"Let's repeat the Github Developer spider. # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com { value } ' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = await response . text ()): catalogue . append ( cat ) for page in catalogue [: 20 ]: if '#' in page . link : continue yield Request ( url = page . link , metadata = { 'title' : page . title }, callback = self . parse_page ) async def parse_page ( self , response : Response ): item = await PageItem . get_item ( html = await response . text ()) title = response . metadata [ 'title' ] print ( title , len ( item . content )) if __name__ == '__main__' : GithubDeveloperSpider . start () This time, there's a line added: concurrency = 5 Here's a brief introduction about concurrency. Some websites are friendly to crawlers, while some are not. If you visit a website too frequently, you will be banned from the server. Besides, to be a good crawler, we should protect the server, rather than making it crash. Not every server can burden a huge spider. To protect both, we have to control our concurrency. Concurrency means the connection numbers in a time. In this case, we set it to 5. Let's have a short look on the log. Output: [2019:01:23 00:01:59]-ruia-INFO spider : Spider started! [2019:01:23 00:01:59]-ruia-WARNINGspider : ruia tried to use loop.add_signal_handler but it is not implemented on this platform. [2019:01:23 00:01:59]-ruia-WARNINGspider : ruia tried to use loop.add_signal_handler but it is not implemented on this platform. [2019:01:23 00:01:59]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/media/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/oauth_authorizations/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/auth/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/troubleshooting/> [2019:01:23 00:02:01]-Request-INFO request: <GET: https://developer.github.com/v3/previews/> Overview 38490 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/versions/> OAuth Authorizations API 66565 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/> Media Types 8652 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/events/> Troubleshooting 2551 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/events/types/> API Previews 19537 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/feeds/> Other Authentication Methods 6651 [2019:01:23 00:02:03]-Request-INFO request: <GET: https://developer.github.com/v3/activity/notifications/> Versions 1344 Feeds 14090 [2019:01:23 00:02:03]-Request-INFO request: <GET: https://developer.github.com/v3/activity/starring/> Activity 2178 [2019:01:23 00:02:04]-Request-INFO request: <GET: https://developer.github.com/v3/activity/watching/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/> Events 11844 Starring 55228 [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/runs/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/suites/> Event Types & Payloads 1225037 Notifications 65679 Watching 35775 Checks 7379 Check Runs 116607 [2019:01:23 00:02:06]-ruia-INFO spider : Stopping spider: ruia Check Suites 115330 [2019:01:23 00:02:06]-ruia-INFO spider : Total requests: 18 [2019:01:23 00:02:06]-ruia-INFO spider : Time usage: 0:00:07.342048 [2019:01:23 00:02:06]-ruia-INFO spider : Spider finished! Focus on the first several lines. [2019:01:23 00:01:54]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/media/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/oauth_authorizations/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/auth/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/troubleshooting/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/previews/> Overview 38490 [2019:01:23 00:02:07]-Request-INFO request: <GET: https://developer.github.com/v3/versions/> OAuth Authorizations API 66565 The first request is at our requesting the catalogue page. Then, our spider send 5 requests at almost same time, at [00:02:00] . 5 seconds later, at [00:02:05] , our spider receives a response, and then sent another request. The response was parsed immediately. 2 seconds later, at [00:02:07] , our spider receives another response, and sent another request. Then, it parsed the response immediately. That is to say, at any time, there are 5 connections between spider and server. That is concurrency control. Hey, notice that our spider sent 5 requests at same time! Thanks to python's asyncio library, we can write asynchronous crawler easier and faster. Coroutines runs faster than multi-threadings.","title":"Concurrency Control"},{"location":"en/tutorials/spider.html#use-middleware","text":"Ruia provides mainly two ways to enhance itself. Firstly let's talk about middlewares. Middlewares are used to process a request before it's sending and to process a response after it's receiving In a word, it is something between your spider and server. Here is a simple middleware named ruia-ua , it is used to automatically add random User-Agent to your requests. Firstly, install ruia-ua . pip install ruia-ua Then, add it to your spider. from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , res ): async for item in HackerNewsItem . get_items ( html = res . html ): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) ruia.Spider has an argument middleware . It receives a list or a single middleware.","title":"Use Middleware"},{"location":"en/tutorials/spider.html#use-plugin","text":"If you want better control of your spider, try to use some plugins. ruia-pyppeteer is a ruia plugin used for loading JavaScript. Firstly, install ruia-pyppeteer . pip install ruia_pyppeteer # New features pip install git+https://github.com/ruia-plugins/ruia-pyppeteer Note When you use load_js, it will download a recent version of Chromium (~100MB). This only happens once. Here is a simple example to show how to load JavaScript. import asyncio from ruia_pyppeteer import PyppeteerRequest as Request request = Request ( \"https://www.jianshu.com/\" , load_js = True ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) print ( await response . text ()) Here is an example to use it in your spider: from ruia import AttrField , TextField , Item from ruia_pyppeteer import PyppeteerSpider as Spider from ruia_pyppeteer import PyppeteerRequest as Request class JianshuItem ( Item ): target_item = TextField ( css_select = 'ul.list>li' ) author_name = TextField ( css_select = 'a.name' ) author_url = AttrField ( attr = 'href' , css_select = 'a.name' ) async def clean_author_url ( self , author_url ): return f \"https://www.jianshu.com { author_url } \" class JianshuSpider ( Spider ): start_urls = [ 'https://www.jianshu.com/' ] concurrency = 10 # Load js on the first request load_js = True async def parse ( self , response ): async for item in JianshuItem . get_items ( html = await response . text ()): # Loading js by using PyppeteerRequest yield Request ( url = item . author_url , load_js = self . load_js , callback = self . parse_item ) async def parse_item ( self , response ): print ( response ) if __name__ == '__main__' : JianshuSpider . start ()","title":"Use Plugin"}]}